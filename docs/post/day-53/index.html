    <!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Energize Andover Summer 2017 Blog">
		<meta name="description" content="Blog Description">
		<meta name="generator" content="Hugo 0.50" />
		<title>Day 53 &middot; Energize Andover Summer 2017 Blog</title>
		<link rel="shortcut icon" href="https://mattrossman.github.io/energize-andover-blog/images/favicon.ico">
		<link rel="stylesheet" href="https://mattrossman.github.io/energize-andover-blog/css/style.css">
		<link rel="stylesheet" href="https://mattrossman.github.io/energize-andover-blog/css/highlight.css">
		

		
		<link rel="stylesheet" href="https://mattrossman.github.io/energize-andover-blog/css/font-awesome.min.css">
		

		

        
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
	</head>

    <body>
       <nav class="main-nav">
	

	
	
		<a href='https://mattrossman.github.io/energize-andover-blog/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://mattrossman.github.io/energize-andover-blog/post'>All Posts</a>
	
	<a href='https://mattrossman.github.io/energize-andover-blog/about'>About</a>

	

	
</nav>


        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>
                        Day 53
                    </h1>
                    <h2 class="headline">
                    
                    Aug 21, 2017
                    · 1217 words

                      <span class="tags">
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<h2 id="function-decorators">Function decorators</h2>

<p>In python you can put <em>function decorators</em> in front of methods to apply some overarching steps to them. In my case I want to time each method call using <code>time.perf_counter()</code>. I can either apply a wrapper to every method of a class as explained <a href="https://stackoverflow.com/a/25828876/8371763">here</a> or just decorate specific methods within my classes.</p>

<pre><code>def time_func(func):
    @wraps(func)
    def wrapper(*args, **kw):
        start = time.perf_counter()
        try:
            res = func(*args, **kw)
        finally:
            end = time.perf_counter()
            dur = '%.2f sec' % (end-start)
            print('{0}.{1} finished in {2}'.format(
                args[0].__class__.__name__,
                func.__name__,
                dur))
        return res
    return wrapper
</code></pre>

<p><br></p>

<pre><code>&gt;&gt;&gt; model.train()
MultiRFModel._get_training_windows finished in 0.01 sec
SingleRFModel._get_feats finished in 1.54 sec
SingleRFModel._get_training_arrays finished in 1.64 sec
SingleRFModel.train finished in 3.60 sec
SingleRFModel._get_feats finished in 1.54 sec
SingleRFModel._get_training_arrays finished in 1.64 sec
SingleRFModel.train finished in 3.63 sec
SingleRFModel._get_feats finished in 1.53 sec
SingleRFModel._get_training_arrays finished in 1.63 sec
SingleRFModel.train finished in 3.73 sec
SingleRFModel._get_feats finished in 1.53 sec
SingleRFModel._get_training_arrays finished in 1.63 sec
SingleRFModel.train finished in 3.56 sec
SingleRFModel._get_feats finished in 1.53 sec
SingleRFModel._get_training_arrays finished in 1.63 sec
SingleRFModel.train finished in 3.48 sec
SingleRFModel._get_feats finished in 1.57 sec
SingleRFModel._get_training_arrays finished in 1.67 sec
SingleRFModel.train finished in 3.05 sec
SingleRFModel._get_feats finished in 1.52 sec
SingleRFModel._get_training_arrays finished in 1.62 sec
SingleRFModel.train finished in 3.74 sec
MultiRFModel.train finished in 24.81 sec
</code></pre>

<p>When interpretting this, remember that <code>_get_feats()</code> is a subset of <code>_get_training_arrays()</code>, which is a subset of <code>train()</code>.</p>

<p>You can see that in general, a good chunk of the <code>SingleRFModel.train()</code> process is being taken up by the <code>_get_training_arrays()</code> step, and most of that comes from the <code>_get_feats()</code> step. If I could optimize that step, I could shave close to ten seconds off the training time.</p>

<p>Problem is, I have very little idea how to improve it. I timed the individual steps of <code>_get_feat()</code> for a single model:</p>

<pre><code>Got data features in 1.26 sec
Got time features in 0.02 sec
Got extra features in 0.26 sec
</code></pre>

<p>I did notice that at the moment, the list comprehension runs <code>_aggregated_data_features</code> on every iteration which unecessarily re-runs the downsampling of the data. I changed it so that <code>_aggregated_data_features</code> just runs once at the beginning and that shaved a big portion of the time off:</p>

<pre><code>Got data features in 0.15 sec
Got time features in 0.02 sec
Got extra features in 0.26 sec
</code></pre>

<p>Which have this overall multi-model performance:</p>

<pre><code>MultiRFModel.train finished in 16.94 sec
</code></pre>

<p>That&rsquo;s approximately a 32% performance improvement. Now, the entire feature generation step takes about half a second for a single model. The biggest time-suck is now scikit-learn&rsquo;s training process.</p>

<h2 id="multicore-processing">Multicore processing</h2>

<p>One parameter of <code>sklearn.ensemble.RandomForestRegressor</code> that I have ignored thusfar is <code>n_jobs</code>. This parameter lets you split up a computational task onto multiple CPU cores. I assumed my chunky old laptop would be single-core, but according to <code>~$ lscpu</code> I apparently have two cores (<em>EDIT: turns out I have 4</em>). I assume the server has at least this many as well. By setting <code>n_jobs=-1</code> the parameter will automatically set to the number of system cores.</p>

<pre><code>&gt;&gt;&gt; model.train()
SingleRFModel.train finished in 1.48 sec
SingleRFModel.train finished in 1.48 sec
SingleRFModel.train finished in 1.58 sec
SingleRFModel.train finished in 1.49 sec
SingleRFModel.train finished in 1.48 sec
SingleRFModel.train finished in 1.19 sec
SingleRFModel.train finished in 1.63 sec
MultiRFModel.train finished in 10.34 sec
</code></pre>

<p>Impressive! That&rsquo;s a 58% decrease in training time from my original measurement. And it would be even faster on the higher core (and likely higher clocked) server CPU. I also wonder if I can somehow split up the multi-model processes across CPU cores or if that would break something (since each model would then try to run its own multi-core process).</p>

<h4 id="more-multiprocessing">More multiprocessing</h4>

<p>The <code>multiprocess</code> module lets you perform custom processes across your CPU cores. Interestingly, the <code>multiprocessing.cpu_count()</code> method claimed I have 4 cores so I must have been interpreting the <code>~$ lscpu</code> output incorrectly.</p>

<p>At first I just followed <a href="http://sebastianraschka.com/Articles/2014_multiprocessing.html">this guide</a> to quickly get started using <code>Pool</code> objects, however after multiple tests I noticed my system start to slow down significantly. I believe this was because I was not running <code>close()</code> on the pool objects after finishing training with them. That addition seemed to resolve the gradual system slowdown. This is what I added to the <code>MultiRFModel.train()</code> method:</p>

<pre><code>pool = mp.Pool(processes=mp.cpu_count())
pool.map(SingleRFModel.train,self.models.values())
pool.close()
</code></pre>

<p>And here&rsquo;s the new performance:</p>

<pre><code>&gt;&gt; model.train()
SingleRFModel.train finished in 4.61 sec
SingleRFModel.train finished in 4.61 sec
SingleRFModel.train finished in 4.63 sec
SingleRFModel.train finished in 4.70 sec
SingleRFModel.train finished in 2.57 sec
SingleRFModel.train finished in 2.90 sec
SingleRFModel.train finished in 3.15 sec
MultiRFModel.train finished in 7.92 sec
</code></pre>

<p>Notice that the individual training times do not add up to the total model training times anymore as they are distributed processes now. I assume that previously multiple cores were working on the same problem since the individual model training time is higher here, but the overall training time has improved. It&rsquo;s nearly a 70% improvement from where I was at the start of today.</p>

<h4 id="issue-changes-not-being-preserved">Issue: changes not being preserved</h4>

<p>The training process does not seem to be preserved when using the pool mapper. When I try predicting, it thinks the models haven&rsquo;t been fitted yet. Even without the multiprocess part, using a regular <code>map</code> to train each of the submodels doesn&rsquo;t preserve the training process while a regular loop does.</p>

<h4 id="solution">Solution?</h4>

<p>This is still a very new area for me so there&rsquo;s probably a better way to be doing this. From what I read it sounds like the <code>Pool.map</code> process deals with clones of objects rather than moving around the original. For that reason I needed my <code>SingleRFModel.train</code> method to return a copy of the trained model. Then I made a helper function <code>MultiModelRF.subtrain</code> that takes in a (key, value) pair from the dictionary of submodels, trains the submodel, and returns an updated (key, value) pair. These pairs are built into a big list by mapping over <code>MultiRFModel.models.items()</code>, and that list is turned <em>back</em> into a dictionary with the <code>dict()</code> constructor. That dictionary is re-assigned to <code>MultiRFModel.models</code>, thus completing the update and reassignment process. By the end of it all I am able to properly <code>predict</code> with the trained models.</p>

<p><em>EDIT: I actually don&rsquo;t need the <code>SingleRFModel.train</code> method to return a copy of itself, I can just train each cloned submodel and return those trained clones.</em></p>

<p>The performance boost is not entirely consistent. At least, I&rsquo;m definitely not get the sub-eight second time I got on my first multiprocess attempt. Sometimes I get better performance using only 2 processes rather than all of the CPU cores. Most of the time the multiprocess approach is better than just a serialized approach, but sometimes the multiprocess implementation will hang and take longer. I&rsquo;m hoping this just has to do with the limited memory of my laptop but if I have to remove the multiprocess feature in the end, it won&rsquo;t be that big of a performance difference.</p>

<pre><code>&gt;&gt;&gt; model.train()
SingleRFModel.train finished in 4.77 sec
SingleRFModel.train finished in 4.85 sec
SingleRFModel.train finished in 4.92 sec
SingleRFModel.train finished in 5.00 sec
SingleRFModel.train finished in 2.65 sec
SingleRFModel.train finished in 3.17 sec
SingleRFModel.train finished in 3.42 sec
MultiRFModel.train finished in 8.88 sec
</code></pre>

<p>Overall about a 64% improvement from the start of today. When I go back to just the school-time subset of data, it feels incredibly quick to train (just a couple of seconds). This makes me more confident in the model efficiency on large future data sets.</p>

                </section>
            </article>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'energize-andover-blog';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

		
            

            

            <footer id="footer">
    
	<div align="center">
	<a class="small" href="https://github.com/mattrossman/energize-andover-blog/">
        	<i class="fab fa-github-square"></i>
		Page Source
	</a>
	</div>
    <p class="small">
    
       © Copyright 2019 
    

    </p>
    
    </p>
</footer>

        </section>

        <script src="https://mattrossman.github.io/energize-andover-blog/js/jquery-3.3.1.min.js"></script>
<script src="https://mattrossman.github.io/energize-andover-blog/js/main.js"></script>
<script src="https://mattrossman.github.io/energize-andover-blog/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-86883080-2', 'auto');
	
	ga('send', 'pageview');
}
</script>






<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	MathJax.Hub.Config({
	  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],processEscapes:true}
	});
</script>


<script src="https://unpkg.com/lunr/lunr.js"></script>


<script src="https://mattrossman.github.io/energize-andover-blog/js/site-search.js"></script>


<script src="https://mattrossman.github.io/energize-andover-blog/js/featured-posts.js"></script>

    </body>
</html>
