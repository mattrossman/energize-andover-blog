<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Energize Andover Summer 2017 Blog</title>
    <link>https://mattrossman.github.io/energize-andover-blog/post/</link>
    <description>Recent content in Posts on Energize Andover Summer 2017 Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 18 Jul 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mattrossman.github.io/energize-andover-blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Day 30</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-30/</link>
      <pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-30/</guid>
      <description>An exciting find I ended yesterday with a CMU lecture on nonlinear regression that looked eerily similar to the work we&amp;rsquo;re doing. After sharing it with the team, Frank noticed that it&amp;rsquo;s actually part of an entire course called Computational Methods for the Smart Grid. All of the resources (lecture videos, slides, notes) are freely availabe online. The course teaches how to do precisely what we&amp;rsquo;ve been trying to accomplish this whole summer.</description>
    </item>
    
    <item>
      <title>Day 29</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-29/</link>
      <pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-29/</guid>
      <description>Today I&amp;rsquo;ll be taking a look at the new Bancroft school data. Bancroft is supposed to be a very energy efficient building so it&amp;rsquo;ll be ineresting to compare the energy usage patterns to AHS.
The simplest filter to compare would be night data. Some activities may go on during the summer or weekends, but most buildings will not be operating at night.
  Interestingly we&amp;rsquo;re seeing a bimodal shape in the night-time distribution.</description>
    </item>
    
    <item>
      <title>Day 28</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-28/</link>
      <pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-28/</guid>
      <description>Met with the team today at the library. I gave an update on the status of the fitting process. Frank had looked into the Poisson regression and was having some trouble so I wanted to take a stab at it.
I too was unable to see a clear application of the theory to our scenario. From what I understand, a Poisson distribution is used for counts of event occurences. Our data doesn&amp;rsquo;t immediately apply to this since we are measuring a continuous value, but you can tweak it by, for instance, measuring the counts of data points that lie between 400-450 kW.</description>
    </item>
    
    <item>
      <title>Day 27</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-27/</link>
      <pubDate>Thu, 13 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-27/</guid>
      <description>This morning I joined Anil on a guided tour of Andover High School. He pointed out key parts of the electric and heating framework for the building. Starting with the main power switch and working down to the sub-panels distributed throughout the school it became easier to visualize the tree structure of the circuitry. It also became appart how challenging it is to navigate the electrical map, so it&amp;rsquo;s good that the other team(s) are working on improving this.</description>
    </item>
    
    <item>
      <title>Day 26</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-26/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-26/</guid>
      <description>Wrapping up from yesterday Before I go on, I was thinking more about the importance of the quantile residual plot. The benefit of using the local extrema of this chart is that they identify the points of greatest dissonance between the sample and model. On Day 23 we picked the 95th percentile of the model and found it correlated to the 86th percentile of the sample. But that was just a nice sounding threshold.</description>
    </item>
    
    <item>
      <title>Day 25</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-25/</link>
      <pubDate>Tue, 11 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-25/</guid>
      <description>Plotting the Quartiles scipy.stats.probplot is a method for comparing data quantiles to a distribution.
stats.probplot(school_main,egz.lognorm_params(school_main),&#39;lognorm&#39;, plot=plt)    For some reason even if I set fit=False it still shows the regression line. This visual doesn&amp;rsquo;t interest me too much because I can&amp;rsquo;t see the actual percentage values, I&amp;rsquo;d rather have a plot comparing the percentile ranks
I can see this happening two ways:
 a range of quantiles (for the sample) compared to the expected CDF of their corresponding values a range of quantiles (for the model) compared to the sample percentileofscore of their values (using the model PPF)</description>
    </item>
    
    <item>
      <title>Day 24</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-24/</link>
      <pubDate>Mon, 10 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-24/</guid>
      <description>Improving the maximization Last time I mentioned how I wanted to find a better way to find the peak of the Kernel Density Estimation. Previously we were evaluating the KDE probability at 10,000 sample points within the region and letting Pandas find the max of that set. I wanted to see if there was a more direct, precise way to do this.
Numpy has a gradient function for calculating partial derivatives, but it only applies to arrays of sample points.</description>
    </item>
    
    <item>
      <title>Day 23</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-23/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-23/</guid>
      <description>The team met at the library today. I started by making a list of the items that we may want to consider working on:
 fitting a log-normal distribution to the data marking values past a certain percentile of that PDF translating data sample to match the fitted estimation doing something with the temperature data Poisson regressions likelihood estimation calculation, alternatively SSE  
A lot of these rely on already having a fitted distribution, so that was our goal for today.</description>
    </item>
    
    <item>
      <title>Day 22</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-22/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-22/</guid>
      <description>Another brief post today. I spent another session going over the remainder of the energize module (unstack_by_time, plot_normal and trapz).
I also went over python features like lambda, map, filter, and list comprehension.
That&amp;rsquo;s all I can really think of that needs to be covered, so tomorrow we&amp;rsquo;re going to meet at the library to start planning the actual work that needs to get done.</description>
    </item>
    
    <item>
      <title>Day 21</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-21/</link>
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-21/</guid>
      <description>Not much new to post about today. I worked with Frank and Ajay this morning to go over a detailed explaination of how to perform some core pandas operations.
We covered how to read in the CSV file, how to use the ical_ranges and time_filter functions in the energize module, various visualization methods like line plots and histograms, and data indexing methods.
We&amp;rsquo;re going to go over some more examples tomorrow, and hopefully by the end of the week we&amp;rsquo;ll be ready to get back to work on the actual problem statement.</description>
    </item>
    
    <item>
      <title>Day 20</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-20/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-20/</guid>
      <description>Note: Today I uploaded my main Python module here for others to reference
 Meeting Today I had a meeting with Frank, Ajay, and Anil&amp;rsquo;s son Viraj. Viraj is experienced both with statistics and the programming tools we are using so he was able to offer some useful insights. I&amp;rsquo;ll go over some of the main points:
Expected Distributions On Day 18 I ran a script to calculate the best fitting distribution of our data, but theoretically this isn&amp;rsquo;t a good model to &amp;ldquo;expect&amp;rdquo; of our data since it just happened to be the distribution of our sample.</description>
    </item>
    
    <item>
      <title>Day 19</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-19/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-19/</guid>
      <description>Power → Energy To gain a meaningful understanding of the difference between sets of power data, it would be helpful to know how much energy was used. Energy is the resource being consumed, and small changes in power usage can accumulate significant differences in energy consumption. It also enables a smoother conversion into dollars spent or saved.
Energy is represented by the area under the power plot. A very basic approximation of this can be calculated with a Reimann sum, which multiplies the power value at one point by the span of time it represents (either spanning to the right or left).</description>
    </item>
    
    <item>
      <title>Day 18</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-18/</link>
      <pubDate>Thu, 29 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-18/</guid>
      <description>The primary library in Anaconda for performing complex scientific calculations is scipy. It includes 82 built-in distribution functions. You can test how well a distribution applies to a sample using the fit() function.
Someone online was nice enough to write a script that iterates over every included distribution function and finds the best fitting one. This calculation takes quite a while to run. I tried running it on the Main power data with my school hours filter:</description>
    </item>
    
    <item>
      <title>Day 17</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-17/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-17/</guid>
      <description>Before I get sidetracked, here&amp;rsquo;s a nice resource of statistical tests and their use cases. I also really like this site which goes into great detail on not only how to perform various statistical measures, but also situations when you should (and more importantly shouldn&amp;rsquo;t) use them.
Daily power models I don&amp;rsquo;t want to ignore the fact that we&amp;rsquo;re dealing with time series data. Most statistical tests are based around random samples with no inherent ordering, but our data has the added factor of ordered time stamps.</description>
    </item>
    
    <item>
      <title>Day 16</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-16/</link>
      <pubDate>Tue, 27 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-16/</guid>
      <description>Sample Data Makeup So far I&amp;rsquo;ve not been very particular about what kind of data sample I&amp;rsquo;m getting. I&amp;rsquo;ve mentioned that the sampling rate is not consistent and there&amp;rsquo;s lots of empty entries but it hasn&amp;rsquo;t been a concern yet.
Now I need to pay attention to these details. I&amp;rsquo;m going to set the record straight on what the sample data looks like by asking myself two questions:
1. Where are the null entries occuring?</description>
    </item>
    
    <item>
      <title>Day 15</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-15/</link>
      <pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-15/</guid>
      <description>Today we got to hold our first real team meeting. Frank and Ajay joined me at the library, and I spent time explaining some of the work I had done and sharing the basics of how to use Pandas. For now I&amp;rsquo;m suggesting they spend time setting up Anaconda and Pandas and get a feel for how to use DataFrame and Series objects.
In the meantime I&amp;rsquo;ll continue working with probability distribution functions.</description>
    </item>
    
    <item>
      <title>Day 14 - Summary</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-14/</link>
      <pubDate>Fri, 23 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-14/</guid>
      <description>Blog Guide A good chunk of today was spent writing a guide on how to host your own blog just like the one you&amp;rsquo;re reading. Feel free to comment on it if there&amp;rsquo;s areas that need clarification. There&amp;rsquo;s also a link to it on the &amp;ldquo;About&amp;rdquo; page so you can find the guide later.
Now I&amp;rsquo;d like to summarize the main ideas of my first couple of weeks of work.</description>
    </item>
    
    <item>
      <title>Day 13</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-13/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-13/</guid>
      <description>I&amp;rsquo;m going to hold off on pattern detection right now. I spent some time thinking about it today and it would be a pretty extensive problem, and I don&amp;rsquo;t have solid sample data to even test it on.
Instead I&amp;rsquo;ll play around with one of Anil&amp;rsquo;s suggestions which focuses on percentages of data count in a certain value region.
Percentage bounds Pandas has a quantile() function that returns the data value at a given percentile.</description>
    </item>
    
    <item>
      <title>Day 12</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-12/</link>
      <pubDate>Wed, 21 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-12/</guid>
      <description>While I wait on the temeprature data report, I&amp;rsquo;ll transition back to the night data. Before I was just looking at distributions, now I want to see the plots and look for any patterns.
As a reminder, I&amp;rsquo;m saying &amp;lsquo;night&amp;rsquo; data lies between 11:00PM and 4:00AM
Downsampling the Night Data The plots thusfar have been pretty cluttered because I&amp;rsquo;m looking at every timestamp entry from every day of the sample region.</description>
    </item>
    
    <item>
      <title>Day 11</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-11/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-11/</guid>
      <description>To start off, I&amp;rsquo;m enabling comments on the blog. Even though the site is static, it can connect to Disqus for third-party comment handling. This theme even has Disqus built in so it should be a simple matter of changing some lines in the config file.
Centering the data It irks me is how I can&amp;rsquo;t center my rolling window when I use a time offset. One workaround (I think I mentioned this yesterday) is reindexing my data at higher detail.</description>
    </item>
    
    <item>
      <title>Day 10</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-10/</link>
      <pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-10/</guid>
      <description>The pandas rolling object has a few built in commands that I have already made use of, like .median(). But for broader scenarios, you can use the .apply() function to, as the name suggests, apply your own function across the windows.
The function must accept a Numpy array (I wish it was just a Series instead) and return a single number. For the time being I made a temporary function to handle Numpy median absolute deviations.</description>
    </item>
    
    <item>
      <title>Day 9</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-9/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-9/</guid>
      <description>In Day 4&amp;rsquo;s post I talked about median absolute deviations and their relationship to estimate population standard deviation. It&amp;rsquo;s a start, but its not a great way to detect anomalies. It&amp;rsquo;s a bit too static.
To illustrate this, here&amp;rsquo;s a plot of the Main power entries from yesterday&amp;rsquo;s filter. I ran the calculations of median, MAD and $\hat{\sigma}$.
  The black line is at the sample median, and the red line is $3\hat{\sigma}$ above that.</description>
    </item>
    
    <item>
      <title>Day 8</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-8/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-8/</guid>
      <description>I printed the 2016-17 Andover District Calendar and got to work entering the data. Ultimately I organized it into two calendars, one for days with no school entirely (which I inputted as &amp;lsquo;all-day&amp;rsquo; events) and one for half days (to ensure sufficient overlap I entered the events starting from 10:50AM when school gets out and ran until 11:59PM at night).
I exported these .ics files, then imported them into python with my .</description>
    </item>
    
    <item>
      <title>Day 7</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-7/</link>
      <pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-7/</guid>
      <description>For institutions with very complex schedules, the basic filtering I created yesterday probably won&amp;rsquo;t suffice. Instead, it may be nicer to let the user set their desired schedules in a graphical environment and use that as a time filter in the application.
Rather than making my own graphical solution, I&amp;rsquo;ll let the user do so in their environment of choice and simply import that data in the popular .ical format, which consists of .</description>
    </item>
    
    <item>
      <title>Day 6</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-6/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-6/</guid>
      <description>Today I&amp;rsquo;ll be implementing the time_filter function that I outlined yesterday. Since then, I made some slight adjustments to the signature that you can see in the final version below.
Packing and Unpacking First thing I had to look up was how to handle optional parameters. It seems that you can use an asterisk * before an argument to get a Tuple of optional positional parameters, while a double asterisk ** is for a Dictionary of optional keyword parameters.</description>
    </item>
    
    <item>
      <title>Day 5</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-5/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-5/</guid>
      <description>Daytime usage I haven&amp;rsquo;t really addressed the daytime power usage yet. To be consistent with the EnergizeApps parser, I&amp;rsquo;ll define &amp;lsquo;day&amp;rsquo; as 5AM - 8PM (inclusive, exclusive). Likewise going forward I should define &amp;lsquo;night&amp;rsquo; as 11PM - 4AM.
I will not only want to look at the overall daytime usage, but isolate the weekday and weekend usage since I&amp;rsquo;d expect the building to be unoccupied on weekends. Once again I get to leverage the handy DatetimeIndex structure which holds a weekday component.</description>
    </item>
    
    <item>
      <title>Day 4</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-4/</link>
      <pubDate>Fri, 09 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-4/</guid>
      <description>I did some searching about median absolute deviation in pandas and found this post. The devoper mentioned that they were considering adding a center parameter to the built in Pandas.Series.mad() function, but advocates against using the function altogether and instead just implementing median absolute deviation manually like this:
abs(x - x.median()).median()  In my case, I&amp;rsquo;ll be using x_right - x.median() since I&amp;rsquo;m only looking at the upper values.</description>
    </item>
    
    <item>
      <title>Day 3</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-3/</link>
      <pubDate>Thu, 08 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-3/</guid>
      <description>Quick blog update Just wanted to share, I was playing around with Hugo today and learned about shortcodes. I was looking for a better way to handle images. Previously I would have to type ![[alt](/path/to/img)](/path/to/img) to have self-linking images. Instead, I made a shortcode that just needs the path once to create the linked image. Furthermore, I was manually typing the image paths, which were organized as /month/day/file.png. Now the shortcode reads the post&amp;rsquo;s date and inserts the month and day straight into the path, so all I need are file names.</description>
    </item>
    
    <item>
      <title>Day 2</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-2/</link>
      <pubDate>Wed, 07 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-2/</guid>
      <description>Now that the blog is running relatively smoothly I&amp;rsquo;m able to focus on the actual problem at hand. I started by installing the latest release of Anaconda, which I then used to install the pandas library.
Pandas in a Nutshell The pandas library is centered around DataFrame structures and their child Series structures. A dataframe functions like an excel spreadsheed, with various columns of data matched by each row&amp;rsquo;s index.</description>
    </item>
    
    <item>
      <title>Day 1</title>
      <link>https://mattrossman.github.io/energize-andover-blog/post/day-1/</link>
      <pubDate>Tue, 06 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattrossman.github.io/energize-andover-blog/post/day-1/</guid>
      <description>Today marks the start of my work on the Energize Andover project. I met with Anil at the library and went over the main goals of my team&amp;rsquo;s project.
Basically, given an arbitrary data set of timestamped energy measures, we want to be able to perform various analytics and use those to identify key areas where there is potential for energy savings. How we will accomplish that is to be decided, though I was given some places to start.</description>
    </item>
    
  </channel>
</rss>