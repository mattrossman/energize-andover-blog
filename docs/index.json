[{"basename":"day-51","content":" Resolving yesterday\u0026rsquo;s mystery After some testing I realized the only difference in the CSV-writing script\u0026rsquo;s approach was that I had ommited temperature data, having come to the conclusion earlier that it hurt the prediction performance. That may have been the case for the particular day I was looking at, but on a larger scale (such as the 4-day test region) I guess it actually improved performance.\nWhen I added the temperature data back to the features table I got a more familiar result:\n  Some columns still have difficulty with predictions, especially the Collins Center. This could be improved with more detailed occupancy data. This brings to my attention the possibility that some columns of a MultiRFModel may require specific extra features. I still want to keep some extra features communal (since all columns will likely depend on common factors like holidays or temperature), but if certain columns are only contained in specific parts of the school (with specific occupancy schedules) then all the other occupancy data would just add noise to the predictions. Perhaps the MultiRFModel could have an addition argument that contains a list of unique feature tables that apply uniquely to each column.\nI added it by creating a different _get_training_arrays() method for MultiRFModel that includes the additional column_features attribute. I also added some validation steps in the class initialization function to allow the user to only pass in additional tables for some columns, or for none at all.\nInferring data frequency I made some changes to how data is handled to allow the user to pass in data with an unspecified frequency. I also learned that pandas.read_csv() has a parse_dates parameter that you can set to [0] to automatically parse the index column dates as datetime objects - pretty handy.\nSwitching to slice based windows At the moment the training windows contain a big list of index values. In reality I only need the start and end date of each window, and to use a slice with those endpoints. I\u0026rsquo;ll start by getting this from just the min() and max() of the existing windows and making sure that it doesn\u0026rsquo;t break anything. Then I\u0026rsquo;m curious if I can rework the window generation step so that it doesn\u0026rsquo;t need to store all the in-between dates (that might just be a waste of processing power and memory to store all those datetime objects).\nI got it working with slices with the min/max technique. As far as I can tell the in-between dates are only stored temporarily in the _get_training_windows() method before being reassigned by the slice endpoint indicies. Computationally it\u0026rsquo;s not demanding to calculate them since it still uses NumPy strides. If anything, using slices runs the extra step of calculating the min/max of each window. However, the benefit of using slices as opposed to regular lists of indicies is that the slice indicies don\u0026rsquo;t need to be contained in the target data index. That is, a feature table can be at a different frequency than the power value table without any errors being thrown. Even though previously you could pool feature columns that were at different frequencies, the entire table still had the be at the same frequency as the data (so there would just have to be a bunch of blank spaces). Now you can do without those blank spaces.\nLarger scale test This time I tried a test of the multi-model on all of the AHS data I have. I excluded the extra features since I don\u0026rsquo;t have any future data for them. In practice there would only be a couple of extra features so the performance would not be very different. At a daily power sample frequency it took about 23 seconds to train the model. At a weekly sample frequency it took about 13 seconds.\nSample frequency implementation When I say sample frequency, I mean the way the model selects historical values to use as features. It can be pretty demanding to use every power sample from the past 4 weeks as a feature, so I instead take a power value from every hour or day. That ignores a lot of information though. I may be better off aggregating the values in each sample pool rather than dropping all but the first element. I\u0026rsquo;ll start my taking the mean of each pool.\nWorking with missing power data Currently I\u0026rsquo;ve been running a single pass of the sample data, filling in data from the week before where there are missing values. What if that\u0026rsquo;s not enough to get rid of all the blank spots? Or what if the current implementation makes the model weaker by adding fake data? One way to counteract this is to have the model not train on windows that include missing values. That way it stays ignorant about topics it doesn\u0026rsquo;t know about rather than being potentially misinformed. I\u0026rsquo;m having some trouble implementing this (NumPy logic will make your head spin after looking at it too much) but I\u0026rsquo;ll get back to it some other time. It only becomes an issue if you have lots of missing data and don\u0026rsquo;t take precautions for it.\nWarm starting? Not an idea that I want to implement right now, but the warm_start argument of sklearn.ensemble.RandomForestRegressor would be a potential way to save a lot of processing power. Rather than building a new forest each time, it reuses the old forest and adds some more estimators to it. Since the only difference between nightly models is 1 day\u0026rsquo;s worth of power data, it would save a ton of redundant calculations. Granted, making a new model every day ensures that the random forest is really random. Also I don\u0026rsquo;t have to worry about keeping the same script running in the background all the time. It\u0026rsquo;s not a big priority but maybe something I could check out in the future.\nDocumentation I started documenting the classes at the start but I\u0026rsquo;ve made a lot of changes since then. I\u0026rsquo;m going to spend some time going through and documenting what the attributes and functions do so others can follow along.\nI finished documenting the BaseModel methods, and as I did so I caught a couple bugs. I also found areas I\u0026rsquo;d like to improve next time, mainly with the training array generation step which is a bit messy at the moment.\n","description":"Slice based windows, more tests, bugfixing, writing up documentation","href":"https://mattrossman.github.io/energize-andover-blog/post/day-51/","title":"Day 51"},{"basename":"day-50","content":" Assigning the same data to various objects shouldn\u0026rsquo;t pose a memory issue, as in Python variables store references to data so only one instance of the source data is actually stored. You can verify this with the built-in id() method to see where variables point to. There are still some areas I\u0026rsquo;ve have to optimize in the multi-column model though.\nAs I pointed out yesterday, I don\u0026rsquo;t need to run the window calculations for every column. Even if two objects run the same calculations on a variable, they will point to different areas of memory (so in that case you would be storing duplicates of the same object). So once I set those attributes for the forests, I want to make sure the forests don\u0026rsquo;t try to do any further manipulation to those variables.\nReorganizing the classes Since the classes for single and multi random forest models will share a lot of attributes / methods, it would make more sense to start with a BaseModel class from which the SingleRFModel and MultiRFModel classes branch off of.\nRight now, BaseModel holds a majority of the attributes and methods. SingleRFModel and MultiRFModel just have their own versions of the train() and predict() methods since that\u0026rsquo;s the only area where they really differ. SingleRFModel.predict() returns the values and standard deviations as Series, while MultiRFModel.predict() returns them as DataFrames (with columns labeled accordingly). That means the structure of the predictions should mirror the structure of the original data.\nHere is the current code for MultiRFModel:\nclass MultiRFModel(BaseModel): def __init__(self, *args, **kwargs): super().__init__(*args,**kwargs) self.columns = list(map(lambda col: self.data[col], self.data)) self.estimators = list(map(lambda col: RandomForestRegressor(n_estimators=self.n_estimators), self.data)) def train(self): for i,rf in enumerate(self.estimators): rf.fit(*self._get_training_arrays(self.columns[i])) def predict(self): pred_start_date = self._get_pred_start_date(self.data) preds = list( map(lambda rf,s: self._get_prediction(rf,s,pred_start_date), self.estimators,self.columns)) arr_vals, arr_std = list(zip(*preds)) return (pd.concat(arr_vals,axis=1), pd.concat(arr_std,axis=1))  You can find the full code including BaseModel in the energize module.\nHere\u0026rsquo;s an example of using a MultiRFModel to run the same type of prediction that I\u0026rsquo;ve done in the past, but on all of the circuit panels in df_energy. The solid lines are the actual values (not fed into the model) and the dotted lines are the predictions.\n  Not pictured are the thresholds, but the data for them is calculated. The entire process takes about 10 seconds to run. That includes all of the following:\n Reading the source data Creating the training windows Building the training arrays Auto-upsampling the extra features Training all 7 of the forests (each forest contains 100 decision trees) Calculating the expected values and standard deviations for each column  \nPretty amazing, especially considering the old hardware I\u0026rsquo;m running on.\nHere\u0026rsquo;s another plot that shows the predictions above but with $2\\sigma$ threshold regions added. It\u0026rsquo;s pretty cluttered but you get the idea:\n  In practice you would care mostly about the top of the threshold, and you would probably only look at one panel at a time rather than all of them overlayed.\nI\u0026rsquo;m really pleased with the status of the model right now. It\u0026rsquo;s incredibly straightforward to now set up, whereas previously you had to use a much longer script to set up all the features manually and tinker with things until it worked.\nA quick note on temperature features I experimented with adding the temperature data as a feature and found that it did not improve the predictions (from my anecdotal test it provided a worse fit than without including it).\nAppending to a CSV file I know by default the to_csv() method will just write the data contents to the file at the given path, overwriting it. I want my script to add the new predictions to the end of the prediction table. This post explains how to do that by setting the mode argument to 'a' for append.\nI also wonder if I should have a third table that contains metadata about the predictions (i.e. the model attributes), or at least some kind of text log. That way if someone decides to change the parameters of the prediction process, then the previous prediction will have some context as to how they were made (so you don\u0026rsquo;t mistakenly view all predictions as equal). For example, it could contain the input/gap/output sizes, sample frequecy, number of estimators, extra feature names, and time attributes to name a few.\nI could compromise and make a log stored in a CSV file, so each time it predicts it adds a single entry to the log with each column holding a different meta-attribute.\nRunning a trial To really put this to the test I\u0026rsquo;m going to make a new data file, this time with the final week of power data removed. I\u0026rsquo;ll then make two more files, each with one more day of data than the last. For each file I\u0026rsquo;ll make a model and add its predictions to a continuous prediction table. Then I\u0026rsquo;ll pull in the prediction table and verify that it lines up with the actual values.\nI set up the headers for the value and variance prediction tables as follows:\npd.DataFrame(columns=df_data.columns).to_csv('resources/test_files/vals.csv') pd.DataFrame(columns=df_data.columns).to_csv('resources/test_files/std.csv')  The predictions were made with:\ndef make_pred(num): base_path = 'resources/test_files/' df = pd.read_csv(base_path+'file'+str(num)+'.csv',index_col=0) df.index = pd.to_datetime(df.index) df = egz.only_full_days(df,'15min') df.index.freq = egz.inferred_freq(df) model = egz.MultiRFModel( data = df, td_input = pd.Timedelta(weeks=4), td_gap = pd.Timedelta(days=1), td_output = pd.Timedelta(days=1), time_attrs = ['dayofyear','dayofweek'], extra_features=pd.DataFrame(df_extras['noschool']), n_estimators=50) model.train() vals, std = model.predict() vals.to_csv(base_path+'vals.csv', mode='a', header=False) std.to_csv(base_path+'std.csv', mode='a', header=False)  Then I just ran a loop calling the above function on all 4 test files I made.\nSemi-success I was able to append the predictions to the value and variance tables.\n  This set of predictions looks off compared to the predictions I\u0026rsquo;ve seen so far. When I use my old script to predict the 19th for example, I get a much closer fit that what is pictured here.\nI\u0026rsquo;ll have to figure out what the problem is tomorrow.\n","description":"Restructuring the classes, predictions on multiple columns, outputting a CSV file, trial run","href":"https://mattrossman.github.io/energize-andover-blog/post/day-50/","title":"Day 50"},{"basename":"day-49","content":" Getting the variance For clarity\u0026rsquo;s sake I think I\u0026rsquo;ll keep the variance output seperate from the prediction output. They should be stored in two seperate tables, one for predictions and one for variance. However instead of making two seperate functions (which would have some overlap of calculactions) I can run the calculations in the same function and return a tuple of the two Series.\nI\u0026rsquo;ll use the same technique from before, getting the standard deviation of the residuals of the forest\u0026rsquo;s decision trees:\ndef _get_pred_std(self,X): all_pred = np.array([t.predict(X) for t in self.rf]) return np.std(all_pred,0).ravel()  This is called from the _get_prediction method of the class (called from the public predict method) which then returns a tuple of the predicted power values and the predictions\u0026rsquo; standard deviations.\nmodel.train() vals,std = model.predict() data[vals.index].plot(title='RF Model Demo', label='Actual') vals.plot(label='Predicted') (vals + std*2).plot(label='$2\\sigma$ threshold', style='r--') plt.ylabel('Power (kW)') plt.legend()  That demonstrates the updated process of training the model and using predictions to make a sample plot:\n  Issue: changing window sizes I found that changing the output window size throws some errors in my tests. The main issue was in the pred_start_date calculation which I resolved. Another issue was that it was possible to get null values when calling resample().asfreq() but this did not occur when I replaced it with just asfreq(). Now I can generate larger output periods. The reason I was getting errors on smaller windows sizes was because my extra_features table was not appropriately upscaled.\nI think I found a convenient way to automatically upscale the extras features if necessary:\nextras.fillna(extras.asfreq(timedelta(self.td_output)).ffill())  That only upsamples rows that have missing values where values are needed. I moved this process into a _validated_feat() method that runs when the RandomForestModel object is initialized.\nNow I can safely change all the window parameters regardless of the setup of the feature table and have the predictions run properly.\nReusing models The current setup implies that the model will be trained and make a single prediction every day. No model is expected to be used more than once. The benefit of this approach is that the model will always be trained on the most up to date information. At smaller output window sizes this requires more computing power as the model has to be trained many times per day, but that\u0026rsquo;s not the route I\u0026rsquo;m planning on taking anyways.\nSample weights I remember at least one of the research papers I read focusing on giving a higher preference to more recent samples when making a model. At the moment, all samples have the same weight. I want to incorporate some form of weight decay for older samples. I don\u0026rsquo;t want the dropoff to be too sharp. I think some sort of logarithmic shape is appropriate for the decay. The last sample probably shouldnt\u0026rsquo; have zero weight, rather I should set a baseline weight such as 50%. Here\u0026rsquo;s how I\u0026rsquo;d generate that region:\n1-np.logspace(np.log10(0.5),-3,n_samples)+1e-3)  I\u0026rsquo;ll probably tweak the baseline weight in the future (I don\u0026rsquo;t want to discredit too much of the historical data). The extra 1e-3 on the end ensures that the most recent sample has 100% weight. I save the array as sample_weights_. This is a plot of what the region looks like:\n  Note that here, \u0026ldquo;sample\u0026rdquo; means an input vector which contains a window of input data. So the temporal scale of the x-axis depends on your output window size.\nScheduling tasks To schedule tasks on Linux you can use the built-in cron tool. This post explains how to set up the /etc/crontab file with your desired commands. Before I can set this up, I need to make the actual script that it will run. This moves onto the next order of business:\nI drafted up a script that basically does what I\u0026rsquo;ve already demoed so far except it runs the entire CSV reading process (so each time it runs it accesses the most up to date version of the source data). From there it\u0026rsquo;s really simple to export a CSV of the predictions with the pandas.Series.to_csv() method.\nTodo At the moment only one circuit panel is considered (I\u0026rsquo;ve been using the main panel). It would probably be helpful to have predictions for all of the sub-panels as well. To handle this I will most likely want to make another class that works with dataframes and implicity creates a bunch of RandomForestModels for each column (and can return CSV files accordingly). The challenge is to see if I can make a more efficient way to do this. By simply creating a bunch of independent RandomForestModel objects I\u0026rsquo;m causing a lot of computational overlap. If all the columns occupy the same indicies then the training window index calculation is unecessarily repeated. There is also some overlap of attributes (although I\u0026rsquo;m pretty sure in Python everything is just stored as pointers anyways so this shouldn\u0026rsquo;t be a big deal).\n","description":"Getting prediction variances, auto-upsampling extra features, weighting samples by time, learning how to schedule commands","href":"https://mattrossman.github.io/energize-andover-blog/post/day-49/","title":"Day 49"},{"basename":"day-48","content":" Today\u0026rsquo;s plan I\u0026rsquo;m mainly ironing out the details of the model implementation today. Rather than copying all the code here, I\u0026rsquo;ve moved the model class (now called RandomForestModel) into the energize module.\nI had a brief scare today when I thought my window calculations were all wrong, but I realized it was a simple mistake of limiting my working data to start at egz.df_school.index.min() when it should have been egz.df_school.index.date.min() (to preserve the 00:00:00 start time)\nConsidering how to include additional features Last time my model accounted for two features: historial true values and time features of the target values. Besides that there are additional features the user may want to consider such as building occupancy or temperature.\nI can see these types of features being classified in two ways. First there are features that broadly apply to the target region (e.g. the state of being a holiday is the same for every data point in the 24 hr output region). Then there are features that may vary throughout the output region (e.g. the building may be occupied only for some hours of the day).\nCurrently the implementation of time features acts like the first description - only the first element in the output region is included as a feature since it assumed to be constant for the whole day.\nRather than creating two seperate parameters for region-spanning and intra-region extra features, perhaps there could be one variable that takes in a list of feature tables and reads the frequency specified by the index of each to determine what frequency should be used as features. The benefit of this is the features could each be sampled at arbitrary frequencies (as long as that frequency divides the output region size cleanly)\nThe trouble with implementing this is that the windows use integer based indices, which would not line up with the extra-feature tables since they are at different frequencies.\nRandom thought that I need to consider - the integer-index window approach requires that all the feature tables start at the same point in time. A clean way to get around this would be to form the windows from dates rather than integers. As I mentioned a few posts ago, I don\u0026rsquo;t there\u0026rsquo;s a simple way to get the window bounds from a Pandas rolling object. Perhaps I could just roll across the data index rather than what I\u0026rsquo;m currently doing (rolling across a range equivalent to the length of the data to generate integer indices).\nAnother thing to consider - should I have the model take in a list of extra feature tables, each at their desired frequencies, or should there be a single large table, entirely at the data frequency with Nan\u0026rsquo;s as placeholders to simulate different frequencies among columns? (i.e. a single DataFrame where one column might be a variable that changes hourly while another column may have daily incremented values). I thought the first would be easier but the second sounds more robust and clean.\nTo deal with the inconsistent column frequencies, once I select a region with one of the y_ixs windows I can stack() that sub-frame and then get a flattened copy from its values attribute. The only problem is that if the feature frequency is lower than the output window size, only some windows will contain all of the features. To resolve this I have to foreward fill values at a minimum frequency of the output window size (i.e. 1 day in my case).\nI did test this out by setting the column equal to a new DataFrame with the index as a date_range sampled at td_output and the data as the column with Nan values dropped. However I\u0026rsquo;m going to skip implementing this extra step since it\u0026rsquo;s perhaps just as easy to just pass in an upsampled table upfront. That means the extra feature table can handle features at a higher frequency than the output window, but for features at a lower frequency it\u0026rsquo;s up to the user to upsample the data.\nMaking new predictions My initial attempt at next-prediction implementation returned a date for the day predicted along with an array of predicted values. I changed this to simply return a Series of predicted values with an appropriate DatetimeIndex. To make the predictions I have a input_vector function that generates the array of input values from a date.\nToday\u0026rsquo;s result I made some good progress with the RandomForestModel class today. It\u0026rsquo;s now a very straightforward to make a model from existing data and create the next prediction. I\u0026rsquo;ll walk through an example of the process.\nFirst you create the RandomForestModel object:\nmodel = egz.RandomForestModel( data, td_input = timedelta(weeks=4), td_gap = timedelta(days=1), td_output = timedelta(days=1), sample_freq = timedelta(hours=12), time_attrs = ['dayofyear','dayofweek'], extra_features=extras, n_estimators=100)  Then you train the model:\nmodel.train()  Finally you can make the prediction:\nmodel.next_prediction()    Todo  Add a way to get the variance of the predicted values (it should be outputted in a corresponding Series, or perhaps as a DataFrame along with the predictions) Figure out how to schedule scripts to run on the server Clean up the energize module, get rid of or move unused functions See if it\u0026rsquo;s worth transforming the energize module into some kind of package? (never made them before so I don\u0026rsquo;t know)  ","description":"Adding extra features to the model, streamlining the training and prediction process","href":"https://mattrossman.github.io/energize-andover-blog/post/day-48/","title":"Day 48"},{"basename":"day-47","content":"Very brief post today as I am really short on time.\nOur team met at AHS today but we quickly realized without access to the logged power data there\u0026rsquo;s not a whole lot we can do to start integrating the predictions.\nI spent today starting to move the prediction process into it\u0026rsquo;s own class. My goal is to make the prediction process as clean and simple as possible. This is actually my first time using Python classes so it\u0026rsquo;s probably not very elegant. Nonetheless, I\u0026rsquo;ve made some progress on a few of the internal functions:\nclass rf_model: def __init__(self, data, td_input, td_gap, td_output,time_attrs): self.data = data self.n = len(data) self.data_freq = data.index.freq self.td_input = td_input self.td_gap = td_gap self.td_output = td_output self.time_attrs = time_attrs self.rf = RandomForestRegressor(n_estimators=10) def _time_features(self,attrs): df_f = pd.DataFrame(index=self.data.index) for attr in attrs: df_f[attr] = getattr(df_f.index,attr) return df_f def _index_windows(self): ixs = np.array(range(self.n)) input_size = int(self.td_input / self.data_freq) gap_size = int(self.td_gap / self.data_freq) output_size = int(self.td_output / self.data_freq) ix_windows = egz.rolling_window(ixs, input_size + gap_size + output_size, output_size) X_ixs,_,y_ixs = np.split(ix_windows,[input_size,input_size+gap_size],1) return X_ixs,y_ixs def _training_arrays(self): X_ixs,y_ixs = self._index_windows() time_feat = self._time_features(self.time_attrs).as_matrix() X = np.concatenate((np.array([self.data[w] for w in X_ixs])[:,::int(pd.Timedelta(hours=1)/self.data_freq)], np.array([time_feat[w] for w in y_ixs[:,0]])), axis=1) y = np.array([self.data[w] for w in y_ixs]) return X,y  You can pass in a list of time attributes that you want to use as features and the object will appropriately pull those features from the training data index, combining them with the downsampled historical value features.\n","description":"Starting on a model class","href":"https://mattrossman.github.io/energize-andover-blog/post/day-47/","title":"Day 47"},{"basename":"day-46","content":" Scrutinizing yesterday\u0026rsquo;s approach The method yesterday took a quantile approach at creating a prediction interval. I read the blogger\u0026rsquo;s explanation a bit further and I finally have some trust in his approach.\nThis statement still bothers me though:\n A prediction interval is an estimate of an interval into which the future observations will fall with a given probability. In other words, it can quantify our confidence or certainty in the prediction\n I have my doubts about how well that concept is represented by the method they propose. future outcomes, however I don\u0026rsquo;t know that this is the case. I would imagine that each tree is more or less following a similar path and is not capable of predicting the full range of possible response values. What if none of the trees are able to predict a certain response value?\nAlso you can only be 100% confident in an interval that spans the entire y-axis, while this method will give you a much narrower region that only spans the range of values predicted across all the trees.\nI just found this from the community scikit-learn docs that claims to add confidence interval calculations to scikit-learn RF through a module called randomforestci.\nThe official PyPi release of randomforestci gave me an error when I tried running fci.random_forest_error() because the documentation online is for the development version (v. 0.2), different from the official release (v. 0.1). The order of the arguments was different, so I instead installed the development version.\nHowever I was still getting errors, this time from NumPy function calls. The problem seemed to stem from the fact that I have multi-dimensional output.\nTo the source code I looked through the source code and found the area that was throwing the error. By manually running bits of the function line by line I was able to narrow down the root of the issue.\nI modified this section:\npred = np.array([t.predict(X_test) for t in forest]) pred = pred.reshape(pred.shape[0],-1).T  Previously it was creating a 3D matrix (retaining the daily seperations) but I removed the seperations and just created a stream of power values (kind of like how I ravel() the predicted values before I plot them).\nTo do this I copied the random_forest_error() function from the source code, prepended fci. to its internal function calls and edited the region I showed above.\nNow the code will run properly, but I think something is still wrong. At some points the variance it reports is negative which is illogical. While reshaping the array might have stopped the computational error, I probably caused some of the calculations to become invalid. I don\u0026rsquo;t know enough about their calculation technique to tell for sure what part is invalid. It\u0026rsquo;s probably not worth trying to fix their implementation. In the meantime I opened an issue for it on the GitHub source.\nConfidence intervals vs Prediction intervals It\u0026rsquo;s important to differentiate between the two. This page does so in an understandable way. Prediction intervals are broader in that they account for both variance in the sample and variance in the predictions (as the page states, prediction intervals are larger than confidence intervals). I think for my purpose, prediction intervals would be more appropriate.\nTweaking the old method One thing I wanted to quickly verify was how representative the trees were of all possible predictions. I increased my forest size to 500 and plotted the distribution of predictions for a single output value (first element of the first output):\n  By no means a perfect normal distribution, but it looks better than the sparse distribution of my smaller 50 tree forest. I know that the parent RF estimator returns the mean of these values, so I could take their standard deviation as a way to estimate the population variance and assume normal distribution. Or could I continue with the percentile-based approach. The benefit of the newer method is that it has no size limit so it\u0026rsquo;s fairer at large prediction intervals (i.e. 100% prediction interval is infinite)\nHere\u0026rsquo;s the full threshold calculation:\npred = np.array([t.predict(X_test) for t in est]) sd = np.std(pred,0).ravel() quantile = 0.95 z_max = norm.ppf((quantile+1)/2) y_thresh = y_pred + z_max*sd  And here\u0026rsquo;s a resulting plot with 100 trees:\n  It\u0026rsquo;s a bit different from the plot I had yesterday (the threshold has moved up in some areas). I\u0026rsquo;m satisfied with it for now.\nIn the future I may want to consider standardizing the residuals so that when I report the potential energy savings it\u0026rsquo;s not overinflated in areas of high variance.\nStarting the abstraction My scripts have all been very specific but I want to start generalizing the functions.\nI spent the rest of today laying some groundwork for a function to create the RF model:\ndef make_model(target_vals,target_features,input_size,gap_size,output_size): n = len(target_vals) ixs = np.array(range(n)) ix_windows = egz.rolling_window(ixs, input_size + gap_size + output_size, output_size) X_ixs,_,y_ixs = np.split(ix_windows,[input_size,input_size+gap_size],1) X = np.concatenate((np.array([target_vals[ixs] for ixs in X_ixs])[:,::int(egz.pp_day/24)], np.array([target_features[ix] for ix in y_ixs[:,0]]), ),1) y = [target_vals[ixs] for ixs in y_ixs] est = RandomForestRegressor(n_estimators = 50) est.fit(X,y) return est  I was trying to figure out how to simplify the creation of the feature matrix. Since there\u0026rsquo;s two main sections (part based on previous results and part based on properties of the target day) I split up the parameters, one for the historical/output data (target_vals) and one for extra feature properties (target_features). I would just call them vals and features but the values have a dual usage, partly as features and partly as training targets.\nThe setup could be used alongside a dataframe containing properties about each power entry (e.g. occupancy, time of day, month, etc.) although most of those properties could be calculated at runtime rather than storing them in advance.\n","description":"Changing the prediction interval approach, starting abstraction of the training process","href":"https://mattrossman.github.io/energize-andover-blog/post/day-46/","title":"Day 46"},{"basename":"day-45","content":" Presentation This morning Jordan, Peter and I presented our work to a group of AHS electricians and Janet. We got some valuable feedback about how we can best serve their needs.\nI was pleased to hear that somewhere out there is occupancy information for the thermostats, which I could tap into and use as a replacement for my current holiday/half-day variables.\nPutting the model to use I\u0026rsquo;ve gotten the model to a point that I think it could be useful. I want to shift from the static sample file that I\u0026rsquo;ve used all summer to using the most up-to-date data so I can start making real predictions. I\u0026rsquo;ve asked Anil for help on this, as I don\u0026rsquo;t know where all of the updated data is stored.\nThat\u0026rsquo;s the next big focus for me. In the meantime I can play around a bit more with the sample data (maybe try it out on Bancroft). I can also start laying the groundwork for the prediction database.\nMore intuitive window Currently the 15-minute time interval is hardcoded into the application (I specify 4 points per hour, i.e. 96 per day). This may not always be the case for certain meters or datasets. I can generalize the points-per-day calculations with:\ndata_freq = '15 min' pp_day = int(pd.Timedelta('1 day') / pd.Timedelta(data_freq))  I would also use data_freq to resample the data at the start and if needed cut off incomplete days.\nThen I can incorporate pp_day into my window calculations:\ninput_size = pp_day*7*4 gap_size = pp_day output_size = pp_day  That way if the situation requires a different frequency of data points I only need to change data_freq (which is an easily understood string).\nDo I need a testing set? In a realtime context I\u0026rsquo;m not sure how to handle model validation. If I continue doing things the way I have so far (splitting the data into a training/test set) then the most recent data will be used for testing rather than training. That means if there is a recent change in power usage which would have a significant impact on predictions, the model would be delayed at learning it.\nI think that for RF, validation isn\u0026rsquo;t as important since it already reduces variance as an ensemble of random decision trees. I might just ditch it altogether, which leads to my next difficulty\u0026hellip;\nRethinking the threshold Without a testing set I can\u0026rsquo;t estimate the RMSE of the predictions, which is how I was setting my threshold. I\u0026rsquo;m not sure if this is the best approach for setting the acceptable regions for power values.\nI googled \u0026ldquo;random forest standard deviation\u0026rdquo; and found this page that talks about prediction intervals. What\u0026rsquo;s different about this approach is that it doesn\u0026rsquo;t require that you know the true values ahead of time, it just uses information about discrepancies between the predictions of each of the forest\u0026rsquo;s trees.\nI quickly tried implementing it and had some trouble, because it appears to only work for single dimensional output.\nTweaking the implementation It wasn\u0026rsquo;t too hard to fix, I pretty much just had to specify the axis of the percentile calculation:\ndef pred_ints(model, X, percentile=95): err_down = [] err_up = [] for x in range(len(X)): preds = [] for pred in model.estimators_: preds.append(pred.predict([X[x]])[0]) err_down.append(np.percentile(preds, (100 - percentile) / 2. , axis=0 )) err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2. , axis=0)) return np.array(err_down), np.array(err_up)  This calculates a unique prediction interval for every data point (not just for each day) based off the inner $X\\%$ of the sub-trees\u0026rsquo; predictions.\nHere\u0026rsquo;s how it changed the plot:\n  You can see that now the threshold is dynamic across time. In some cases it\u0026rsquo;s more forgiving of the actual values, such as 3\u0026frasl;11-3\u0026frasl;12. This more fairly accounts for areas where the model is unsure of its results as opposed to setting a constant threshold height.\nI still want to look at other sources and do more research before I fully commit to this threshold approach.\n","description":"Presentation, generalizing data frequency, quantile-based prediction intervals as threshold","href":"https://mattrossman.github.io/energize-andover-blog/post/day-45/","title":"Day 45"},{"basename":"day-44","content":" Adding features Index windows In order to avoid doing extra work I wanted to switch to using value indexes for my windows as opposed to rolling across the raw data. This way I can reuse the indexes of these windows later to pull other information from the same sections of the data (things like day of the year, day of the week, etc).\nHere\u0026rsquo;s an example of adding day of the week features (about the target values) to my input array.\nixs = np.array(range(len(data))) ix_windows = egz.rolling_window(ixs,input_size+gap_size+output_size,output_size) def index_data(ixs_arr,data): return np.apply_along_axis(lambda ixs: data[ixs],0,ixs_arr) X_ixs,_,y_ixs = np.split(ix_windows,[input_size,input_size+gap_size],1) X = np.concatenate((index_data(X_ixs,data),index_data(y_ixs,data.index.dayofweek)),1) y = index_data(y_ixs,data)  This had a minor improvement to my model (Median APE from 0.149 -\u0026gt; 0.146).\nI added in some more features including whether or not the building is occupied:\nocc = pd.Series(0,data.index) occ[occ.index.intersection(egz.df_school.index)] = 1 X_ixs,_,y_ixs = np.split(ix_windows,[input_size,input_size+gap_size],1) X = np.concatenate((index_data(X_ixs,data),, index_data(y_ixs,occ), index_data(y_ixs,data.index.dayofweek), index_data(y_ixs,data.index.month)),1)th)),1)  This brought the Median APE down to 0.139. The occupancy implementation is flawed, as I\u0026rsquo;m supposed to be feeding the model completely true information. However, I only have school session information starting in September of 2016, so I\u0026rsquo;m lying about the occupancy in the 2015 and early 2016 data. Additionally, there are times when the school is occupied but school is not in session. I may be better off simply having a variable set for the entire day, 1 if that day is a holiday or 0 if not (and another variable for half days).\nI implemented that as follows:\nno_school = pd.Series(0,data.index) no_school[np.where(np.in1d(data.index.date, (egz.time_filter( no_school,include=(egz.no_school +['2/9/17','2/13/17','3/14/17','3/15/17'] ))).index.date))[0]] = 1 half_day = pd.Series(0,data.index) half_day[np.where(np.in1d(data.index.date, (egz.time_filter( half_day,include=egz.half_days)).index.date))[0]] = 1  Note: to be more strict about the data I\u0026rsquo;m using, I\u0026rsquo;m currently just testing/training on sets of the data within the \u0026lsquo;16-\u0026lsquo;17 school year (so that I\u0026rsquo;m not ignoring the academic calendar for the rest of the data)\nTesting the features I spent a bunch of time just trying different combinations of features on the data and seeing what got the best results.\nI actually found that after adding some more features, the model performs significantly better when predicting an entire day of data rather than just an hourly chunk. I\u0026rsquo;m keeping the gap at 1 day and the input at 4 weeks. Also, I noticed a slight improvement after downsampling the input data to hourly intervals.\nYou\u0026rsquo;d think that adding all possible features (giving the model the most data to work with) would always increase the accuracy, but that is not always the case as I saw last time. Additionally, some changes would appear to make a better fit and would decrease the MAPE but would actually increase the median APE. For instance, when I realized that I wasn\u0026rsquo;t adding the extra snow days to my no_school array my median APE went from a solid 0.11 up to 0.13, while the MAPE went from 0.19 to 0.18. I\u0026rsquo;m going by the assumption that an improved MAPE trumps an improved median APE since it\u0026rsquo;s important to account for those outlier days.\nAdding the half-day variable somehow made both the median and mean APE perform worse so I\u0026rsquo;m excluding that info for the moment.\nA note on randomness I was surprised to see that changing the order of the features affected the MAPE and median APE scores.\nI have been setting random_state (the seed for the random forest) at a constant value accross trials to try to be consistent with my scoring. I guess changing the features in turn changes the way the seed behaves, so my scores comparing the features are not really consistent since they\u0026rsquo;re coming from different trees.\nFor instance, maybe adding the half day variable doesn\u0026rsquo;t actually hurt my model in the long run, it\u0026rsquo;s just that that particular tree performed worse than the previous one.\nI\u0026rsquo;m wondering if there\u0026rsquo;s a built-in way to run numerous trials and create a confidence interval for the score values so I can more fairly judge my features.\nCurrent feature set I added in a couple more metrics (things like mean of past 4 weeks, min and max, etc) which may just be presenting redundant information but it didn\u0026rsquo;t seem to hurt the model either. Here\u0026rsquo; the feature construction at the moment:\nX = np.concatenate((np.std(index_data(X_ixs,data),axis=1).reshape(-1,1), np.median(index_data(X_ixs,data),axis=1).reshape(-1,1), np.min(index_data(X_ixs,data),axis=1).reshape(-1,1), np.max(index_data(X_ixs,data),axis=1).reshape(-1,1), index_data(y_ixs,no_school), index_data(y_ixs,half_day), index_data(y_ixs,data.index.weekday), index_data(y_ixs,data.index.dayofyear), index_data(y_ixs,data.index.hour*60+data.index.minute), index_data(y_ixs,data.index.month), index_data(X_ixs,data)[:,::4]),1)  Playing with the hyperparameters So far the only hyperparameter I changed from the default is max_features. I actuallly read that for regression problems it\u0026rsquo;s better to leave this as the default of $n_{estimators}$. The other significant parameter I\u0026rsquo;ve ignored is n_estimators which defaults to 10. I upped this to 40 and got significantly lower variation across random trials.\nMetrics To detect anomalies I\u0026rsquo;m going with the RMSE as an estimate of the predictions\u0026rsquo; standard deviation from the true values. I\u0026rsquo;ll then use $2\\sigma$ as a threshold for acceptable values. Sklearn can do the MSE for you with sklearn.metrics.mean_squared_error, so I just have to take np.sqrt of that value and multiply by 2.\nUpdate on the plot Here\u0026rsquo;s how the model looks with its latest upgrades:\n  I showed off this section of the test data since you can see how it handles school days, weekends, anomalous behavior, and holidays.\nYou also might notice that the x-axis is actually readable now, as the units track the dates. I did this by creating another set of arrays:\ndates = index_data(y_ixs,data.index) dates_train,dates_test = np.split(dates,[num_train])  and setting my axis to dates_test.ravel().\nI am very pleased with how it is looking currently. It handles weekends and holidays very well. The predictions are fairly consistent and look reasonable. The only hitch at this point is having to manually specify holidays and half days. I don\u0026rsquo;t know if clustering will help us take care of that automatically or if we\u0026rsquo;ll just have to settle for importing an .ical file.\nA couple last notes I was playing around with the input/gap/output parameters with the new features implemented. I noticed that increasing the gap size doesn\u0026rsquo;t have a huge impact on the model as it did before. That gives us some flexibility with how far we want to predict ahead of time (e.g. if we want to have our predictions a week or more ahead of time, that\u0026rsquo;s probably doable)\nMore shockingly, I removed the previous power usage feature entirely and even that had minimal impact on the fit. That could potentially speed up training significantly. I\u0026rsquo;ve spent enough time fiddling with parameters today. Tomorrow I\u0026rsquo;ll be presenting some of the work we\u0026rsquo;ve done so far with some other team members so I\u0026rsquo;ve got to prepare the presentation for that.\n","description":"Index windows, more input features (holidays, descriptive stats, time data), trying non-default hyperparameters, flagging anomalies","href":"https://mattrossman.github.io/energize-andover-blog/post/day-44/","title":"Day 44"},{"basename":"day-43","content":" Setting up the input/output Cluttered approach First I split the data up into hourly chunks with the rolling_window method. Then I run a 2D rolling window to pull enough hours to include both my predictor value and response values. Note that there\u0026rsquo;s extra data in the middle. I drop this middle section and assign the X and y arrays via np.split(). Then I reshape the arrays so that the hours are no longer divided. I\u0026rsquo;m using the rolling_window method that I mentioned yesterday (using strides) and a 2D version that doesn\u0026rsquo;t use strides as defined here.\ndata = egz.df_energy['Main (kW)'] bins = egz.rolling_window(data,4,4) windows = egz.rolling_window2D(bins,24*7*5+1) X,_,y = np.split(windows,[24*7*4,24*7*5],1) X = X.reshape(X.shape[0],-1) y = y.reshape(y.shape[0],-1)  Streamlining it I can declutter this process by getting rid of the step where I break into hour-long bins (although that section did make it a bit easier to follow):\nwindows = egz.rolling_window(data,4*(24*7*5+1),4) X,_,y = np.split(windows,[4*24*7*4,4*24*7*5],1)  See below for a more readable edit\nThe result Now my X array of 4-week predictor values starts at 00:00:00 on 8/5/15 and my y array holds the hour-long predictions 5 weeks from then starting on 9/9/15 at 00:00:00.\n4 weeks of prediction data, 1 week gap, 1 hour of target data.\nNull values Apparently sklearn doesn\u0026rsquo;t like when you give it null values. For now I\u0026rsquo;ll just fillna(0) although this may impact the accuracy of the model. I\u0026rsquo;d be better off filling in values from the week before to give a better estimate of the actual usage that occured then (or average the value the week before with the interpolated values to also reflect that day\u0026rsquo;s context).\nMaking predictions Here\u0026rsquo;s a really big plot of all the test samples compared to their target values. Since the predictions are each hour-long bins of data, I plotted them by raveling the predicted array.\nClick the image to expand it to full size:   Measuring the fit I tried measuring the Mean Absolute Percent Error to measure the fit, however the outlier days when the model didn\u0026rsquo;t know that there was no school threw this measure off entirely (it was over 4 billion percent).\nTo reduce the impact of these outlier days I instead measured the Median Absolute Percent Error which gave a much more reasonable value. Note the addition of a 1e-9 term so that the system doesn\u0026rsquo;t try to divide by zero.\nnp.median((np.abs(((y_test+1e-9) - (pipe.predict(X_test)+1e-9))) / (y_test+1e-9))) \u0026gt;\u0026gt;\u0026gt; 0.16049520499889364  To put this in comparison, the MIT report from yesterday measured a MAPE of about 0.12 for RF and 0.14 for ANN, so we\u0026rsquo;re not far off. And this is only trained on data about previous consumption, so it doesn\u0026rsquo;t have information about holidays yet.\nChanging variables I tweaked the ranges of some of the window variables to see how it affected the predictions. To do so I added some more variables which also makes the assignment more readable:\ninput_size = 4*24*7*4 gap_size = 4*24*7 output_size = 4 windows = egz.rolling_window(data,input_size+gap_size+output_size,output_size) X,_,y = np.split(windows,[input_size,input_size+gap_size],1)     Input Gap Output Median APE     4 weeks 1 week 1 hour 0.1605   1 weeks 1 week 1 hour 0.1718   4 weeks 1 week 1 day 0.1597   8 weeks 1 week 1 day 0.1717   8 weeks 1 day 1 hour 0.1439   8 weeks 1 day 1 day 0.1569   4 weeks 1 day 1 day 0.1593   4 weeks 1 day 1 hour 0.1325   1 weeks 1 day 1 hour 0.1526   1 day 1 hour 1 hour 0.0794    Note that due to the random nature of the model, the accuracy is subject to change across trials (but as an ensemble method it should be somewhat consistent).\nAs expected, the model with the smallest gap (1 hour) had the best accuracy but at that small of a gap the forecast is no longer very useful. From what I can tell, the winner is 4 weeks of input, 1 day gap, 1 hour predicted. That way the forecast will be available a day beforehand. We could also present the user with an option to run a forecast of their desired length, although they would have to wait for the server to process their request.\nBetter handling of null values Instead of replacing the nulls with 0 kW, I can replace them with last week\u0026rsquo;s value with\ndata.fillna(data.shift(-4*24*7))  I actually tried using the 'W' offset alias instead of calculating the week index interval but it wasn\u0026rsquo;t behaving as I expected.\nSlow training The training process is rather slow, even if I drastically reduce the number of samples. It appears the biggest time-suck is the large number of features that need to be tracked.\nThe feature_importances_ property of a trained RF estimator shows the relative importance of your features. Here\u0026rsquo;s what the distribution looks like:\n  As you can see, almost all the features have close to zero importance. Here\u0026rsquo;s the top 10 values of the sorted list:\n   Index Importance     2114 0.335970   2687 0.107384   2115 0.096686   2113 0.050099   97 0.043213   96 0.031255   98 0.024074   2116 0.014845   99 0.011222   2208 0.006476    The importance drops off very quickly. Only a couple of sparse values are significant predictors (the top contenders appear to be from a few weeks in the past).\nBy changing the max_features argument to something like 'sqrt' or 'log2' I was able to increase the training speed at the expense of the median APE (0.13 vs 0.15).\nHere\u0026rsquo;s an example output using 'sqrt' max features:\n  ","description":"Preparing the inputs/output matricies, testing the RF fit with median APE using different input/output/gap sizes, starting improvements to train speed","href":"https://mattrossman.github.io/energize-andover-blog/post/day-43/","title":"Day 43"},{"basename":"day-42","content":" Yesterday I performed a regression using decision trees because it\u0026rsquo;s the most straightforward sklearn estimator that support multiple outputs. It turns out the use case for them is pretty similar to that of artificial neural networks.\nFirst thing to address is decision trees vs random forests. Random forests are an ensemble of decision trees so they create a better, more generalized model (note that in yesterday\u0026rsquo;s examples the decision tree model looked like a better fit, but the random forest had a much better score overall). Decision trees alone tend to overfit.\nNext is random forest (RF) vs artificial neural networks (ANN). It\u0026rsquo;s crazy how perfect some of the research out there is. I did a brief search hoping to learn more about the differences between ANNs and decision trees / random forests and came across this research paper that not only compares them, but does it in the context of forecasting building energy consumption. Too perfect.\nHere\u0026rsquo;s some differences that are highlighted in the paper:\nRF have fewer hyperparameters you need to worry about, and just using the default values gives really good results. ANNs are more ambiguous with their parameters, and it can be unclear how many hidden layer neurons to include (although they do mention a starting point in the article as $n+1$ hidden neurons for $n$ output nodes).\nANN is much slower to train than RF (yesterday\u0026rsquo;s use of RF was already really slow).\nRF can make predictions even when some input values are missing. On the other hand ANN tends to produce more accurate predictions than RF. Nonetheless, both estimators proved to be fairly accurate.\nAnother report Here is another report from MIT that compares some forecasting algorithms. It found RF to perform better than ANN. What I like about this paper is that the authors actually explain the entire prediction process in detail so it is clear what the inputs and outputs are.\nThey train their model every night, and every hour they use the recent model to predict the following 12 hours of data. If I understand this correctly, that means their forecast changes throughout the day as newer predictions override old ones. I was under the assumption that once values were forecasted they would remain unchanged. The benefit of this is that your forecast will adapt to unpredictable situations making it a better indicator of what is to come.\nOn the other hand, if the forecast is update too frequently then there is no accountability; the predicted values are subject to change at any time. In some cases a dynamic prediction makes sense (like weather forecasts). But since we are trying to make an accountable model of power usage it would be better to avoid changing the prediction (at least, avoid changing it very often) so that the user has a chance to see the dissonance between predictions and reality.\nAnother idea: update the forecast throughout the day, but save the old forecast. That way the user can both see an accurate model of the future and see how consistent their prediction is throughout the day.\nWhich one to pick? I\u0026rsquo;m liking the sound of random forests at the moment. It may not perform as well as ANN in many situtations, but it still can do a great job and it sounds like it is much less resource intensive.\nBrainstorming a prediction flow There a a lot of ways the prediction process can be organized. Some things to consider:\n How often to train the model? How often to run predictions? If there is prediction overlap, how is it resolved? How much to predict? (output size) How much to predict off of? (input size) How far ahead to predict?  \nRegarding the last point, so far I\u0026rsquo;ve only been predicting value(s) that immediately follow my input(s). It would probably make more sense to predict farther ahead, both so that your predictions are actually useful and so that you don\u0026rsquo;t run out of forecasted values during the time when the process needs to relearn the model / make new predictions.\nI don\u0026rsquo;t see a lot of value in overlapping predictions. I think it would be reasonable to predict either a full day of data or 24 hours of data at some point in advance. For instance, predicting an hour of data a day in advance. Or predicting a day of data 2 days or a week in advance. A shorter gap between prediction/actual would likely make a more accurate prediction. A longer gap (or a longer prediction) means farther predictions. A longer predictions means slower calculations. In the realtime anomaly detection report, the model predicts 1 day of data a week in advance based on the previous 4 weeks of data.\nI\u0026rsquo;ll have to try a few different methods and find a reasonable compromise between accuracy and performance.\nCode For the sake of consistency between Pandas and non-Pandas list structures, I\u0026rsquo;m calling\negz.df_energy['Main (kW)'].resample('15 min').asfreq()  so that the intervals between values are proportionate (otherwise missing values will cause calibration issues with the model\u0026rsquo;s timeline). I haven\u0026rsquo;t forgotten that I\u0026rsquo;m calling dropna() before this on df_energy. I\u0026rsquo;m dropping the null values (which are not all at 15-minute intervals), then reintroducing the null spaces where appropriate.\n  Here\u0026rsquo;s the autocorrelation plot for part of the data. The initial oscillation shows significance up to lag 30 (which is 8 hours of time).\nTruncating the start of the sample Right now my data starts in the middle of a day. I want to have nice windows that line up on the hour, so I\u0026rsquo;ll cut off the days that are incomplete as follows:\ndata.groupby(data.index.date).filter(lambda x: x.size==96)  There should be 96 15-minute intervals in a complete day. Note that this is not going to exclude days with missing values since I filled those spots with null placeholders.\nNow my data starts at midnight on 08/05/15.\nSplitting it into bins Last time I made my bins from a rolling window, but that takes a lot of processing power when iterating over a lot of data. Also it\u0026rsquo;s unecessary for my training since I will only be predicting at the same intervals as my bin sizes (i.e. there\u0026rsquo;s no need to train the model how to make a forecast in the middle of an hour).\nThe grouping can be done with some nifty Python code:\ngroups = np.array(list(zip(*[iter(data)]*4)))  Read this for a breakdown of how the zip(*[iter(data)]*n) step works. It\u0026rsquo;s pretty clever.\nOn second thought I\u0026rsquo;m going to need to use sliding windows anyways (to get the selections from these hourly groupings). Luckily there\u0026rsquo;s a much more efficient way to do it in Numpy as described here. It gets into some hiden functions that access NumPy\u0026rsquo;s underlying C code. That post in particular provides a function for getting rolling windows with a desired window and step size, so I can set my step size to be 4 for an hour. That means I can replicate the behavior of the above code with\nstrided_app(data.values, 4, 4)  ","description":"Random forest vs ANN, forecast flow, more efficient window calculation with NumPy strides","href":"https://mattrossman.github.io/energize-andover-blog/post/day-42/","title":"Day 42"},{"basename":"day-41","content":" Multi-step forecasting I quickly corrected it after posting, but I mistakenly thought I had struck gold last time when working with lagged features. In reality, I was violating the rules of predictions by including data about the future in my testing inputs (I was performing a bunch of one-step-ahead forecasts which made it look like my overally forecast was really strong).\nWhen I tried recursively generating lagged predictions for my inputs, I got a much weaker model. In fact, I found that the lag features only hurt my overall model compared to solely accounting for current conditions. At the moment I don\u0026rsquo;t have a whole lot of hope left for the multiple regression approach, but the problem may lie in my naive attempt at multi-step forecasting. This page explains a few different techniques (I was using the recursive one).\nI\u0026rsquo;m particularly interested in the final method (multiple output approach) which uses a model which predicts an entire span of time rather than a single prediction value. It does so by training the model on multi-dimensional outputs, something I\u0026rsquo;ve not yet tried. For instance, I could train a model to predict the entire next day\u0026rsquo;s power usage based on the past week.\nIn order to do this I\u0026rsquo;ll want an array of rolling window measurements. Unfortunately the Pandas rolling objects don\u0026rsquo;t appear to have a handy way to return their underlying array structures.\nData involved For now I\u0026rsquo;ll only consider lag data, not features about current time. To see how far I can push it, I\u0026rsquo;ll have my input be an array of all of the data for the past 4 weeks, and my output be an array out the data for the next week. To generate this from my regular time series I made a helper function:\ndef train_target_windows(arr, windows, step=1): trains = [] targets = [] for i in range (len(arr) - windows[0] - windows[1] + 1): trains.append(arr[i:i+windows[0]:step]) targets.append(arr[i+windows[0]:i+windows[0]+windows[1]]) return (np.array(trains),np.array(targets))  Here you pass in your starting array arr and a tuple of desired window sizes for your input/output sets. For me this was (4*24*7*4, 4*24*7) (i.e. 4 weeks, 1 week). The step parameter lets you take larger steps between values on the input matrix to reduce the number of features that need to be regressed.\nHere\u0026rsquo;s what I got when training a DecisionTreeRegressor model on windows extracted from 10,000 of the data points:\n  \u0026gt;\u0026gt;\u0026gt; est.score(X_test,y_test) 0.59252701026688193  And here\u0026rsquo;s the same calculation with a RandomForestRegressor:\n  \u0026gt;\u0026gt;\u0026gt; est.score(X_test,y_test) 0.80756960256071542  Not bad. I\u0026rsquo;m just using the parameter defaults here since I don\u0026rsquo;t know enought about these predictors to know what parameters to change. And this time I\u0026rsquo;m not cheating when I make this prediction. That entire week of data is predicted from the previous 4 weeks in 1 calculation, so there\u0026rsquo;s no funny business where I\u0026rsquo;m using future values within the prediction input.\nRight now the code is pretty slow (takes a minute or so to run), mostly due to the slow window generation step. Currently I\u0026rsquo;m making a lot of windows with a lot of unecessary overlap so that\u0026rsquo;s one area I know for sure I could improve.\nRealtime anomaly detection Remarks on the studies I\u0026rsquo;m impressed by the results of Chou \u0026amp; Telaga in their study on realtime anomaly detection. They tested 2 methods, ARIMA and NNAR on two sizes of sliding windows. Overall, the best predictor was NNAR on a 4-week window of training data. Both NNAR tests performed better than the ARIMA tests.\nI was under the impression that we would need a large span of data in order to make an accurate model so it\u0026rsquo;s a nice surprise that only 1 month of prior data may be necessary for training. That also means calculations can run much more quickly.\nOne thing to note is that for their study they had a data stream at 1-minute intervals, while the data I have for testing is at 15-minute intervals. I\u0026rsquo;m not sure how significantly this will affect my results.\nLastly, they used the R language in their study. It may be worthwhile to try this if there are no equivalent functions for the methods they used in Python. The methodology report was most recently revised in January 2014, and the subsequent report using it for an early warning application was revised in January 2017. So the technique had been implemented fairly recently and has stood the test of time.\n","description":"Multi-step forecasting with multiple output regression, overview of the realtime anomaly detection research paper","href":"https://mattrossman.github.io/energize-andover-blog/post/day-41/","title":"Day 41"},{"basename":"day-40","content":" To follow up on a note I made yesterday about the research paper Early-warning application for real-time detection of energy consumption anomalies in buildings, I found another paper published by some of the same authors that goes deeper into the techniques they used.\nHere is another paper that reviews a variety of forecasting techniques for time series data. It\u0026rsquo;s actually incredibly detailed and includes topics like moving averages (similar to the rolling medians I tried early on), SVMs, ARIMA, Artificial Neural Networks, and many more. It lays out the pros and cons of each. It also explains the benefits of using all sorts of hybrid methods.\nHere\u0026rsquo;s a quote from it that stood out to me regarding the use of temperature data in another study:\n It was seen that the treatment of the dataset as pure time series data gave better results rather than considering other factors like ambient temperature to predict the load demand.\n Additionally, here is a whole online book about the process of forecasting data values. It will be a valuable resource no doubt.\nLag features Currently I\u0026rsquo;ve been using regression techniques that rely on variables extracted from a single point in time. I ask the model, \u0026ldquo;How much power will I use on the first Tuesday in January at 9:00am when it\u0026rsquo;s 30 degrees outside?\u0026ldquo;, and it returns a value.\nHowever, another approach is to use a window of previous values as the inputs. These are referred to as lag values. You\u0026rsquo;d ask, \u0026ldquo;How much power will I use next given that I just used these values recently?\u0026rdquo;. One benefit here is that it doesn\u0026rsquo;t require as much data to learn how to make predictions, whereas my previous method would need data across a wide range of scenarios (albeit making a more accurate model).\nAs a really naive example, here\u0026rsquo;s a plot showing the autocorrelated nature of the timeseries data, comparing each power usage value to the one that came before it (this obviously can\u0026rsquo;t be used for prediction):\n  I could even combine lag features with features about the current time.\nMy main stumbling block is deciding which approach to take. Should I keep looking at this as a regression problem with a broader range of features? Should I try a timeseries approach like S/ARIMA/X? Should I look into neural networks? This response suggests that all are valid options.\nEDIT: I just realized that the method I used in the next section isn\u0026rsquo;t really valid for forecasting since it used the lagged actual values in the testing inputs. Instead I think I\u0026rsquo;d have to iteratively predict values and use those predictive values as lagged inputs for future points. \nAmazing results I\u0026rsquo;m a little scared by how good these results are\u0026hellip; I tried adding some lag features into an SVR and got great scores (over 90% $R^2$). I will do a better implementation next time but I\u0026rsquo;m already blown away by what I\u0026rsquo;m seeing. I trained on 4000 data points and tested on the following 1000. Here\u0026rsquo;s what I got:\n  Wow. That looks pretty darn good to me. Before I get too excited I should remember that a perfect fit is not always a good thing. But I like the fact that the prediction is a little under the actual, leaving area for improvement. \nDummy variables I was under the impression that I could just represent variables like day of the week, month of the year, etc., with quantitative values (e.g. 0=Jan, 1=Feb \u0026hellip; ). However, most of the resources I see instead say to use $n-1$ dummy variables for the $n$ options. I don\u0026rsquo;t totally understand why since the intervals are meaningful (the difference between day 1 and day 2 of the week is 24hrs just like between days 4 and 5) but I\u0026rsquo;ll roll with it.\nTo do I whipped up the lag feature implementation pretty quickly, but next time I\u0026rsquo;ll need to do a lot more extensive testing and maybe use some more proper cross validation. But this was a very good way to end the day.\nEDIT: I need to rethink the implementation of lag features.\n","description":"Overview of forecasting methods, very promising results from multiple regression with lag features","href":"https://mattrossman.github.io/energize-andover-blog/post/day-40/","title":"Day 40"},{"basename":"day-39","content":" Last time I noted that the variance in the power vs. temperature residuals fanned out quite a bit at higher temperature values. I\u0026rsquo;m curious if other data sets behave similarly. One thing to consider: is the changing variance truly an issue of randomness, or is there another variable at play here? The more variables you consider, the less you leave up to chance.\nPreviously I was looking at the school-time data, but you can see the same fanning occuring in the night data (where there is also an annoying flare on the right):\n  However, the fanning is much less pronounced in the weekend data:\n  By comparison, here is the weekend data plot for Bancroft:\n  The difference in model shapes is striking between AHS and Bancroft. Relative to the size of its overall load, Bancroft shows much less change in power load across temperatures than AHS. The regression plot is almost a flat line. Note that the Bancroft load values are significantly larger than those for AHS though, so the actual amount of explained variance for Bancroft is larger than it seems.\nAlas, the score for the above Bancroft plot is incredibly poor (it is negative) which is odd because it looks to be doing a decent job. The score method that I\u0026rsquo;m using here finds the $R^2$ coefficient, and a negative value there means that the model is supposedly performing worse than just a line through the mean (on the test points at least).\nToday\u0026rsquo;s distractions Today I spent some time learning more about how kernel functions and Support Vector Machines work. It sounds like SVMs are more commonly used for classification problems, but they can be implemented for regression too. Letting a kernel figure out the implicit shape of my data sounds more elegant than having me arbitrarily pick a maximum number of polynomial features to look for.\nHere\u0026rsquo;s a visualization of how the kernel trick works for classification (which I may use later on for classifying load profiles):\n  \nResearch papers Energy Consumption Forecasting for Smart Meters\nAnomaly detection for visual analytics of power consumption data\nEarly-warning application for real-time detection of energy consumption anomalies in buildings\nThe papers hosted on ScienceDirect I was able to access after logging in through the UMass libraries website. The second article looks particularly promising.\nI\u0026rsquo;m going to need some time to read and digest all the information avaliable here. I\u0026rsquo;ll jot down what sticks out to me.\nAnomaly detection for visual analytics of power consumption data One approach this paper mentions is weighted predictions, where recent samples have a stronger influence on future predictions than older samples. This would be helpful because it makes the prediction more dynamic as the building undergoes changes in usage.\nThe actual predictive approach this takes is somewhat basic - they stratify historical data by time of day and day of the week and take the average of each pool to create a bank of expected values. That is, it assumes each time of day will behave the same way it did last week and weeks before. To score values by their abnormality, they use the ratio of the difference between an observed and expected value to the average of all such differences for the data.\nThe second approach is not as clearly explained, but it tracks what power load patterns are most common and ranks days by how similar they are to the frequently-occuring patterns. This doesn\u0026rsquo;t explicitly account for variables like temperature, but since it looks for pattern similarity it accounts for a varying load profile for buildings with complex schedules (such as a school where you have weekend patterns, snow day patterns, school day patterns, etc.)\nThe rest of the paper is focused on the effectiveness of a variety of visualization techniques, such as dynamic blurring and interval/ratio color palettes.\nEarly-warning application for real-time detection of energy consumption anomalies in buildings This paper aims to create a user friendly realtime system of anomaly detection and monitoring. The visualizations included in the paper look really good. They are clean and simple, showing a plot of predicted usage versus the actual day\u0026rsquo;s usage. The predictions are saved from the week before and they are calculated using a method described in a different paper, which is a hybrid of a modified Support Vector Regression algorithm and a Seasonal ARIMA algorithm.\nIt\u0026rsquo;s unfortunate that the former paper doesn\u0026rsquo;t go a little deeper into its prediction technique, as the latter paper is a bit too dense to skim over.\nEnergy Consumption Forecasting for Smart Meters The technique used in this paper is Boosted Decision Tree Regression. It is an example of an ensemble method, which is a method that combines the results of a number of base estimators to get a more generalized prediction. The paper is quite readable.\nThere\u0026rsquo;s a lot to unpack here and many different approaches to our problem. It\u0026rsquo;ll be tough to narrow down the best option. On the bright side I think I\u0026rsquo;m reaching the light at the end of the tunnel where I\u0026rsquo;ve seen most of what these machine learning libraries have to offer (even if I don\u0026rsquo;t completely understand what they all do yet) so at least the constant train of discoveries should slow down. I\u0026rsquo;ll read more into each author\u0026rsquo;s argument for their model and hopefully come up with a \u0026ldquo;best\u0026rdquo; approach in the near future. Scikit-learn makes it easy to do the code, but picking the best technique is really tough.\n","description":"Research papers on power usage forecasting, residual plots of more data sets","href":"https://mattrossman.github.io/energize-andover-blog/post/day-39/","title":"Day 39"},{"basename":"day-38","content":" Today is a shorter post as I have to leave this afternoon.\nTesting the cross validated results Last time I noted that the initial scores I was seeing didn\u0026rsquo;t appear high. Today I\u0026rsquo;m going to look deeper into how to interpret the results. I think last time I was just looking at night data but today I\u0026rsquo;m going to use the school data (which gets an even lower score on the testing set, below 30%)\nPlotting the residuals The first area I should look is the residual plot to see if there are any unwanted trends\n  I see a couple of things here:\n There\u0026rsquo;s more variance for the higher temperature values The testing residuals are mostly negative  \nThe latter issue could just be poor luck (more data would help create a stronger prediction on future test points). The former could be a real problem.\nNonconstant variance My analysis of the residual distribution (i.e. how I\u0026rsquo;m going to tell which points are too far from the trend) relies on the assumption that the variance is independent of the explanatory variable. I wouldn\u0026rsquo;t want to end up solely flagging summer data points as abnormal just because there is more variance on hotter days.\nThis presentation looks at the effects of nonconstant variance, also called heteroscedasticity. Heteroscedasticity violates the assumptions of OLS, but the presentation notes that using a more robust estimator such as Huber regression makes this less of an issue.\nSince my residuals \u0026ldquo;fan out\u0026rdquo; in a predictable fashion I could apply a transformation to the response variable to make the variance more constant. My concern is, what if the heteroscedasticity is not so predictable (e.g. what if it oscillates between low and high variance rather than a constant increase)?\nI have to cut this post short as I\u0026rsquo;m leaving for a weekend trip. Next week I\u0026rsquo;ll be back to continue where I left off.\n","description":"Quick look at the cross validated residual plot","href":"https://mattrossman.github.io/energize-andover-blog/post/day-38/","title":"Day 38"},{"basename":"day-37","content":" Side note I found this neat flowchart on the scikit-learn site that guides you through picking an estimator. It basically guided me down the path I expected (on the regression side of things), although it brought to my attention that it may be better to use a Lasso or ElasticNet-based estimator (Huber is closer to Ridge) due to its preference for feature sparsity. That may be beneficial once I start working with mugh higher dimensional inputs. I\u0026rsquo;ll worry about that later since I finally got the robust Huber estimator to work.\nCross validation The purpose of validatation and cross validation is to generalize your model making it a better predictor of future unknowns. The danger of simply fitting a model to you whole data set is that it can create a highly fitted model that works very well at predicting the exact sample set you trained on, but it may be bad at predicting values outside of your training set.\nIn general you want to break your data into training, validation, and testing sets. It can be confusing at first but this answer does a nice job of explaining the unique purpose of each division.\nCross validation more specifically has the advantage of not wasting any of your data since you recycle data from your training/validation sets across iterations. More info here in the scikit-learn documentation on cross validation.\nI also found this post to be a very helpful breakdown of to cross validation procedure in sklearn specifically.\nTime series validation The standard iterative cross validation method in sklearn is k-fold, however it assumes the data are independent and have a consistent distribution as noted here.\nThis is the documentation\u0026rsquo;s recommendation on splitting time-series data. The goal is to always evaluate performance on future data (i.e. data that is least like your training data). This also means I can\u0026rsquo;t use the standard train_test_split because it performs a ShuffleSplit on the data (the documentation warns against shuffling samples that aren\u0026rsquo;t independent).\nTime Series Split The (only) iterator for timer series data in sklearn is TimeSeriesSplit (documentation). This maintains the consecutive nature of the time series data, and only evaluates model performance on \u0026ldquo;future\u0026rdquo; data points.\nExample definition: cross_val = TimeSeriesSplit(n_splits=5)\nPicking hyperparameters To pick the optimal hyperparameters via validation I can use GridSearchCV. I was having a little trouble figuring out how to make this work with my pipeline, but basically the GridSearchCV object becomes the estimator that you finish off the pipeline with. So I wrapped my Huber estimator with the grid search and passed in my cross_val iterator and my dictionary of parameters. I also set iid=False since the data is from a time series, thus it is not identically distributed across my CV folds.\nFor the aforementioned dictionary of parameters I created the following items:\nalphas = np.logspace(-6, 1, 8) epsilons = np.linspace(1.00001, 2, 5) params = {'alpha':alphas, 'epsilon':epsilons}  (Epsilon is required to be larger than one).\nThen I can run the cross validated fit with pipe.fit(X_train,y_train) where pipe is my pipeline\nPoor score Unfortunately I\u0026rsquo;m not getting great scores right off the bat. Right now I\u0026rsquo;m performing an 80\u0026frasl;20 split on my sample data into training/testing sets as follows:\n_train_size = 0.8 _train_cutoff = int(_train_size*X.size) X_train, X_test = np.split(X,[_train_cutoff]) y_train, y_test = np.split(y,[_train_cutoff])  In accordance with the specifications from the sklearn documentation on time series data, I\u0026rsquo;m only testing on future data and maintaining the consecutive order of data points. My cross validation iterater does the same.\n\u0026gt;\u0026gt;\u0026gt; pipe.score(X_test,y_test) 0.73681231517651635  That\u0026rsquo;s not a terrible score but most of the examples online are above 90%. I\u0026rsquo;m running out of time for now but next time I\u0026rsquo;ll look into what the default scoring method actually returns / how to interpret it. I also should see if there are other parameters I could optimize since I just arbitrarily picked alpha and epsilon.\n","description":"Cross validation intro","href":"https://mattrossman.github.io/energize-andover-blog/post/day-37/","title":"Day 37"},{"basename":"day-36","content":" Normalization strikes again I thought I had found the solution yesterday to my normalization problem since the plot looked much better. Today I tried running the whole plot (with the residuals and everything) where I saw my first red flag - the residuals were not displaying properly. After some time debugging, I noticed that my regression\u0026rsquo;s predict function was returning different values for the same inputs depending on how many samples you passed in - not good!\nWhen using the normalize parameter this behavior does not occur. It makes sense why the issue is happening right now - I\u0026rsquo;m normalizing along the columns so as I add more columns (one for each input) the will all have to scale down accordingly. As to why I didn\u0026rsquo;t notice this before, it\u0026rsquo;s likely because I plot my regression line as 100 uniform samples, which is close to the 114 samples I used to create the model.\nTo get to the bottom of this I went to the scikit-learn source code and started a scavenger hunt for how the normalize parameter is implemented. It led me to this _preprocess_data function in the parent linear_model class. Maybe I could manually call this function to perform the appropriate normalization procedure for the Huber regression?\nI copied the Ridge regression\u0026rsquo;s call to _preprocess_data:\nself._preprocess_data( X, y, self.fit_intercept, self.normalize, self.copy_X, sample_weight=sample_weight)  Then I replaced the references to self with the variable for my Ridge object and X and y with my actual data variables, and it spat out some tuples so I hope it worked. I then tried the same process with a Huber estimator, and at first it gave an error because the estimator has no copy_X attribute. So for the attributes that don\u0026rsquo;t exist for Huber estimators I went to the Ridge documentation and replaced them with Ridge\u0026rsquo;s default values (since I just want to replicate Ridge\u0026rsquo;s behavior). The Ridge default is True which matches the _preprocess_data default, so I can actually just delete that argument altogether. Finally I forced normalize=True. It successfully returned the same tuple as the Ridge object did. Remember that this is just a preprocessing step, so there\u0026rsquo;s no reason for the two estimators to return different values yet.\nThe items it is returning are X, y, X_offset, y_offset, and X_scale. Unfortunately the Huber estimator doesn\u0026rsquo;t use those parameters, so I\u0026rsquo;d have to somehow implement the manually. This seems like far too complex of a solution for what should be a simple problem. I\u0026rsquo;ve posted a help thread on a stats forum for a recommended solution so I don\u0026rsquo;t waste time trying to solve this on my own.\nThe real solution(?) I\u0026rsquo;m hestitant now to claim to have the \u0026ldquo;solution\u0026rdquo;, but I got a response to my post that seems to mostly fix my issue. It comes down to how you define normalization. In some contexts it means the scaling a vector to unit length (as the Normalizer() object and normalize() methods do), but in this case it\u0026rsquo;s referring to scaling values to have unit variance and mean of zero.\nTherefore I should be putting StandardScaler() in my pipeline rather than Normalizer(). It doesn\u0026rsquo;t completely replicate the output of the normalize parameter but it\u0026rsquo;s pretty close and can be adjusted by the estimator\u0026rsquo;s parameters.\nNow I get consistent predictions regardless of how many items I predict on.\nHere\u0026rsquo;s some plots comparing the Huber regression to Ridge, note that they look more similar at a higher $\\epsilon$ value.\n    And here\u0026rsquo;s how the residual distribution looks:\n  It may look similar to the Ridge plot, but if you throw in some outliers you\u0026rsquo;ll quickly see the difference.\nRobust Scalers As another followup to my post, I learned that there is a RobustScaler that as the name implies, uses a more robust method for scaling based of interquartile ranges. I also could potentially make my own scaler using the scaled MAD as a unit of variance. From my brief testing, these would take some debugging to implement properly (so far they make the fit worse).\nTo do Tomorrow I will be catching up with Anil at the library. I\u0026rsquo;ve made decent progress with picking an estimator, next on my agenda is to focus on cross validation.\n","description":"Fixing yesterday's normalization misuse","href":"https://mattrossman.github.io/energize-andover-blog/post/day-36/","title":"Day 36"},{"basename":"day-35","content":" EDIT: as a disclaimer, this post\u0026rsquo;s use of normalization is flawed. The subsequent post revises this error.\nHow much error is acceptable? Before I get much further refining the regression technique, it\u0026rsquo;d be good idea to spend some time figuring out how I\u0026rsquo;m going to end up using it. The obvious use of regressions is to allow me to find an expected value given certain arguments, but I\u0026rsquo;m not expecting the sample to fit these predictions exactly. Considering the small random variance in power usage under a given set of parameters, you can assume the residuals should follow a consistent distribution.\nHere\u0026rsquo;s a look at the distribution of residuals in a Ridge regression of the power versus temperature data during school hours\n  Distributions to Consider The usual assumption is that the errors (and residuals) should follow a normal distribution.\nPreviously, I went with the assumption that our response variable follows a lognormal distribution due to the lower bound of power usage. If I continued that assumption, then I think I would have to log-transform all the data before the whole fitting / residual calculation process, and from then on I could treat it as normal.\nI also might want to consider a Student\u0026rsquo;s t-distribution in stratifications of the data where there are less than 30 data points. Here I have 114 so it is probably not necessary.\nI think for now I\u0026rsquo;ll stick with assuming normality since it\u0026rsquo;s the most straightforward. There may be negative implications of log-transforming the regressors that I haven\u0026rsquo;t considered.\n  Here I added shaded regions to highlight the area that is predicted to enclose 95% of the data assuming normality.\nNote that $\\hat{\\sigma} = \\sqrt{ \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y})^2 }$, or in other words the variance equals the mean sum of square errors of the residuals. $\\hat{\\mu}=0$ since in an ideal normal distribution the positive and negative residuals would all cancel out.\nAt first glance it appears that the acceptable region gets narrower on the right, but this is just a visual side effect of the steep slope of that area (like a caligraphy pen). The height of the shaded region is constant all the way through.\nNote for the future: It might be better to use a more robust stastic like a scaled MAD for estimating the underlying standard deviation of the residuals\nDifficulty implementing robust estimators Right now the model gets completely thrown off in the face of large outliers. We don\u0026rsquo;t have huge outliers but if our sample has any abnormal power usage, the model is being impacted by it, making it harder to detect abnormalities. To reduce this effect I looked into this resource on robust linear estimators.\nI tried all the estimators they describe (RANSAC, TheilSen, and Huber), and each one had a poor fit on the data (even at lower polynomial degrees). At higher degrees they weren\u0026rsquo;t even close. Part of the issue is that as far as I can tell, Huber regression is the only one that incorporates regularization (it\u0026rsquo;s related to Ridge regression).\nAside from that, I wonder if normalization has something to do with it? Yesterday I noticed that I had to set normalize=True to get the basic regularized estimators to fit properly. There is no such parameter for the robust estimators, but there is a preprocessing Normalizer(). I thought I could just toss it into the pipeline to replicate the behavior of the normalize parameter, but on trying this with the Ridge regressor I got very different results (I also tried both norm arguments).\nWhat exactly is normalization? Generally, normalization scales a set of values so they all lie between 0 and 1. Using l2-norms, this would scale a vector to be of unit length. In my Calc 3 course we used this method to find unit vectors, though we never actually called it normalization. If using the l1-norm, I believe it scales the vector so that its values sum to 1 (i.e. its l1-norm = 1). The default method of the Normalizer() prepocessor is to use the l2 norm.\nOne odd thing I noticed is the Normalizer tranformation object introduces more calculation error than the non-class-based preprocessing.normalize() method (assuming the numpy.linalg.norm() method is accurate). The former consistently scales the input vectors at least $10^{-4}$ units too high (e.g. 1.000159\u0026hellip;), while the latter isn\u0026rsquo;t more than $10^{-15}$ units lower than it should be (e.g 0.99999999\u0026hellip;). Still, I don\u0026rsquo;t know if that\u0026rsquo;s enough to be causing my troubles. I tried manually using the normalize() method rather than putting the Normalizer() in my pipeline and I didn\u0026rsquo;t see any noticable change in the output plot.\nThe solution I had a hunch this was the case but I was convinced it was mathematically wrong. It turns out the normalize parameter in the other estimators I had used normalizes the features rather than the samples. The documentation claims that normalize=True normalizes the \u0026ldquo;regressors\u0026rdquo;, but as far as I can tell the \u0026ldquo;regressors\u0026rdquo; are the X data, so I must be misunderstanding that definition.\nIn my calls to preprocessing.normalize() I just set axis=0 (it is set to 1 by default) and voila, the Ridge regression looks as good as it did by using the normalize parameter. I\u0026rsquo;ll have to see if there\u0026rsquo;s a way to use this fix in the Normalizer() object so I can put it straight into the pipeline.\nThe moment of truth\u0026hellip; did this actually help fix the bad plots from the robust estimators? Yes and no. RANSAC and TheilSen still completely break at high order polynomials since they don\u0026rsquo;t have regularization. Huber regression is performing wonderfully though. I tested it out by tossing in a ridiculously high outlier at 10,000 kW and Huber didn\u0026rsquo;t even flinch. It\u0026rsquo;s not worth trying the debug the other two estimators when Huber does the job just fine. I\u0026rsquo;m happy with it, as judging by its description it\u0026rsquo;s a relatively noninstrusive method that softly aids in reducing the impact of outliers.\nPutting it in the pipeline Sklearn lets you make custom transformer functions using preprocessing.FunctionTransformer. For my function I used preprocessing.normalize, and I passed the keyword argument to set axis=0:\nmyNormalizer = FunctionTransformer(normalize,kw_args={'axis':0})  Then I can just put myNormalizer in the pipeline after my PolynomialFeatures transformer and it will properly normalize my features for estimators that don\u0026rsquo;t support the normalize parameter.\n","description":"Distribution of the residuals, robust estimators, comprehending and implementing normalization","href":"https://mattrossman.github.io/energize-andover-blog/post/day-35/","title":"Day 35"},{"basename":"day-34","content":" Blog update - site search As my blog has grown, it\u0026rsquo;s gotten harder to find specific posts from the archive. This weekend I had some free time on my hands so I got to implement client-side search on the site. I\u0026rsquo;m using Hugo\u0026rsquo;s output configuration parameter and an index.json template to create a search index of my posts. Lunr.js performs the necessary search algorithms in my search script. The result is a lighting-quick site search that doesn\u0026rsquo;t need to even reload the page - try it out on the home page. It was a neat learning experience.\nImproving the estimator Quick note on pipelines Previously I would manually run the PolynomialFeatures fit_transform() on my sample points prior to fitting and on my test points prior to prediction. sklearn.pipeline streamlines this process by letting me build the PolynomialFeatures tranformations straight into my estimator.\nExample:\nmodel = make_pipeline(PolynomialFeatures(degree=5), linear_model.LinearRegression())  To get the coefficients of the underlying estimator, you would call model.steps[1][1] or model.named_steps[\u0026lt;name\u0026gt;] and access the coef_ attribute.\nRidge regression Ridge regression is a step up from the basic LinearRegression object. Its cost function includes a regularization term consisting of the l2-norm of the parameter vector scaled by a constant $\\alpha$.\nThe default arguments do practically nothing for me, probably because the size of the squared error term vastly outweighs the regularization term for this data. It think as the number of samples grows, the squared error term grows (since the l2-norm is inherently summing more things) but the regularization term can potentially stay constant (since the parameters don\u0026rsquo;t necessarily change, and the number of parameters is constant)\nTo get around this, I found it useful to set normalize=True, which resolves the scaling issue between the penalty factor and the size of your sample.\n  Here you can see how the ridge regression does not suffer from the same overfitting problem as the basic least squares regression, even when permited to use 12th polynomial features. Since the regressors are normalized, $\\alpha$ can be quite small (otherwise I had to set it to around 10,000 to achieve similar results)\nEnergize module update Every day I\u0026rsquo;ve been copying and pasting the same variable definitions into my code. I finally got around to actually moving these definitions into the energize module since I always end up referencing variables like df_energy or df_school. Now they\u0026rsquo;re defined once and I can access them from the energize prefix.\nAdditionally, I added a function that finds the intersection of two Pandas objects by thier indexes (strange that there isn\u0026rsquo;t one built into Pandas) called intersect().\nLasso and ElasticNet Lasso is quite similar to Ridge regression, but it differs in the implementation of the penalty term. While Ridge has the goal of reducing parameter sizes, Lasso has the goal of reducing unecessary parameters. There\u0026rsquo;s also a method Elastic Net which combines the two (it has an l1-norm and l2-norm penalty term). By default it uses a 50\u0026frasl;50 ratio between the two.\n  I spent some time tweaking the degree and $\\alpha$ levels. Overall, I found that the Ridge \u0026amp; Lasso regressions were more closely fitted than the ElasticNet, but most of the differences I was seeing could be settled by simply adjusting the $\\alpha$ value. There\u0026rsquo;s not a clear winner for me yet.\nCross validated estimators? I tried out the cross-validated versions of these estimators out without much luck. I\u0026rsquo;ve been mainly sticking with the default values they provide though, so perhaps that is part of the problem.\n  I\u0026rsquo;m using the alpha_ attribute to get the cross validated alpha values. They seem to be performing worse on their own than when I manually specified the alpha value. I\u0026rsquo;ll have to spend more time reading about how they\u0026rsquo;re supposed to be implemented, but so far changing the cv, max_iter and tol arguments hasn\u0026rsquo;t provided a clear benefit.\nTo do: I want to look into robust regression estimators and figure out what\u0026rsquo;s going wrong with the cross validation. I also should find out whether the standard error of the residuals can be used as a way to detect trend outliers.\n","description":"Regularized estimators","href":"https://mattrossman.github.io/energize-andover-blog/post/day-34/","title":"Day 34"},{"basename":"day-33","content":" Polynomial regression as linear regression There is no specific polynomial regression estimator in sklearn. Zico Kolter goes over this in his lecture, but polynomial regression is really an extension of linear regression because the additional feature terms ($x^2$, $x^3$, \u0026hellip; $x^n$) can be thought of as regular variables with linear coefficients. Scikit-learn has you making a PolynomialFeatures object of a particular degree, then using its fit_transform method to generate the feature matrix.\nimport numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn import linear_model X = np.sort(5 * np.random.rand(40, 1), axis=0) y = np.sin(X).ravel() x_plot = np.linspace(X.min(),X.max(),100).reshape(-1, 1) poly = PolynomialFeatures(degree=5) X_ = poly.fit_transform(X) x_plot_ = poly.fit_transform(x_plot) reg = linear_model.LinearRegression() reg.fit(X_,y) plt.scatter(X,y) plt.plot(x_plot, reg.predict(x_plot_))  That example fits a 5th degree polynomial to a sinusoidal sample.\nMore Dimensions I know how to perform a regression on one-dimensional input, but it would be cool to try adding a dimension. There\u0026rsquo;s no real extra coding involved (other than passing in an array of $x_i \\in \\mathbb{R}^n$ as opposed to $x_i \\in \\mathbb{R}^1$) but the hard part is actually showing the results\u0026hellip;\n3D Visualization I\u0026rsquo;m using this resource to try out 3D plotting in Matplotlib. I\u0026rsquo;m going to start by just making a scatterplot of the data, looking both at the observed temperature for each point\u0026rsquo;s day and the number of minutes since midnight:\n  It\u0026rsquo;s hard to see here, but it should form a tilted saddle shape.\n  I got this far with the regression plot, but it\u0026rsquo;s definitely not right; it\u0026rsquo;s not accounting for changes in the y-axis. I\u0026rsquo;ve verified that the regression is working when changing the y variable, but I\u0026rsquo;m doing something wrong with the plotting process.\nFixed!    I finally got the plot to display correctly. I don\u0026rsquo;t entirely understand how it works since I\u0026rsquo;m unfamiliar with the specifics of the numpy.meshgrid() and Axes3D.plot_wireframe() methods. This post helped me figure out how to organize my Z-array. Visualization is not the most important part of this project so I suppose it\u0026rsquo;s not a big deal if I do a hack-job on the plotting scripts.\nAnyways, you can see the saddle shape now. The red surface represents a least squares regression on 5th degree polynomial features of the data. I can now take a theoretical temperature and time input (and convert that to minutes since midnight) and return an estimated power usage. I could do this for as many input variables as I wanted now, but I wouldn\u0026rsquo;t be able to really visualize that nicely (I had a hard enough time plotting in 3D). Pretty cool!\nTo do I\u0026rsquo;m still using a basic linear regression technique for the estimator. I\u0026rsquo;m fine with using a polynomial for now, but I might want to look at other loss function options (e.g. LassoCV is a regularized linear estimator that has cross validation built in)\n","description":"Regression on higher dimensional input, 3D plotting with mplot3d","href":"https://mattrossman.github.io/energize-andover-blog/post/day-33/","title":"Day 33"},{"basename":"day-32","content":" Quick Disclaimer on Validation For now I\u0026rsquo;m testing models using all the data available. Later on I will worry about proper validation / cross-validation procedures.\nPower vs. Time of Day I have already looked at the load profile of the school throughout the day (see Day 17), but that was using a fairly simple method that relied on the fact that the data is being sampled at the same 15-minute intervals every day. It only considered the midpoint of the data at each 15-minute timestamp, so the trend line had total freedom to jump around as much as it pleased. It would be more useful to fit a regression to the load profile to capture a smoother, more generalized model.\nSince they\u0026rsquo;ll be used in the regression calculations, I\u0026rsquo;ll need to convert the timestamp objects to quantitative values. I\u0026rsquo;ll measure the number of minutes since midnight. An easy way to get this is:\ndata.index.minute + data.index.hour*60  Using the same polynomial fitting method from yesterday:   Robustness of the Regression I often bring up the fact that we\u0026rsquo;re assuming flaws and outliers in our sample, which is why I have leaned towards measures like the median and MAD over mean and standard devaiation. The least squares regression technique tries to minimize the overall cost function, but that means it isn\u0026rsquo;t resistent to outliers in the data. It just wants to \u0026ldquo;please\u0026rdquo; all the points equally by picking a middle ground of \u0026ldquo;maximum likelihood\u0026rdquo;.\nWikipedia has an article on alternative robust techniques. The overall best compromise of effectiveness and efficiency seems to be the MM-estimation.\nThe SciPy documentation also has a post that overviews how you can go about robust nonlinear regression. It notes that the absolute loss (AKA Least Absolute Deviations or LAD) is a robust loss function, but it is more complex to optimize than least squares (since it is non-differentiable). Instead it describes applying a sublinear function (which grows slower than a linear function) on the cost function\n Note: I\u0026rsquo;ve found many scipy.optimize functions that seem to accomplish the same task (leastsq,curve_fit, and least_squares) but from what I\u0026rsquo;ve read the most recent and direct version is least_squares, which is what is used in the post I linked above\n Kernels? I haven\u0026rsquo;t found a clear offering in the \u0026ldquo;Big 3\u0026rdquo; libraries I\u0026rsquo;ve used (Pandas, NumPy, SciPy) for kernel smoothing. It\u0026rsquo;s appealing that kernels let you use complex features without explicitly defining them, but so far our data has\u0026rsquo;t appeared to follow too crazy of a model anyway. The most complex model I can imagine us using would be something that models the entire lifespan of the time series power data, but we might be able to simply fit that using sinusoidal features (to account for the daily and annual cycles).\nRealistically, we won\u0026rsquo;t even need to make that kind of complex model because we could just perform seperate regressions on subsets of the data filterd by factors such as day of the week, week of the year, temperature, etc. which would be causing those hourly or seasonal shifts. We could allow the user to check off boxes for which factors they want to stratify the regressions by. Each stratified regression could most likely be fitted by a low degree polynomial.\nscikit-learn I\u0026rsquo;ve been seeing the name scikit-learn pop up a lot in my research lately (such as in my search for kernel smoothing), so I finally checked it out. It\u0026rsquo;s a library built on Numpy, SciPy and Matplotlib designed for machine learning. It can do classification, regression, validation, and tons more. Plus it\u0026rsquo;s supported by Anaconda. I\u0026rsquo;m definitely going to poke around the documentation and see what it can do for us. This resource provides tutorials that go over the key points. I\u0026rsquo;ll just make some notes of anything interesting I see along the way.\nUnsupervised learning \u0026amp; Clustering So far I\u0026rsquo;ve just been looking at supervised learning which looks for patterns according to a provided set of inputs and outputs. Unsupervised learning on the other hand finds patterns based solely on input vectors. This goes way back to when I mentioned that it would be nice to be able to automatically classify different power usage profiles (e.g. night, weekend, school day, special event) rather than having the user manually specify them. Unsupervised learning is the general topic, clustering is a more specific category that applies to what I\u0026rsquo;m talking about here.\nUsing estimator objects Using models in scikit-learn is amazingly straightforward. There are various estimator object types (e.g. for linear, polynomial, kernel, and many more) which have a simple fit() method that takes in the input and response values, and a predict() method that takes in a new value and predicts the response based on the model. Machine learning is a really complicated subject but this library practically spoonfeeds it to you; it\u0026rsquo;s fantastic.\nPicking estimators You can use folding methods such as k-folding to perform cross-validation on your \u0026ldquo;learning\u0026rdquo; data set to narrow down the best parameters for your estimator. (Estimators actually have a score method that indicates how well they fit the data). Some estimators even automatically pick their parameters via cross-validation.\nThere is a ton more awesome stuff to unpack here. It may be hard to convey through a blog post, but I am really excited about the potential here. I\u0026rsquo;m going to try to start on the intro tutorials for the library, but I probably won\u0026rsquo;t be able to get too far into it yet.\nSo far I\u0026rsquo;ve just been able to replicate the polynomial regression technique that I had previously done with NumPy by following this post\n","description":"Some remarks on least squares polynomial regression and an amazing machine learning library","href":"https://mattrossman.github.io/energize-andover-blog/post/day-32/","title":"Day 32"},{"basename":"day-31","content":" Continuing with the lectures I skimmed over the 4th and 5th lectures, as they started getting into more specific linear algebra topics that I can delve into later if necessary.\nLecure 6 discussed how you can take the concept of minimizing the cost function using linear features and simply add in non-linear features to fit a model more closely to non-linear data.\nProf. Kolter showed two main approaches here. The first is adding polynomial terms to the feature vector, and the other is using a number of Radial Basis Functions (RBFs). More specifically he demonstrates the usage of a Gaussian RBF, which functions a lot like a normal curve. With enough of these RBFs you can mold them to minimize the cost function on your sample.\nSome issues he mentions are that these methods can be computationally demanding, especially when dealing with multi-dimensional input. Additionally, you have to be careful when setting up your feature vector to avoid over/under fitting the model to your data.\nTo remedy the latter issue, he explains the process of regularization, which incorporates the size of the model parameters (relative to a constant $\\lambda$) in the cost function so that features are discouraged from overfitting.\nThe last topic he discusses is kernels, which through mathematical sorcery that he derives allow you to optimize the cost function using an arbitrarily high (or even infinite) dimension feature vector without ever explicity defining that feature vector, making it a much more efficient process.\nTrying out some code As I mentioned yesterday, the course uses MATLAB. Numpy and SciPy are the primary Python libraries that accomplish similar goals. Since these libraries are designed for scientific analysis, most of the regression techniques are probably already included in these libraries (but it\u0026rsquo;s still beneficial to know the theory behind how they work).\nSince the lecture video looks at a plot of power vs temperature, I will do the same with our own data. Note that the video compares maximum power load to maximum temperature because in that context the concern is power distribution, but I\u0026rsquo;m not concerned with the maximum power load as much as an overall indicator of the power usage. I\u0026rsquo;m going to look at the average power usage for each day during school hours versus the observed temperature for that day.\nNumpy has a polyfit function that will perform a least squares regression using polynomial features. Here are a few plots showing the fitted polynomial regressions of varying degrees:\n  I was inspired by the lectures to use x\u0026rsquo;s instead of dots for my plot markers for better visibility.\nI\u0026rsquo;m using NumPy\u0026rsquo;s poly1d objects to plot the polynomial model. Given an input value, they\u0026rsquo;ll return the expected response.\nThe plots for degrees 3 through 5 look the best, the others are either underfitted or overfitted. What stands out about the $d=5$ model is that it approaches $+\\infty$ at both edges, which is nice because you would expect a trend of higher power usage at both temperature extremes. From this example alone, polynomials seem to do the job just fine. Perhaps other variables won\u0026rsquo;t share such a nice relationship though.\nTomorrow I\u0026rsquo;m going to look at other relationships and see if it would be a good idea to use a kernel regression method.\n","description":"Nonlinear regression lecture (polynomials, RBFs, kernels), testing out polynomial modeling on temperature data","href":"https://mattrossman.github.io/energize-andover-blog/post/day-31/","title":"Day 31"},{"basename":"day-30","content":" An exciting find I ended yesterday with a CMU lecture on nonlinear regression that looked eerily similar to the work we\u0026rsquo;re doing. After sharing it with the team, Frank noticed that it\u0026rsquo;s actually part of an entire course called Computational Methods for the Smart Grid. All of the resources (lecture videos, slides, notes) are freely availabe online. The course teaches how to do precisely what we\u0026rsquo;ve been trying to accomplish this whole summer. I only wish we had found it sooner!\nI made a playlist here of all the course\u0026rsquo;s video lectures. Linked in the description is the course page that contains the lecture slides and notes.\nI spent today watching the first couple of lectures to fill in some of the background I was missing for the 6th lecture on nonlinear regression (and beyond).\nLecture 1 This mainly gave an overview of the course, what it is trying to accomplish and why (where energy waste is coming from, the negative consequences of it). It lays out the basic principles of how you can use various factors to model and predict energy usage.\nLecture 2 The main topic here is modeling via linear regression. Prof. Kolter explains the theory and formal notation of models, which consist of:\n Input: $x_i \\in \\mathbb{R}^n, i=1,\u0026hellip;,m$ Output: $y_i \\in \\mathbb{R}$ Model Parameters: $\\theta \\in \\mathbb{R}^k$ Output: $\\hat{y}_i \\in \\mathbb{R}$  I won\u0026rsquo;t waste time copying everything over from his slides, but you can find the full explainations on the course site.\nHe introduces that fact that we can make a loss function that measures how well the model fits the input data (the most common one is the squared loss). The end goal is to figure out how to minimize the loss function for all inputs (called the cost function).\nLecture 3 To go about minimizing the cost function (i.e. perform a least squares regression) he rewrites the cost function in matrix notation using principles from linear algebra. It\u0026rsquo;s pretty cool that I\u0026rsquo;ll be taking linear algebra this fall, so I\u0026rsquo;m already getting a head start on the basics. Then since our variable $\\theta$ is a vector, you have to use the gradient to optimize the function. That is really cool as well, because just last semester I took multivariable calculus where we learned how to do this.\nThe process is somewhat complicated and uses some rules that he derives in the lecture, but after replaying the confusing bits enough times it finally makes sense. The end result is called the normal equations: $$ 2\\Phi^T\\Phi\\theta^* - 2\\Phi^Ty = 0 \\Longleftrightarrow \\theta^* = (\\Phi^T\\Phi)^{-1}\\Phi^Ty $$\nHe\u0026rsquo;s using Matlab for the course and he explains that there\u0026rsquo;s a built in function for the normal equations, so I\u0026rsquo;m guessing there will be something similar in one of the Python libraries that we\u0026rsquo;re using.\nThe rest of the lecture is a review of linear algebra concepts, which are also available in a neatly organized note sheet in his course resources.\nTomorrow I hope to make my way up to the lecture on nonlinear regression and eventually try to implement that in Python. And that\u0026rsquo;s only a quarter of the way through the course, so I wonder what other advanced topics he covers later on!\n","description":"The holy grail of smart energy resources","href":"https://mattrossman.github.io/energize-andover-blog/post/day-30/","title":"Day 30"},{"basename":"day-29","content":" Today I\u0026rsquo;ll be taking a look at the new Bancroft school data. Bancroft is supposed to be a very energy efficient building so it\u0026rsquo;ll be ineresting to compare the energy usage patterns to AHS.\nThe simplest filter to compare would be night data. Some activities may go on during the summer or weekends, but most buildings will not be operating at night.\n  Interestingly we\u0026rsquo;re seeing a bimodal shape in the night-time distribution. To see why I can look at the line plot:\n  The baseline usage starts out a bit above 20,000 kW, then after August 2016 it rises to around 40,000 kW. Filtering just the data points that are after this point in time yields a more familiar shape:\n  Catching Errors in the Time Filterer When I tried running the school filter on the Bancroft data, I noticed an issue with the time filtering method. When you specify bounds that extend past the range of the data, usually it extends as far as it can and doesn\u0026rsquo;t raise an issue. But when specific dates are out of the data range, it throws a KeyError. To combat this, I added a Try/Except block in the data range token parser:\nif (type(token)==str): try: return data[token] except KeyError: #returns None print('[!] energize.py : range_token_df : ' + token+' not in range')  This way it lightly warns you about your unused dates without causing the script to halt. Strangely, by default Pandas won\u0026rsquo;t throw a KeyError if you use slice indexes that are completely out of the data range (it\u0026rsquo;ll just return an empty data set), it only does the error for individual bad indexes.\nNonlinear Regression I spent a lot of time today fumbling around web pages describing various regression techniques. Viraj elaborated on the Poisson regression that I probably could consider the decimal kilowatts used as \u0026ldquo;counts\u0026rdquo; of infinitesimally small wattage units, and he shared my concern about the independence of the events. This example from Wikipedia is somewhat reassuring that the events could be considered independent (since counts are wattage units used, and using a unit of power does not make it more or less likely to subsequently use another unit of power).\nHowever, Viraj also noted that Poisson regression is just one of many regression techniques available, which led me down the rabbit hole of online resources on the subject.\nI ended up finding something very promising on the topic of non-linear regression. This Carnegie Mellon lecture goes over exactly what I hope to achieve, fitting a non linear multi-dimensional model to a data set. The professor literally uses power data compared against variables like temperature and time of day. It\u0026rsquo;s almost funny how perfect of an example this is. It\u0026rsquo;s comforting to see the familiar tilted-U shape in the lecture\u0026rsquo;s temperature versus power plot as I saw in Day 12. The video is quite long and I\u0026rsquo;ve only seen part of it, but tomorrow I will try to cover as much as seems necessary and probably look at other sources.\n","description":"Some Bancroft plots, time filter KeyError fix, and a promising lead on nonlinear regression","href":"https://mattrossman.github.io/energize-andover-blog/post/day-29/","title":"Day 29"},{"basename":"day-28","content":"Met with the team today at the library. I gave an update on the status of the fitting process. Frank had looked into the Poisson regression and was having some trouble so I wanted to take a stab at it.\nI too was unable to see a clear application of the theory to our scenario. From what I understand, a Poisson distribution is used for counts of event occurences. Our data doesn\u0026rsquo;t immediately apply to this since we are measuring a continuous value, but you can tweak it by, for instance, measuring the counts of data points that lie between 400-450 kW. However that seems like an unintuitive adjustment to make. Perhaps I am just misunderstanding the Poisson distribution. Another area of concern is the requirement that \u0026ldquo;events\u0026rdquo; be independent, since it would seem that having a data point occur at a certain power value would make nearby values more likely to occur at a similar value (due to the time series nature of the data).\nI spent most of the remainder of the day researching more stats methods that might be able to help us. I don\u0026rsquo;t have much to talk about each of these so I\u0026rsquo;ll just list off some of the topics that looked interesting:\n Stepwise regression Regression trees C4.5 Algorithm Beta distributions  Anil also stopped by today to provide some more data files. We now have data from Bancroft to work with, and I was able to load it into a Python script just fine. Later on I will take a look at some of the plots and see how things compare to the data from AHS.\n","description":"More data to work with","href":"https://mattrossman.github.io/energize-andover-blog/post/day-28/","title":"Day 28"},{"basename":"day-27","content":" This morning I joined Anil on a guided tour of Andover High School. He pointed out key parts of the electric and heating framework for the building. Starting with the main power switch and working down to the sub-panels distributed throughout the school it became easier to visualize the tree structure of the circuitry. It also became appart how challenging it is to navigate the electrical map, so it\u0026rsquo;s good that the other team(s) are working on improving this.\nLater on Anil will show me more about the structure of the building\u0026rsquo;s heating and ventilation system. Also soon he will be providing our team with more data samples which will be really helpful for branching out our expectations for power usage, and it will allow us to test the usefulness of what has been done so far.\nBack to the code I moved the work from yesterday into a function and tweaked some labels:\n\u0026quot;\u0026quot;\u0026quot; adjust_sample: Series *int --\u0026gt; Series returns an adjusted version of the data that approximately follows the energize fitted lognormal distribution Buffer count (for setting the quantiles) defaults to 1 on each side (to take the place of the 0th and 100th percentiles) and can optionally be changed \u0026quot;\u0026quot;\u0026quot; def adjust_sample(series, buffer=1): fit_params = lognorm_params(series) s_sorted = series.sort_values() q_step = 1/(series.size+2*buffer-1) q_array = np.linspace(buffer*q_step, 1-buffer*q_step, series.size) quantiles=pd.Series(q_array, s_sorted.index).sort_index() return pd.Series(stats.lognorm.ppf(quantiles,*fit_params), quantiles.index)  I\u0026rsquo;ve been playing around with this and have found some areas where it succeeds, and plenty of areas where it has some trouble.\nHere\u0026rsquo;s a few subplots of the adjustments running on subsets of the weekend data:\n  The Lighting plot looks good and shows a solid 6.85% improvement. The Main and Plug load show a nice improvement but it\u0026rsquo;s questionable how realistic the fit is. The Gym plot is a different story. The fit is not able to keep up with the crazy looking data sample. The histogram looks just as raggedy as the KDE suggests. I think the model makes a decent effort, but by smoothing out all the spikes in the data it ends up using more energy that the sample did. To be fair, the data is more of a problem here than the model is.\nThe night data doesn\u0026rsquo;t look too bad:   But the summer data is a nightmare:   Again, the problem is really with the data itself. We\u0026rsquo;re assuming a log-normal distribution, and the summer samples don\u0026rsquo;t seem to be following any sort of traditional distribution. Why the erratic data?\nEither there really is some ridiculous randomness going on in the data, or there are underlying variables causing the distortion of the distribution. I\u0026rsquo;m thinking that for this reason the Poisson regression method will allow us to better refine our distributions to rule out as many variables as possible.\nBut for samples that do follow a relatively lognormal distribution, this seems to be doing a good job of filtering out the abnormalities and yielding reasonable savings values.\nYou may notice that the blue adjusted KDE doesn\u0026rsquo;t always follow the the purple fitted distribution. It even looks like the adjusted value distribution extends below 0 kW in some subplots. I looked into it, and it\u0026rsquo;s an issue with the way a KDE plot is calculated. if you plot the histogram it follows the model more closely.\n Random side note: while I was tweaking the buffer value, I realized it may be easier to let the user specify a buffer percentage rather than count? (e.g. a buffer that excludes 2% of the model values on either tail). Note that depending on the sample size this may cause issues with the fit of the adjusted sample.\n Tomorrow I\u0026rsquo;ll be meeting with the group and Anil so we can reconvene and get some more data to work with.\n","description":"Tour of AHS, sample adjustment function and testing","href":"https://mattrossman.github.io/energize-andover-blog/post/day-27/","title":"Day 27"},{"basename":"day-26","content":" Wrapping up from yesterday Before I go on, I was thinking more about the importance of the quantile residual plot. The benefit of using the local extrema of this chart is that they identify the points of greatest dissonance between the sample and model. On Day 23 we picked the 95th percentile of the model and found it correlated to the 86th percentile of the sample. But that was just a nice sounding threshold. The local min on that quantile difference plot would be the best area to highlight since it shows the most dissonance. We weren\u0026rsquo;t far off from the ideal value; we tracked an 8.5% data surplus in the sample at the 95th percentile of model. According the the plot, the most extreme difference was 8.63% which occured at the 95.7th percentile.\nI\u0026rsquo;d be curious to compare the locations of the extrema on this plot to the points of intersection in the density plots.\nAlso in order for this system to be really useful, we need to be sure that the fitted model is right. It looks nice for the main power data during school hours, but some other plots don\u0026rsquo;t look quite as nicely fitted.\nQuantile Translation Theoretically, the models that we\u0026rsquo;ve been fitting thus far should provide better energy usage than the samples they\u0026rsquo;re based on. To prove that, I want to fit the data set to our model and compare the energy usage of those two data sets. Viraj explained that this could be done by translating the quantiles accordingly. My first approach at this was pretty slow and more complicated than it needed to be. I\u0026rsquo;ll explain how I first did it and how I revised it.\nThe Bad Way First, I needed to figure out the quantile of every value in the data. In the past I\u0026rsquo;ve used scipy.stats.percentileofscore(data,val)/100 to get the quantile of a value, but the disadantage is this only works on a single value, not list-likes. While pandas lets you perform the quantile function on a Series to get the value of a given quantile, it doesn\u0026rsquo;t have an inverse version of that function. So I would have to manually apply the SciPy function to each element of the data Seres using pandas.Series.apply() or pandas.Series.map() (the former is more useful for DataFrames I believe since you can specify an axis).\nThis is a very slow operation. While most pandas operations are instantaneous, this takes a few seconds to spit out a result. It felt like there should be a faster way to get my results (there is).\nNext, I\u0026rsquo;d have to run another mapping of the stats.lognorm.ppf to these quantile values to get their corresponding power values in the fitted model. It seemed to work and I tried plotting the results, which brought to my attention another problem. By default, the percentileofscore method uses the 'weak' interpretation of the score, meaning it considers all values in the list that are less than or equal to the given value. That\u0026rsquo;s the best method according to the definition of a cumulative distribution function, but it doesn\u0026rsquo;t play nicely with the .ppf function. With a lognorm distribution, you can\u0026rsquo;t reach the 100th percentile; it\u0026rsquo;s infinite. You can reach the 0th percentile since lognormal distributions have that lower bound of zero, but they stretch infinitely in the positive direction. So for my maximum data value (which by default was recognized as the 100th percentile), the equavalent model value was inf for $\\infty$. This made it unable to be plotted.\nI could have resolved this by simply dropping the value, but it seemed unfair to drop my largest data value. To remedy this, in the percentileofscore call I changed the argument for kind to 'mean', which averages between the 'weak' and 'strict' methods. The 'strict' method would only look at the values less than the given values, so the minimum data value would automatically get assigned to the 0th percentile, which in turn would set it to 0 kW. That didn\u0026rsquo;t seem fair either, so the 'mean' method seemed like the best compromise.\nThis is what the inefficient approach looks like:\nquantiles = data.map(lambda x: stats.percentileofscore(data,x,'mean')/100) adjusted = quantiles.map(lambda q: stats.lognorm.ppf(q,*fit_params))  The Good Way The problem with the current approach is the usage of mapping. It\u0026rsquo;s an inefficient way to manipulate the large list values. There are two instances of mapping that need to be resolved:\nThe first is when calculating the data quantiles. Quantiles look at data solely by count. That means that since the distance between data counts is constant (it\u0026rsquo;s just 1), the difference between quantiles (just the ranks, not the values associated with those quantiles) is constant too. That is, assuming the data values are sorted (since a quantile counts the proportion of data equal or below it).\nSo, what is that constant? Well, it depends how I want to set up my quantile ranges. I realize there\u0026rsquo;s some miscommunication between Pandas\u0026rsquo; quantile method and SciPy\u0026rsquo;s percentileofscore method. Pandas (and Wikipedia) defines the quartile range as $0 \\le q \\le 1$. The first and last values are thus fixed to the 0th and 100th percentiles, then everything in between splits up the middle region. But no matter which 'kind' argument you select, you can\u0026rsquo;t replicate this behavior with percentileofscore. If you say kind='weak' then your range is $\\frac{1}{n} \\le q \\le 1$. For kind='strict' the range is $0 \\le q \\le 1-\\frac{1}{n}$. The 'mean' method splits the difference of these ranges, and 'rank' is only used for duplicate element values.\nI might as well define the range that I want since I won\u0026rsquo;t be needing the percentileofscore method anymore. I already explained the disadvantages of allowing the range to include 0 or 1. So I want to evenly distribute the quantiles with a buffer on either end. The affect of a smaller buffer size is more \u0026ldquo;extreme\u0026rdquo; values in my adjusted set (i.e. as the buffer size decreases, the maximum value approaches $\\infty$). The most straightforward approach I can think of would be to set my range to be $\\frac{1}{n+1} \\le q \\le (1-\\frac{1}{n+1})$ with an equal quantile spacing of $\\frac{1}{n+1}$. That would effectively put two invisible dummy values on the ends of the set to take the places of the 0th and 100th percentiles and let the actual data be evenly spaced in between.\nProof: Note that you would intuitively think that I should be using $\\frac{1}{n+2}$ instead of $\\frac{1}{n+1}$ since I\u0026rsquo;m adding two dummy points on the sides, but if you do it out you\u0026rsquo;ll see that due to the inclusive edges it should just be $n+1$. Consider an array [1, 2, 3, 4, 5]. Take the quantiles for these values and you get [0, 0.25, 0.5, 0.75, 1.0]. The spacing is $0.25=\\frac{1}{4}$ or $\\frac{1}{n-1}$. Now imagine you\u0026rsquo;re just looking at the subarray [2, 3, 4], and the 1 and 5 values are your buffer dummies. The spacing should be the same as before so your output quantiles are [0.25, 0.5, 0.75]. Now that $n=3$ you can see this spans the range $\\frac{1}{n+1} \\le q \\le \\left( 1-\\frac{1}{n+1} \\right)$.\nMore generally, given a buffer of $b$ points (on each side), your range should be $$\\frac{1}{n+2b-1} \\le q \\le \\left(1-\\frac{1}{n+2b-1}\\right)$$\nCode First step: sorting the values. I can do this efficiently with pandas.Series.sort_values().\nNext step: assign the desired quantiles to these values. I can generate the range of quantile values using the equation above and numpy.linspace(), then assign it an index and re-sort:\ndata_sorted = data.sort_values() buffer = 1 step = 1/(data.size+2*buffer-1) quantiles=pd.Series(np.linspace(buffer*step,1-buffer*step,data.size), data_sorted.index) quantiles.sort_index(inplace=True)  This part would have been a lot shorter if I just hard-coded the buffer size of 1, but I may want to tweak that later on so it\u0026rsquo;s a bit more future-proofed.\nLastly we need to get the inverse of the CDF on these values from the lognorm distribution. The revision to this step is simple since the .ppf function lets you pass in an array of values so there\u0026rsquo;s no need to use a map function.\nadjusted = pd.Series(data=stats.lognorm.ppf(quantiles,*fit_params), index=quantiles.index)  The Result   The plot shows the distribution of the new adjusted sample against a density histogram of the original data. It also shows how the adjusted sample follows the fitted model almost exactly. I\u0026rsquo;m guessing the small amount of error could be adjusted by changing the buffer size. Visually you may wonder what this even accomplished since it looks just like the fitted plot. However, the benefit is that now we have tangible data to work with as opposed to just some SciPy distribution parameters.\nSo, if I want to compare the energy usage of the two data sets, I can check egz.trapz(data,'15 min') against egz.trapz(adjusted,'15 min'). Here\u0026rsquo;s an output showing the difference:\nSample: 320679.0 kWh Model: 312584.0 kWh Saved: 8095.0 kWh  So in that case the fitted model saved about 8,100 kWh of energy, which is about 2.5% of the sample\u0026rsquo;s energy usage. That doesn\u0026rsquo;t seem like a huge amount, but it\u0026rsquo;s realistic. Also this is just relying on squashing outliers, it\u0026rsquo;s not intended to shift the center point of the data so it can\u0026rsquo;t cause massive changes in energy usage.\nFor the future, I should abstract this process and also look at how the shift affects the daily power plots.\n","description":"Quantile translation for an adjusted data set","href":"https://mattrossman.github.io/energize-andover-blog/post/day-26/","title":"Day 26"},{"basename":"day-25","content":" Plotting the Quartiles scipy.stats.probplot is a method for comparing data quantiles to a distribution.\nstats.probplot(school_main,egz.lognorm_params(school_main),'lognorm', plot=plt)    For some reason even if I set fit=False it still shows the regression line. This visual doesn\u0026rsquo;t interest me too much because I can\u0026rsquo;t see the actual percentage values, I\u0026rsquo;d rather have a plot comparing the percentile ranks\nI can see this happening two ways:\n a range of quantiles (for the sample) compared to the expected CDF of their corresponding values a range of quantiles (for the model) compared to the sample percentileofscore of their values (using the model PPF)  \nOn Day 23 we used an instance of the latter option.\nI tried both and I definitely like the latter choice better. Theoretically our model would be fixed and the sample data is what varies, so it would make sense to have the sample be on the y-axis. This way also shows you visually where the sample data is surpassing the expected model.\nIt\u0026rsquo;s actually easier to use method #1 and then just flip the axes because otherwise you have to set a really small increment for the x-axis in order to see anything interesting\nHere\u0026rsquo;e a readable version of what\u0026rsquo;s going on:\nsample_quantiles = np.arange(0,1,0.01) quantile_vals = school_main.quantile(sample_quantiles) fit_quantiles = stats.lognorm.cdf(quantile_vals,*fit_params)    If you look at yesterday\u0026rsquo;s plot of the sample and fitted densities, you can see that they start to deviate at 475 kW. That corresponds to the 87.3 percentile of the sample data (marked by the red dotten line above), which is right where the upward flare begins.\n   Here I added a straight reference line to show the ideal quantile relationship. You can see that for the entire quantile span, the sample data falls below the model. By definition, quantiles deal with the data below their rank, so the implication of the sample quantiles being consistently lower than the model is that the region above those markers is consistently higher (which is bad).\nI don\u0026rsquo;t know if a flare in the Q-Q plot will always translate into an intersection of the density plots. In the case I visualized above, the flare represents a rapid compensation for quartile spread in the sample values; a large portion of the sample (big $\\Delta y$) is contained in a small portion of the model (small $\\Delta x$). You can make this shape more visibile by plotting the difference between the sample and model quantiles like so:\n  The plot reaches a local extrema at 475 kW, after which it needs to rapidly compensate to bring the difference back to zero.\nHere\u0026rsquo;s the two plots that this is taking the difference of:\n  Overall it seems like the extrema of the quantile residuals sort of indicate key areas for the distributions, but considering that the quantiles of the model and sample aren\u0026rsquo;t supposed to line up I\u0026rsquo;m unsure if the ambiguousness is good or bad. If I just want to find the points were they intersect I\u0026rsquo;m better off just looking at the residuals of the density functions at each power value.\n","description":"Plotting and comparing sample/model quartiles","href":"https://mattrossman.github.io/energize-andover-blog/post/day-25/","title":"Day 25"},{"basename":"day-24","content":" Improving the maximization Last time I mentioned how I wanted to find a better way to find the peak of the Kernel Density Estimation. Previously we were evaluating the KDE probability at 10,000 sample points within the region and letting Pandas find the max of that set. I wanted to see if there was a more direct, precise way to do this.\nNumpy has a gradient function for calculating partial derivatives, but it only applies to arrays of sample points. That wouldn\u0026rsquo;t really be any more precise than our current method since it depends on the resolution of the sample data.\nSciPy has an optimize module that lets you perform a minimize_scalar algorithm on a sample. It still bothers me that this seems to be running an approximation algorithm rather than exact mathematical calculations. I\u0026rsquo;ll keep my eyes out for any other libraries that can do better.\n Note: perhaps this could be solved by manually calculating the KDE to get the exact probability density function (rather than whatever method object SciPy returns) and combine that with SymPy to calculate the derivative?\n Nonetheless, this is probably more efficient than the maximum approximation that we were using before. You may be wondering why I specified the minimize_scalar function as opposed to a maximization function. SciPy only includes optimization for minimizing functions, so to \u0026ldquo;maximize\u0026rdquo; you simply minimize the function with its sign reversed.\nAbstraction \u0026quot;\u0026quot;\u0026quot; lognorm_params: Series --\u0026gt; ( float, float, float ) Returns the shape, loc, and scale of the lognormal distribution of the sample data \u0026quot;\u0026quot;\u0026quot; def lognorm_params(series): # resolve issues with taking the log of zero np.seterr(divide='ignore') log_data = np.log(series) np.seterr(divide='warn') log_data[np.isneginf(log_data)] = 0 kde = stats.gaussian_kde(log_data) est_std = mad(log_data)*1.4826 est_mu = optimize.minimize_scalar(lambda x: -1*kde.pdf(x)[0], method='bounded', bounds=(log_data.min(),log_data.max())).x return (est_std, 0, math.exp(est_mu))  The first few lines of the function are there so that there\u0026rsquo;s no issue if the sample data contains zeroes. Usually numpy will give you a warning and fill the offending results with -inf. These first lines temporarily disable that warning and replace the -inf values with zeros.\nThen I calculate the estimated standard deviation of the log sample using the scaled MAD value. Next is the estimated population mean. In the minimize_scalar call, the first argument is a function that evaluates the negative PDF at a point (so that we\u0026rsquo;re really maximizing this PDF). The guassian_kde.pdf() function returns an array even if you only pass in one value, so I needed to extract the first value of the return array to get the actual scalar.\nI had an issue using the default method for minimize_scalar where it would always return zero, so I changed it to the 'bounded' method which let me specify which region to try to optimize. After changing this it was able to optimize properly. For now I\u0026rsquo;m just setting the bounds to be the range of the log sample values, but if efficiency really becomes an issue we could even narrow that down. Lastly note the .x on the end of the call. That\u0026rsquo;s because the method returns an object with many properties, and the x property contains the actual solution.\nI return a tuple containing the shape, location, and scale values of the lognormal distribution. According to this page (and verified on Wikipedia), the shape attribute of a lognormal distribution is the estimated standard deviation of the normal distribution of the log values. The location property translates the distribution along the x-axis, and I\u0026rsquo;ve been advised to force that to zero. That makes sense since the whole point of using the lognormal distribution was to fix our lower bound at zero, and translation would disrupt that. The scale is defined as $e^{\\mu}$.\nThose are the parameters that SciPy interprets natively for the stats.lognorm distribution functions, so you would just have to unpack the resulting tuple of this function when passing it to a SciPy log-normal function.\nExample usage: Let\u0026rsquo;s say we want to plot the distribution:\nx = np.linspace(school_main.min(),school_main.max(),100) params = egz.lognorm_params(school_main) plt.plot(x,stats.lognorm.pdf(x,*params))  Here\u0026rsquo;s a plot of the fitted distribution (orange) against the sample KDE (blue):\n  Note that this unlike yesterday this is actually plotting the lognormal distribution as opposed to the normal distribution of the log values.\nLikelihood Function The SciPy fit() function tries to maximize the likelihood of a distribution function when picking its parameters, who I\u0026rsquo;m curious how the likelihood of our fitted distribution function compares to that of the one that SciPy picks.\nBut first, I want to make sure I really understand what the likelihood represents. This post does a nice job of explaining what it represents both in discrete and continuous contexts. Basically, likelihood operates sort of opposite to probability. Where probability asserts the parameters ahead of time and quantifies the chances of getting a certain outcome, likelihood asserts the outcomes first and quantifies the chances of them coming from a certain set of parameters.\nA likelihood is calculated as the joint density of the observations given the parameters.\nThe Problem: Really Small Numbers The probability density of a given value occuring is already really small, as in less than 1. Looking at the graph above, the density never even exceeds 0.014. Remember than when you take a joint probability of such events, the probability is only going to keep going down. Taking the product of 3,000+ such values yields a miniscule result. Python doesn\u0026rsquo;t even have the resources to keep track of events this small. It runs out of decimal places and rounds it out to zero everytime.\nIf I wanted I could probably look into a way to counteract this (one idea I saw mentioned was to use log values or the mpmath package) but I should ask myself, is it really worth it?\nAs far as I\u0026rsquo;m aware there is no straightforward way to even interpret the likelihood of a sample, since the value can change drastically depending on how many sample values you have. It seems to be just a tool used to find optimal parameters. I think that a SSE may be a more reasable value for this purpose.\nComparing Quantiles One area we played around with last time was comparing the quantiles of the sample data and fitted distribution. A more generalized approach for this is visualized in a Q-Q plot which directly plots the quartiles against each other. Ideally this would give a straight line, but since our expected fit model is not intended to line up 1:1 with our sample data it definitely won\u0026rsquo;t.\nExample of a nice fitting Q-Q plot from Wikipedia:   Looking at the fit of that Q-Q plot could be another way to analyze the fit of the estimated model.\n","description":"Abstracting the fit implementation, trouble with likelihood, and a way to compare quantiles","href":"https://mattrossman.github.io/energize-andover-blog/post/day-24/","title":"Day 24"},{"basename":"day-23","content":" The team met at the library today. I started by making a list of the items that we may want to consider working on:\n fitting a log-normal distribution to the data marking values past a certain percentile of that PDF translating data sample to match the fitted estimation doing something with the temperature data Poisson regressions likelihood estimation calculation, alternatively SSE  \nA lot of these rely on already having a fitted distribution, so that was our goal for today.\nYou can find the code here.\nFitting an expected distribution At our last meeting with Viraj we discussed using a log-normal distribution as our expectation to account for the lower bound. SciPy can automatically fit() a given distribution to a dataset. At first we used the built-in scipy.stats.lognorm distribution to perform this fit, however, it was finicky to deal with and it involved extra parameters. Instead we found it easier to just take the log of our sample values and fit a scipy.stats.norm distribution to that adjusted set.\nOut of the box, the fit() function didn\u0026rsquo;t achieve what we had hoped:\n  Apparently these parameters maximize the \u0026ldquo;likelihood estimation\u0026rdquo;, however it\u0026rsquo;s not visually close to the KDE plot.\nTo get us closer, we scrapped the fit() approach and tried calculating our own mean and SD values by using the sample median and adjusted MAD:\n  Already looking a lot better. This is pretty much where things were left off on Day 13. It\u0026rsquo;s still a bit off center though. The center we really want is essentially the peak of the KDE plot. To find this, we converted the KDE value array into a Series and found the .max(). Then based on the index of that max, we found the corresponding x value in the x-array by converting that to a Series as well. The end code is pretty ugly, but it gets the job done:\nx = pd.Series(np.linspace(log_school_main.min(), log_school_main.max(), 10000)) kde = stats.gaussian_kde(log_school_main) kde_pdf = pd.Series(kde.pdf(x)) est_mu = x[kde_pdf[kde_pdf == kde_pdf.max()].index].iloc[0]  One thing to remember is that the step count in the np.linspace() call affects the accuracy of this calculation. We tried using np.arrage() to set a fixed step size, but on larger samples it made the sample count unreasonably large causing an error at runtime.\n Note: I would like to improve this implementation in the future to directly calculate the exact peak of the KDE PDF rather than an estimate. Perhaps there is some way to find the derivative of the KDE function\n   Now it is centered properly. It would be a better fit if there was a lower variance which could be accomlished by decreasing our scaling factor, but after tweaking the MAD scaling factor and testing various sample data sets we found the original value of 1.4826 to be the fairest overall.\nSince there was some variation in the fit using this constant Frank suggested implementing some kind of dynamic scaling factor that tries to match the peak value of the KDE PDF to the estimated PDF, so that may be an area to look into.\nFlagging the high points Now that we have a expected fit density function we can calculate the value at a given percentile using scipy.stats.norm.ppf()\nThen we can plug that into the scipy.stats.percentileofscore() function to get the percentile equivalent of that value from our original sample. For the Main power data during school hours, we got:\nIn [16]: stats.percentileofscore(log_school_main,stats.norm.ppf(0.95, est_mu, est_sd)) Out[16]: 86.532858530268697  That means that about 13.5% of the sample data is operating at power values predicted by only 5% of the expected model. That yields an 8.5% surplus of power usage in this region. Ideally there would be no difference, though I\u0026rsquo;m not yet sure what percentage difference would qualify as statistically significant. That is another area we should look into.\n","description":"Fitting the lognorm distribution","href":"https://mattrossman.github.io/energize-andover-blog/post/day-23/","title":"Day 23"},{"basename":"day-22","content":"Another brief post today. I spent another session going over the remainder of the energize module (unstack_by_time, plot_normal and trapz).\nI also went over python features like lambda, map, filter, and list comprehension.\nThat\u0026rsquo;s all I can really think of that needs to be covered, so tomorrow we\u0026rsquo;re going to meet at the library to start planning the actual work that needs to get done.\n","description":"Working with the team (cont.)","href":"https://mattrossman.github.io/energize-andover-blog/post/day-22/","title":"Day 22"},{"basename":"day-21","content":"Not much new to post about today. I worked with Frank and Ajay this morning to go over a detailed explaination of how to perform some core pandas operations.\nWe covered how to read in the CSV file, how to use the ical_ranges and time_filter functions in the energize module, various visualization methods like line plots and histograms, and data indexing methods.\nWe\u0026rsquo;re going to go over some more examples tomorrow, and hopefully by the end of the week we\u0026rsquo;ll be ready to get back to work on the actual problem statement.\n","description":"Going over examples with the team","href":"https://mattrossman.github.io/energize-andover-blog/post/day-21/","title":"Day 21"},{"basename":"day-20","content":"  Note: Today I uploaded my main Python module here for others to reference\n Meeting Today I had a meeting with Frank, Ajay, and Anil\u0026rsquo;s son Viraj. Viraj is experienced both with statistics and the programming tools we are using so he was able to offer some useful insights. I\u0026rsquo;ll go over some of the main points:\nExpected Distributions On Day 18 I ran a script to calculate the best fitting distribution of our data, but theoretically this isn\u0026rsquo;t a good model to \u0026ldquo;expect\u0026rdquo; of our data since it just happened to be the distribution of our sample. We would have to define ahead of time what distribtion the sample actually should follow. The normal distribution poses the issue of negative values, however a log-normal distribution is a reasonable assumption because it takes that zero limit into account.\nMarking abnormal values So far I\u0026rsquo;ve been basing by work around normal distributions (which due to the lower bound is not a good choice) and determining outliers in terms of (estimated) standard deviations from the (estimated) mean (really the median). A more generalized way to approach this is to just mark outliers using percentiles.\nThat\u0026rsquo;s essentially what I was doing with my normal estimations (the points past 2.5 SD would be past approximately the 99th percentile) but since I\u0026rsquo;m no longer assuming normality this is a better general approach. SciPy distributions have a .ppf (percent point function) that perform an inverse of the CDF function allowing us to specify a desired quantile.\nQuantifying savings Given a sample and an expected distribution, it would be helpful to compare the actual sample values to the equivalent ideally-distributed set. I have the means to compare samples, but I would need a way to mold an unoptimized sample set into an optimal expected distribution.\nViraj suggested using the quantile values as markers to perform the translation between the sample\u0026rsquo;s empirical distribution and the expected continuous one. This would be a fairer quantification of energy savings than the method I described on Day 17 since it would maintain the desired distribution rather than arbitrarily lowering the sample values until they fit in a desired region.\nMore advanced techniques Viraj brought up the complex method of Poisson regression as an area to look into. Given arguments to a number of paramters (e.g. time of day, day of week, temperature) it would provide an expected distribution of values that are described by those arguments. Based on that distribution, we can filter the sample data by those same parameters and flag the high outliers.\nMeasuring fits of distributions Lastly, Viraj explained how the .fit() function in SciPy calculates the Maximum Likelihood Estimation to find the optimal parameters to fit a distribution to sample data. When analyzing the fit, we can calculate this likelihood estimation by calculating $$ \\prod_{i=1}^{n} f(x_i | \\theta)$$ where $\\theta$ is a vector of the parameters for the distribution and $x_i$ is the $i^{\\text{th}}$ value from the $n$ sample data points. I\u0026rsquo;ll have to look into a way to present this value in a meaninful way to the user. Another value that could be of use the the residual sum of squares.\nGoing forward I\u0026rsquo;m going to probably slow down a bit now that there are other team members to work with. I want to bring the others up to speed before I get started on some of these complex implementations. Later this week I hope to spend more time with the team to ensure they understand how to use the tools at our disposal. After documenting the contents of our meeting, I\u0026rsquo;ll spend the rest of the day reading up on the SciPy fitting method implementations.\n","description":"Team meeting with guest Viraj","href":"https://mattrossman.github.io/energize-andover-blog/post/day-20/","title":"Day 20"},{"basename":"day-19","content":" Power  Energy To gain a meaningful understanding of the difference between sets of power data, it would be helpful to know how much energy was used. Energy is the resource being consumed, and small changes in power usage can accumulate significant differences in energy consumption. It also enables a smoother conversion into dollars spent or saved.\nEnergy is represented by the area under the power plot. A very basic approximation of this can be calculated with a Reimann sum, which multiplies the power value at one point by the span of time it represents (either spanning to the right or left). A closer approximation can be made with the Trapezoidal Rule which forms trapezoids between values, yielding angled tops rather than just flat bars. Simpson\u0026rsquo;s rule goes further and estimates a curve between data points. That works better for smooth functions, but I don\u0026rsquo;t know if power usage is a very smooth process. I\u0026rsquo;d expect distinct increases or decreases in power usage as appliances are turned on or off, although there are many small electrical processes throughout the school that contribute to overall changes in the main. I think the trapezoidal approach is a safe bet.\nTrapezoidal approximation with NumPy numpy.trapz() takes in a list of y and x values. I wasn\u0026rsquo;t sure how this would play along with timestamp indexes so I did some tests. The type of timestamp that pandas.Series.index returns is of type datetime64 from the NumPy library. I tried running a very simply calculation between two points an hour apart at a constant power value.\nnp.trapz([1,1],[np.datetime64('2017-03-20 12:00:00'),np.datetime64('2017-03-20 13:00:00')])  That returned numpy.timedelta64(3600,'s'), which represents a timespan of 3600 seconds or 1 hour. NumPy doesn\u0026rsquo;t know what units I\u0026rsquo;m using for my y-values, but given that I\u0026rsquo;m defining them as kilowatts I know that this return object represents 1 kWh of energy. To check that the trapezoid math is working properly, I\u0026rsquo;ll try again but with different power values:\nnp.trapz([1,3],[np.datetime64('2017-03-20 12:00:00'),np.datetime64('2017-03-20 13:00:00')])  This should be equivalent to keeping the power constant at 2 kW (the midpoint of 1 and 3). Sure enough it returns numpy.timedelta64(7200,'s') or 2 kWh. To save myself a step, I can append .astype('timedelta64[h]') to convert my timespan values to units of hours (or kWh in my case), and astype(int) after that to just extract the value associated with that time span (since I already know the units).\nTo make sure that this works with my actual series objects, I\u0026rsquo;ll single out the first two main power entries:\nnp.trapz(main[0:2],main[0:2].index).astype('timedelta64[h]')  I did the math out and got 65.06 kWh, which matches the output of numpy.timedelta64(65,'h')\nNow for a whole day:\nIn [38]: np.trapz(main['2017-03-03'],main['2017-03-03'].index).astype('timedelta64[h]') Out[38]: numpy.timedelta64(5795,'h')  So during all 24 hours of Mar 3rd 2017 (a school day), I\u0026rsquo;m seeing 5795 kWh of energy used.\nThis report mentions a commercial rate of \\$0.10/kWh (that\u0026rsquo;s probably being conservative). That\u0026rsquo;s over \\$570 spent on that day alone.\nOverestimating the gaps Some gaps I have no control over (a few posts ago I stated that there were seven inconsistencies I was aware of). But when data is filtered, it systematically adds in gaps. For instance, in the school days filter there are gaps on weekends or past school hours. When calculating the trapezoidal approximation, NumPy doesn\u0026rsquo;t know to skip over those gaps. It sees adjacent data points and calculates the trapedoizal area that they span. In reality, those data points might be spanning overnight (e.g. the last point of one day is followed by the first of the next), meaning NumPy would mistakenly include the energy usage of that time period.\nFirst I tried just reindexing my data at a constant 15 minute interval so that it wouldn\u0026rsquo;t see adjacent points that were temporally far apart. However, that just made the trapz function return a null value. The other solution that would take more work is splitting the data up into pieces that are at the proper 15 minute frequency, then sum the trapezoidal approximations of all of them. The first step is figuring out how to break up the data into adjacent sections.\nSplitting the data into continuous groups pandas.Series.groupby() lets you group a set of data by a function called on each index or by a corresponsing list of group labels. I could make my group labels simpy be numbers counting up from 0. Each time an interval higher than the desired threshold is encountered, the group label value increases.\n\u0026quot;\u0026quot;\u0026quot; consecutives : Data, Offset --\u0026gt; GroupBy organizes data in sections that are not more than the threshold time span apart Group labels are just a count starting from 0 Example use: consecutives(df_energy, '15 min') \u0026quot;\u0026quot;\u0026quot; def consecutives(data, threshold): dates = pd.Series(data.index, data.index) indicators = dates.diff() \u0026gt; pd.Timedelta(threshold) groups = indicators.apply(lambda x: 1 if x else 0).cumsum() return data.groupby(groups)  The core of this code comes from this post, but I relabeled things to make it clearer what\u0026rsquo;s going on and added the ability to specify your desired time interval with a Pandas offset alias.\nNow if I call egz.consecutives(school_main, '15 min').groups.values() I get a list containing DatetimeIndex lists. So the first element would contain all of the timestamps of the first chunk of consecutive values, then indexes for the second chunk, and so on.\nUsing the GroupBy objects Let\u0026rsquo;s say I store my resultant groups in a variable grouped. To perform the trapezoidal approximation on each group, I must say:\ngrouped.aggregate(lambda x: np.trapz(x,x.index).astype('timedelta64[h]').astype(int))  which gives me a list of the trapezoidal approximation as calculated on each group. Now I can just sum that series for the total.\nSuccess! I checked that it is working by running the approximation on the 15-minute grouped data from just March 1st and 2nd of 2017, which summed to 5260 kWh. When I calculated the individual approximations for each day and added them I got the same exact value.\nCompare this to running the approximation on non-grouped data, in which case it tries to include the night period in between the days, resulting in 12186 kWh which is clearly wrong.\nAbstracting it The last step is wrapping everything up nicely:\n\u0026quot;\u0026quot;\u0026quot; trapz: Data [opt: Offset ] --\u0026gt; int Uses a trapezoidal approximation to calculate energy used during the time period. If power data is in kW, energy result is in kWh. Optional offset parameter determines how large of a time gap between entries is the threshold for data grouping \u0026quot;\u0026quot;\u0026quot; def trapz(data, offset=None): if offset is None: offset = pd.Timedelta.max grouped = consecutives(data,offset) approx_kwh = lambda x: np.trapz(x,x.index).astype('timedelta64[h]').astype(int) return grouped.aggregate(approx_kwh).sum()  I\u0026rsquo;m reminded in this moment of how much I love Pandas. I initially just wrote the function with a Series in mind, since that was all I had tested on. Just out of curiousity, I tried plugging a DataFrame into it to see if it would break. Like magic, it returns a labeled series of the individual trapezoidal approximations of each column (e.g. Main, Lighting, etc). Thanks Pandas for (sometimes) making things easy.\n","description":"Trapezoidal approximation of energy usage from power data","href":"https://mattrossman.github.io/energize-andover-blog/post/day-19/","title":"Day 19"},{"basename":"day-18","content":" The primary library in Anaconda for performing complex scientific calculations is scipy. It includes 82 built-in distribution functions. You can test how well a distribution applies to a sample using the fit() function.\nSomeone online was nice enough to write a script that iterates over every included distribution function and finds the best fitting one. This calculation takes quite a while to run. I tried running it on the Main power data with my school hours filter:\n    Scipy says that the Johnson $S_U$ distribution is the closest fit across all Main power entries. Perhaps that distribution could serve as a reference for subsets of that data.\nI was hoping I could just use a normal distribution as the reference for my time slices, but it doesn\u0026rsquo;t seem to be effective. Scipy has a normtest function that calculates the P-value of a sample coming from a normal population (documentation). Everything I feed to it generates an extremely small P-value (like, numbers to the power of -60), so even if I set $\\alpha=0.01$ every sample would be considered abnormal. I know that the function works though, because if I generate a normal sample with numpy it gets acceptable P-values.\nA second glance at the Kolmogorov-Smirnov test I spent a lot of time taking a second look at the Kolmogorov-Smirnov test. Scipy has KS tests built in but it was a trick to set up my distribution as a callable CDF. I figured maybe I could use the Johnson $S_U$ distribution that was calculated earlier and compare subsets of my data to that, however I\u0026rsquo;m still paranoid that this is not statistically valid.\nI already mentioned that when testing normality with KS you shouldn\u0026rsquo;t estimate the population distribution from the data. But Wikipedia goes so far as to say that whatever the distribution, it shouldn\u0026rsquo;t be estimated from the sample. I suppose it depends what I define as \u0026ldquo;population\u0026rdquo; vs \u0026ldquo;sample\u0026rdquo;. Is my population just all of the data that I have (i.e. a population would be something like df_school['Main (kW)'] and a sample would be just the data on a certain day or from a certain time)? If I\u0026rsquo;m being really strict, the \u0026ldquo;population\u0026rdquo; data would be an infinite number of power recordings starting at the origin of AHS.\nSo everything I have is just a sample, and stratifying that data just makes smaller samples. Wikipedia isn\u0026rsquo;t explicit about the restrictions on a 2-sample KS test, but this forum post says you can\u0026rsquo;t compare a sample with a subsample that way. A random internet post isn\u0026rsquo;t the most trustworthy source, but I\u0026rsquo;d rather be safe than sorry.\n","description":"Scipy, calculating the best fit distribution","href":"https://mattrossman.github.io/energize-andover-blog/post/day-18/","title":"Day 18"},{"basename":"day-17","content":" Before I get sidetracked, here\u0026rsquo;s a nice resource of statistical tests and their use cases. I also really like this site which goes into great detail on not only how to perform various statistical measures, but also situations when you should (and more importantly shouldn\u0026rsquo;t) use them.\nDaily power models I don\u0026rsquo;t want to ignore the fact that we\u0026rsquo;re dealing with time series data. Most statistical tests are based around random samples with no inherent ordering, but our data has the added factor of ordered time stamps.\nYou would expect a properly filtered section of the data to follow a temporal model. That is, power should be used in a similar pattern between the start and end of each day, week, month, etc. Of these I think days are the fairest way to split the data.\nTo visualize this, I want to split my data into its respective days, then plot the power usage at each time of day, overlaying each day on top of the other. I found a solution here\nschool_main_stacked = school_main.copy() school_main_stacked.index = [school_main.index.time, school_main.index.date] school_main_stacked = school_main_stacked.unstack()  After making a two-part index of my copy of the school data (one part time, one part date) I can use the unstack method to convert the dual-indexed series into a more organized dataframe. Note that school_main = df_school['Main (kW)'].\nNow the index just contains the times and each column is labeled according to its date. When I plot this dataframe each column will share the same time axis, with a seperate line for each date:\n  I lowered the line opacity so that the darker areas signify a higher plot density. The dark areas form a smooth, broad curve that\u0026rsquo;s higher in the middle than the edges. Maybe I can calculate values for the curve and compare each day\u0026rsquo;s plot to that general model.\nIf I calculate the median at each time value, I get this median trend (shown by the red line):\n  I could even smooth this out some more by using a rolling window, but then the model would have a gap at the beginning (or both ends if I centered the windows).\nSo, what can I do with this model?\n1. Flag individual points I could take a similar approach as I took with the rolling windows last week. If you assume a normal distribution of power usage at any given time of day, you can flag points that are past $3\\hat{\\sigma}$ from the median trend. I should point out that a normal distribution says you can have values that high, just that it\u0026rsquo;s pretty unlikely.\n2. Test the distributions I can perform a statistical test on each slice of data (one for each time of day) to check the distribution. Maybe that\u0026rsquo;s a test for normality, maybe it\u0026rsquo;s a log-normal test, maybe something non-parametric, etc. (that can be determined later). If it fails the test, warn the user that at that time of day their power usage is abnormal.\n3. Quantify power savings This area also is subject to change based on what underlying distribution you assume, but for convienence I\u0026rsquo;ll stick with normality. I noticed that the most dense part of the plot above occurs within 1 scaled MAD (a.k.a. $\\hat{\\sigma}$) from the median trend:\n  Even when I tried the same process with the weekend data or data from lighting, plug load, etc. that was the case. Given normal distributions, this region should capture about 68% of the data points. If you wanted you could even customize what percent of the data you enclose using an invnorm function.\nIf you set this region as a goal you could then calculate a new optimized data set where you shift the points that are above this region down so they are enclosed. That could be done either by just setting those points equal to the top boundary of the region, or doing some kind of exponential shift (where the points that are farther away get shifted more) on the points above the median until everything fits nicely within the desired region. Then you could perform a comparison on the data sets to see how much energy (and money) you could save by calculating the difference in areas under the graphs.\n","description":"Daily power trends and couple of stats test resources","href":"https://mattrossman.github.io/energize-andover-blog/post/day-17/","title":"Day 17"},{"basename":"day-16","content":" Sample Data Makeup So far I\u0026rsquo;ve not been very particular about what kind of data sample I\u0026rsquo;m getting. I\u0026rsquo;ve mentioned that the sampling rate is not consistent and there\u0026rsquo;s lots of empty entries but it hasn\u0026rsquo;t been a concern yet.\nNow I need to pay attention to these details. I\u0026rsquo;m going to set the record straight on what the sample data looks like by asking myself two questions:\n1. Where are the null entries occuring? I know already that there are lots of blank entries in my raw energy dataframe. If entire rows are empty in a consistent pattern then this is not an issue. I can find how many entries are null in the rows by saying\nnum_null = df_energy.isnull().sum(axis=1)  I don\u0026rsquo;t quite understand why axis=1 is correct here, since even in the documentation is states that an axis of 1 refers to columns when it actually sums up the row values. Perhaps the logic was that for each row entry, it sums the value in every column.\nFrom here I can see that num_null.unique() returns [6, 5, 3, 7, 1, 0]. That may be a problem, because it means that some rows are partially filled. I can understand the 6\u0026rsquo;s (at first only the \u0026lsquo;Main (kW)\u0026rsquo; column has entries), and the 0\u0026rsquo;s represent full rows while the 7\u0026rsquo;s are empty columns. The other entries I was not expecting.\nI can use this existing logic to index the dataframe by the number of empty values each row contains and see what timestamp is associated with the null counts. Just to be thorough I should check anything that\u0026rsquo;s not a 0 or a 7, since there could be more 6\u0026rsquo;s than I was already aware of.\ndf_energy[num_null.apply(lambda n: n not in [0,7]]  That gives me a dataframe of partially filled rows. The good news is, this array only has entries up until 08/04/15. That\u0026rsquo;s great because I was going to drop those first couple of days anyways (those are the days when only the Main column has entries). It\u0026rsquo;s a relief that all of the rest of the data is clear.\nFrom now on I\u0026rsquo;ll run dropna() on my energy dataframe when I first define it.\ndf_energy = df_energy.dropna()  This removes all partially filled rows.\n2. What are the time differences between entries? There\u0026rsquo;s not a very clean way to do this. I can put my datetime index into a series, then use the shift() function to compare the difference between entries.\ntimes = pd.Series(df_energy.index, index=df_energy.index ) time_diffs = times - times.shift()  Now I have a series of timedelta64 objects. An overwhelming majority of these entries are just 15 minute intervals.\nIt\u0026rsquo;s not convenient to work with this type of data; Pandas isn\u0026rsquo;t as forgiving with it as it is with regular datetime objects. I can\u0026rsquo;t just pull out the number of minutes for each entry. However, time_diffs.min() corresponds to a 15-minute time difference object. All I want are entries that differ from this, and since this is already the minimum time difference that means all that\u0026rsquo;s left is values higher than 15 minutes.\nI can say time_diffs[time_diffs \u0026gt; time_diffs.min()] to get the entries that have larger gaps than 15 minutes. This comes out to only 7 entries, and they\u0026rsquo;re spread out thoughout the whole data set. Compared to the tens of thousands of overall entries, these shouldn\u0026rsquo;t be of concern. The largest gap that occured was on June 7th, when the system didn\u0026rsquo;t record for 1d 9hr 15min. Besides that, there were a couple of 1hr 15min gaps then four 30min gaps. I don\u0026rsquo;t see these gaps having a significant effect on the analysis results. If there was a greater presence of gaps I would have to consider weighing each data point accoring to the timespan it represents.\nGetting more familiar with the distributions In order to determine which statistical test is best suited for our power data, I think I need to spend some more time looking at the distributions of different breakdowns of our sample data. On [Day 13](https://mattrossman.github.io/energize-andover-blog/post/day-13/ I found a nearly normal distribution when looking at the intra-day data filtered during school hours. I will now look at the density functions of other strata.\nHere are a few Kernel Density Estimate plots of subsets of the data with a dotted estimated normal curve overlay (using the median and scaled MAD value):\n       Far from perfect. The night and summer plots look pretty good but the weekend plot is quite off. Currently I\u0026rsquo;m making these judgements off intuition. It feels like the estimation plots should wrap more tightly around the left side, especially since our goal is reducing power usage.\nLog-Normal Distributions Oftentimes when a parameter has a lower bound it follows a log-normal distribution. This is when the natural log of the values follow a normal distribution. This creates a right skew in the expected models:\n  For example, here is the natural log adjusted distribution of the weekend data:\n  It\u0026rsquo;s looks more symmetric than the original plot, and this could be a viable model because of our inherent lower bound of zero.\n","description":"Settling the null values and time gaps, looking at more filtered sample density estimations","href":"https://mattrossman.github.io/energize-andover-blog/post/day-16/","title":"Day 16"},{"basename":"day-15","content":" Today we got to hold our first real team meeting. Frank and Ajay joined me at the library, and I spent time explaining some of the work I had done and sharing the basics of how to use Pandas. For now I\u0026rsquo;m suggesting they spend time setting up Anaconda and Pandas and get a feel for how to use DataFrame and Series objects.\nIn the meantime I\u0026rsquo;ll continue working with probability distribution functions.\nKolmogorovSmirnov test Over the weekend I came across a statistical test designed for comparing probability functions. It\u0026rsquo;s called the Kolmogorov-Smirnov test, or K-S test for short.\nThis page warns that it is not valid when the distribution is estimated. I assume \u0026ldquo;the distribution\u0026rdquo; refers to the one being compared against. I am unsure of how this applies to our situation, since in a way we are estimating the underlying normal distribution. This other page also says not to use the K-S test when your expected normal distribution is estimated from the sample. Instead it recommends Lilliefors test if I decide to use the estimated normal distribution. I think that\u0026rsquo;s enought to convince me that the K-S test is not the best option.\nOther distribution tests The K-S test is just one of many statistical methods to compare data distribution. Here\u0026rsquo;s a list of other such tests. It\u0026rsquo;s pretty overwhelming how many options there are, and each test has a very specific purpose. I\u0026rsquo;m going to spend some time reading up on each of them.\nResearch paper on abnormal stock trading I came across this interesting research study on analyzing abnormal stock trading patterns. It\u0026rsquo;s a lot of dense information to sift through but it could prove very useful. It seems to be tackling a very similar problem statement.\nNon-parametric tests I have learned to distinguish between parametric and non-parametric tests. Parametric tests assume an underlying normal distribution, while non-parametric ones don\u0026rsquo;t. So far you could say my work has been more like the former since I assume that the power usage should be normal. I should emphasize that this is not set in stone and I am very open to trying non-parametric methods since I don\u0026rsquo;t know what the ideal population distribution should look like.\nI have more to talk about but not enought time to write it all down. More to come tomorrow.\n","description":"Team meeting, researching distribution functions and tests","href":"https://mattrossman.github.io/energize-andover-blog/post/day-15/","title":"Day 15"},{"basename":"day-14","content":" Blog Guide A good chunk of today was spent writing a guide on how to host your own blog just like the one you\u0026rsquo;re reading. Feel free to comment on it if there\u0026rsquo;s areas that need clarification. There\u0026rsquo;s also a link to it on the \u0026ldquo;About\u0026rdquo; page so you can find the guide later.\nNow I\u0026rsquo;d like to summarize the main ideas of my first couple of weeks of work.\nIntro I\u0026rsquo;ve been using the Anaconda distribution of Python and Spyder IDE to experiment with parsed power data from the existing Metasys application. To get my sample data I clicked \u0026ldquo;Plot\u0026rdquo;, submitted the sample \u0026ldquo;2017 Mar - 2016 Aug - Electric - Detail - 24 Hrs\u0026rdquo; file, and downloaded the results.\nAnaconda Python scripts can gain functionality by importing packages. Anaconda makes it easy to install a collection of math and science based packages. You can do so with\nconda install package-name  It will automatically take care of all of the dependencies (some packages require others to be installed first).\nPandas The primary library I have used is Pandas. It lets you easily manage and perform calculations on large datasets. Get it with conda install pandas.\nSpyder IDE Anaconda comes with a couple of IDEs. For no particular reason I decided to use the Spyder IDE. You can launch it by simple typing spyder in the command line.\nTime Filtering The first main function I worked on was a robust time filter. It lets you select data points from a given date range (or list of such ranges), then filter it down further by factors like day of the week (Mon, Tue, etc.), month of the year (Jan, Feb, etc.). You can specify a specific range of times to include (ex. just between 05:00 and 12:00) or list of such ranges. Lastly, you can specify a blacklist of date ranges to be exluded, which overrides other filter parameters.\nThis makes it easier to look at a very specific subset of the data in just one step.\nDetails in [Day 6\u0026rsquo;s](https://mattrossman.github.io/energize-andover-blog/post/day-6/ and [Day 7\u0026rsquo;s](https://mattrossman.github.io/energize-andover-blog/post/day-7/ posts.\niCal Parsing To augment the time filter, I made functionality for generating date ranges from .ics files. These date ranges can be passed to the time filter function either as the range to include or as the blacklist range.\n.ics files are the most popular online calendar format. You can export them from Google Calendar or your calendar software of choice.\nThis makes it easier for a user to create date ranges for the time filter function. For instance, I loaded the Andover Public Schools academic calendar into Google Calendar and exported the .ics file to easily filter out days when there was no school.\nFor more details, read [Day 7\u0026rsquo;s](https://mattrossman.github.io/energize-andover-blog/post/day-7/\nTemperature Data One factor that could influence power usage is the outside temperature. Most weather APIs do not provide historical data, but I was able to download a report from the NOAA website with air temperature data from the nearby town of Lawrence. Using this data we can look for patterns between temperature and power usage.\nMore details in [Day 12\u0026rsquo;s](https://mattrossman.github.io/energize-andover-blog/post/day-12/\nI would like to find a better weather source in the future since this method requires waiting a day for the report to be delivered via email.\nAnomaly Detection Normal distributions The core of my techniques for anomaly detection lies in the concept of a normal distribution. This is the kind of distribution most commonly found in nature and other random scenarios. My assumption is that when properly filtered to restrict the influence of external variables, the power data should follow a normal distribution. My goal is to identify areas where it does not abide by this expected distribution.\nDescriptive Statistics You probably are familiar with the term mean. It is the average value of a data set used to describe central tendency. Alongside the mean is the standard deviation, which describes the amount of variation in the data.\nWhen data is skewed, it is often better to use the median as an indicator of the center rather than the mean. This is because unsymmetric values like outliers can drastically change the mean, which the median is less affected by these types of values.\nThere is no direct equivalent of the standard deviation for medians, but you can make a good estimate. You can calculate the Median Absolute Devitation (MAD) and apply a scaling factor to \u0026ldquo;convert\u0026rdquo; to an estimated population standard deviation. For normal distributions, the scaling factor is 1.4826.\nA normal distribution can be represented by its Probability Density Function, which shows the probability of certain values occuring:\nA general rule of thump is the 68-95-99.7 rule which states the probability of an occurence within 1, 2, or 3 standard deviations of the mean.\nPopulation vs Sample It\u0026rsquo;s important to distinguish between a population statistic and a sample statistic. Population statistics refer to the absolute, true value of something. You could get the population mean height of a human by considering every human in existence. That is nearly impossible to do, so instead we use sample statistics, for instance, taking the mean of just 100 random people. It\u0026rsquo;s important to keep in mind that the sample mean is not the same as the population mean. You should practice smart sampling techniques to make sure your sample is a good representative of the population.\nIt\u0026rsquo;s helpful to remember that the population mean is represented by the Greek symbol $\\mu$ (mu) and the population standard deviation by $\\sigma$ (sigma).\nWhen you see a hat above a symbol, it means that is an estimator rather than a population values (e.g. $\\hat{\\sigma}$)\nRolling windows Because there are seasonal changes in the power use, I tried working with rolling samples of the power data. This calculates statistical indicators at subsets of the data which may be more similar than the entire data pool would (e.g. there will be less variation in just July than all of 2016). The result is a series of values that track this dynamic statistic. See the simple moving average in stock market analysis for reference.\nIn my case, I\u0026rsquo;m looking at rolling medians and rolling MADs because I am factoring in the possibility for abnormal skew in the sample data. Values that are more than 2.5 rolling estimated standard deviations of the rolling median can be flagged as abnormal.\nPercentage Boundaries One way Anil suggested we can highlight areas for power savings is looking at what percent of the data is contained at a certain value. Pandas has a built in quantile function that can tell you the value at a given percentile of the data. We could inform the user that they are able to use less than $x$ kW of power $x\\%$ of the time, which would set a target for them to reduce usage.\nComparing Probability Distribution Functions My latest goal has been to compare the probability distribution function (PDF) of the sample with an estimated normal distribution. Whereas the rolling median and MAD helps identify where abnormalities are occuring, this could identify to what degree power could be saved.\nThat about sums it up so far. I encourage you to glance over my daily posts for visuals of some of the statistics that I\u0026rsquo;ve gone over.\n","description":"Overview of my first couple of weeks, and a guide on how to make a blog","href":"https://mattrossman.github.io/energize-andover-blog/post/day-14/","title":"Day 14 - Summary"},{"basename":"day-13","content":" I\u0026rsquo;m going to hold off on pattern detection right now. I spent some time thinking about it today and it would be a pretty extensive problem, and I don\u0026rsquo;t have solid sample data to even test it on.\nInstead I\u0026rsquo;ll play around with one of Anil\u0026rsquo;s suggestions which focuses on percentages of data count in a certain value region.\nPercentage bounds Pandas has a quantile() function that returns the data value at a given percentile. For instance,\nmy_series.quantile(0.5) == my_series.median()  That means that df_school['Main (kW)'][df_school['Main (kW)'] \u0026lt; df_school['Main (kW)'].quantile(0.9)] represents the bottom 90% of the data points (by count).\nWe can use this to highlight days where there could be energy savings. Variation is to be expected, but if we tell a user that 90% of the time during school days it can get along just fine using less than $x$ kW of power, then it shouldn\u0026rsquo;t be a problem to reduce the power usage of days that used more.\n  From this it would be pretty simple to figure out what percent of the data value range lies in this count percentile, but I don\u0026rsquo;t know how relevant this information would be. The range of a sample is so arbitrary, and just 1 outlier could drastically change the range of a data set.\nWith a normal distribution, there\u0026rsquo;s no golden rule that 50% of the data should lie within $x\\%$ of the population range, becuase the population range is $\\infty$. The only standards you can apply with a normal distribution are how the percentiles compare to the mean. For example, you can say that 50% of the data should lie within $0.6745\\sigma$ from the mean. But this doesn\u0026rsquo;t translate in any way to the data range.\nFor this reason, there\u0026rsquo;s not much more I can do on this front. Pandas already has the quantile function built in so there\u0026rsquo;s no more coding for me to do. You give it a percentage, and it tells you the corresponding value.\nBack to normal distributions I was playing around with pandas visualization tools and came across the density plot (more specifically it is called Kernel Density Estimate). It looks very similar to the histograms I\u0026rsquo;ve been using, but instead of measuring counts of frequency it measures proportional density, and it smooths out the results into a nice curve.\n  It\u0026rsquo;s easier to see the shape of this distribution this way, and I think it would interesting to see if I could plot a true normal distribution against this shape. That would be a good way to refine whether my MAD calculations are actually valid.\nNumpy Pandas doesn\u0026rsquo;t have a built in way to do this, but Numpy has some tools for generating statistical data sets. I used this guide to graph a normal distribution with a custom $\\mu$ and $\\sigma$ value.\nI just wanted to see how normal I could get the KDE plot to look, so through trial and error I narrowed down the values until the graphs were close to overlapping:\n  The larger your variance, the smaller the values of \u0026lsquo;density\u0026rsquo; became, which makes sense since the data is getting spread out. The total area under the probability curve should always equal 1. I eventually set $\\mu=419$ and I set the variance to 890, which translated to $\\sigma \\approx 29.83$\nYou can see on the right side how the blue KDE plot has a bulge that contrasts with the normal plot. My goal is to highlight the data that is causing that bulge.\nThis is a chance to see if my median and MAD calculations are working like I want them to. I manually found a pretty good estimate of the population normal distribution, so I can use those values of $\\mu$ and $\\sigma$ to test against.\nMedian I used the median as an estimator for $\\mu$.\nIn[173]: df_school['Main (kW)'].median() Out[173]: 423.0857  Pretty close. Compare that to using the mean:\nIn[174]: df_school['Main (kW)'].mean() Out[174]: 430.74404519261896  You can see that the median (423) is closer to my accepted value of $\\mu$ (419). It\u0026rsquo;s not perfect though. The median doesn\u0026rsquo;t line up with the peak of the KDE, and I\u0026rsquo;d like to fix that later.\nM.A.D. Next, I used an adjusted MAD to estimate the value of $\\sigma$.\nIn[175]: egz.mad(df_school['Main (kW)']) Out[175]: 20.762 In[177]: egz.mad(df_school['Main (kW)'])*1.4826 Out[177]: 30.7817412  That\u0026rsquo;s pretty darn close. I said that $\\sigma=29.83$, which is less than a kW away from my calculated value. I\u0026rsquo;m pretty happy with this, and if I adjust my center value according to the peak of the KDE and use that as my \u0026lsquo;median\u0026rsquo;, that could change the MAD to make my estimation of $\\sigma$ even closer.\nHere\u0026rsquo;s the plot using the estimated $\\mu$ and $\\sigma$ values I got from these calculations:\n  I\u0026rsquo;m surprised how close it actually is. Up until this point I was pretty much crossing my fingers that I wasn\u0026rsquo;t just pulling random numbers out of thin air. It still catches a good portion of the bulge, but it would be better if I adjusted the center point.\nJust for fun, here\u0026rsquo;s the plot using the sample mean and standard deviation:\n  Much worse, right? It just goes to show why I\u0026rsquo;m not using these values, and instead using median and a scaled MAD value.\n","description":"Percentage bounds and density plots","href":"https://mattrossman.github.io/energize-andover-blog/post/day-13/","title":"Day 13"},{"basename":"day-12","content":" While I wait on the temeprature data report, I\u0026rsquo;ll transition back to the night data. Before I was just looking at distributions, now I want to see the plots and look for any patterns.\nAs a reminder, I\u0026rsquo;m saying \u0026lsquo;night\u0026rsquo; data lies between 11:00PM and 4:00AM\nDownsampling the Night Data The plots thusfar have been pretty cluttered because I\u0026rsquo;m looking at every timestamp entry from every day of the sample region. At the moment I\u0026rsquo;m mostly looking for outlier days, so I can clean up the plots by first downsampling my data to broader time entries.\nI can say\nnight_main.resample('D').mean()\nto break my night series into chunks by the day on their timestamp, and then find the average of each chunk. This replaces a day\u0026rsquo;s worth of variation with a single data point representing the mean value of that day. I pick the mean here rather than the median because if there is an outlier in that day, I want to be able to detect it. The mean will be more affected by outliers than the median.\nHere\u0026rsquo;s the plot with every single entry:\n  And here\u0026rsquo;s the mean downsampled plot:\n  You can see how much cleaner it is when just looking at one point per day.\nThere is a lot of weird variation going on here. The average power values range from 72 to 230 kW. If this was the daytime, the variation would make sense. But between 11 and 4 the school should be empty.\nWeirder still is the fact that there\u0026rsquo;s a surge of night power usage midway through July 2016 when school isn\u0026rsquo;t even in session. I\u0026rsquo;m going to focus in on that Jul-Oct period and narrow down what\u0026rsquo;s causing the increase.\n  It looks like the gym is the biggest culprit here, along with the Collins Center. Anil mentioned I should ignore the Collins Center as it is rented out for various events throughout the year.\nTemperature update I just got an email with my finished temperature data order. Time to see what kind of data I\u0026rsquo;m dealing with.\nIt\u0026rsquo;s a CSV file with timestamps, station name, min, max, and observed temperatures. I can ignore the station name column, it has the same value for every entry. There are some values entered as -9999 which are presumably errors. In the read_csv function I can set na_values=-9999 to avoid having ridiculous outliers.\ndf_temp = pd.read_csv('resources/temperature.csv', index_col=1, na_values=-9999).drop('STATION',axis=1) df_temp.index = pd.to_datetime(df_temp.index,format='%Y%m%d')  This gives me a nice dataframe of time indexed temperature data. Here\u0026rsquo;s a plot of all the data I got:\n  It follows a pretty smooth seasonal wave. To look for a relationship between mean power usage (using my downsampled nighttime data) and temperature, I plotted one against the other:\n  It\u0026rsquo;s an interesting tilted U-shape. Nighttime power usage sinks on days that are 50F, but rises on hotter or colder days. In the hot seasons, the gym and lighting usage contribute to the rise, while on the colder days it\u0026rsquo;s the kitchen and emergency power mains that see a noticable increase:\n  Switching over to a time plot of the kitchen and emergency power usage:\n  This is not what I would have expected. I would imagine emergency power only gets used in case the power gets knocked out, and the kitchen should be using a constant amount of power at night year-round.\nIn 2016 alone, there were exactly 100 days when the average nighttime kitchen/emergency power usage was above 40 kW. It would help if I had a better understanding of the sources of this column\u0026rsquo;s data. Maybe it actually is used for charging an emergency power bank, so in the colder months it is used in preperation of power issues? Or maybe the lunch staff has been cooking up some midnight snacks\u0026hellip;\nDetecting data disjointedness Rather than me manually finding things like the odd split in the kitchen/emergency data, I want to make code that will find patterns like this for me. This code could then be run on any dataset that is supposed to be constant (e.g. nighttime data) but probably not on data with expected variation.\nSome patterns it could look for:\n Day of the week Week of the month Month of the year Time of day Outside temperature I will begin work on this next time.  ","description":"Night data plotting, temperature data update","href":"https://mattrossman.github.io/energize-andover-blog/post/day-12/","title":"Day 12"},{"basename":"day-11","content":" To start off, I\u0026rsquo;m enabling comments on the blog. Even though the site is static, it can connect to Disqus for third-party comment handling. This theme even has Disqus built in so it should be a simple matter of changing some lines in the config file.\nCentering the data It irks me is how I can\u0026rsquo;t center my rolling window when I use a time offset. One workaround (I think I mentioned this yesterday) is reindexing my data at higher detail. All of the data is at least 5 minutes apart, so I can reindex at a daterange with a frequency of '5min' and get a series at a constant frequency. Now if I want a window of 30 days I can set window=12*24*30 since there are 12 5-min intervals in an hour, 24 hours in a day, and 30 days in my window. Now that I\u0026rsquo;m using an integer based window I can set center=True.\nOnly problem: it is really slow. I timed it at 33 seconds to run the calculations. Before this, my calculations only took a second or so to run. Alternatively, later I can try using a time offset and then manually shifting the indexes by half the window size. Here\u0026rsquo;s the output:\n  This shifts the shaded accepted region to be better fit to the power plot. What are the effects of this? Centering the window considers an equal range of data to the right and left of a sample data point. If a sudden uptrend occurs, the centered rolling median will catch on quicker to this uptrend, making the trend less likely to be flagged as abnormal.\nWhen a positive or negative trend occurs, the uncentered MADs will be high all the way up until the end of the trend. The centered MADs will decrease as the trend approaches the end. This makes the uncentered method less likely to catch anomalies after a trend occurs. You can see this mid-way through October 2016, where the centered chart easily rejects the spike, whereas the uncentered one almost lets it slide.\nI exported a few larger plots to compare the effects of centering as well as using a 14 day vs 30 day window size. Pick your favorite:\n         I haven\u0026rsquo;t decided yet which is the best option. If centering is best, I\u0026rsquo;ll want to work on a more efficient way to do the calculation. In the meantime I\u0026rsquo;ll work on something else to clear my mind.\nTemperature Data Besides building occupancy, another factor that can affect power usage is temperature. It would be handy to be able to fill our dataframe with outside temperature data and see if the correlates to the power usage.\nMany of the popular weather providers have an API, however their services are limited at the free tier. You can make a limited number of requests, and you can\u0026rsquo;t really pull historical data (which is exclusively what I need).\nThe NOAA website has a bulky tool for downloading historical weather data, but for Andover it only seemed to have precipitation data, not temperature. The closest station with air temperature data is in Lawrence just North of Andover, so that will have to do.\nI submitted a data request for the same dates as the sample power data (7/30/15 - 3/20/17). Now I just have to wait to get an email response with the output file.\n","description":"Centering time based windows, temperature data intro","href":"https://mattrossman.github.io/energize-andover-blog/post/day-11/","title":"Day 11"},{"basename":"day-10","content":" The pandas rolling object has a few built in commands that I have already made use of, like .median(). But for broader scenarios, you can use the .apply() function to, as the name suggests, apply your own function across the windows.\nThe function must accept a Numpy array (I wish it was just a Series instead) and return a single number. For the time being I made a temporary function to handle Numpy median absolute deviations.\ndef np_mad(nda): ser = pd.Series(nda) return abs(ser-ser.median()).median()  I\u0026rsquo;m more familiar with Pandas than Numpy so for convenience I simply converted the input Numpy array into a Pandas series, then performed the usual MAD calculation on it. If I become more comfortable with Numpy I can change the function but for now Pandas is my easiest option for item-wise calculations.\nI then used Matplotlib\u0026rsquo;s .fill_between() feature to highlight the regions above and below the rolling median. I will set the bounds as $median \\pm 2.5\\hat{\\sigma}$. The result looks like this:\n  The shaded region encloses values deemed acceptable. The points where the power plot moves past the shaded bounds signify problem areas.\nCompared to the basic horizontal line I started with last time, this is a much more dynamic indicator of abnormality because the acceptable region changes is value and range according to the trend and volatility of that time period.\nVariables The qualification for abnormality is determined by a couple of factors that are not set in stone. First is the window size. I explained yesterday my reasoning for selecting 533, but if you are looking at a shorter sample you would want a smaller window. The smaller your window, the more your medians and MADs are affected by sudden changes.\nThere\u0026rsquo;s also the factors that I am applying to the MADs. I\u0026rsquo;m pretty sure the normal scaling factor is good to keep, but I\u0026rsquo;m not set on the 2.5 value. Perhaps 2 or 3 would be better. I played around with this value on my own and found 2.5 to be a happy medium that didn\u0026rsquo;t flag too many points but didn\u0026rsquo;t ignore too many.\nOne thing to consider would be getting rid of this variable altogether. Instead of specifying a threshold value, we could calculate the ratio of each value\u0026rsquo;s distance from the median over the MAD, and rank them from most abnormal to least abnormal. That would let the adminstrator simply take care of the worst offendors as they please.\nAbstracting it Given a Series of power usage with datetime indexes and a window size, I want to be returned a Series with datetime indexes of the ratio $\\frac{e_{med}}{MAD}$ where $e_{med}$ represents the residual (deviation) from the median.\nI thought this was going to be an annoying process, because when plotting I had to keep stripping indexes and performing data interpolation to avoid errors. But I didn\u0026rsquo;t have to do any of that. Note that I am storing my generic functions in an energize.py module which I am importanting as egz. My sample calculations and plots are performed in a seperate temporary script.\ndef mad(series): return abs(series-series.median()).median() \u0026quot;\u0026quot;\u0026quot; mad_rankings: Series, int --\u0026gt; Series Returns Series with ratios of median residual / MAD uses rolling window of desired size (data point count) \u0026quot;\u0026quot;\u0026quot; def mad_rankings(series, w_size): roll = series.rolling(window=w_size, center=True, min_periods=1) meds = roll.median() # convert numpy nd-array into pandas series before calculation mads = roll.apply(lambda nda: mad(pd.Series(nda))) return (series-meds)/mads  Now if I say\nschool_main[egz.mad_rankings(school_main, 533)/1.4826 \u0026gt; 2.5]  I am handed a series of main power usage entries while school is in session that are more than 2.5 estimated population standard deviations from the month-wide median trend. Isn\u0026rsquo;t that a mouthful!\nA more intuitive window I don\u0026rsquo;t think my current window implementation is ideal because data point count is not user-friendly. Also, jumping over gaps disrupts seasonal shift scale (e.g. the \u0026lsquo;month\u0026rsquo; that includes winter break spans more than a month of actual time)\nI should at least try switching to a strictly time-based approach and see how disjointed the result looks.\nIt\u0026rsquo;s not as simple as just changing the window argument to one of the predefined pandas offset string values. If I say window='M' I get the error:\nValueError: \u0026lt;MonthEnd\u0026gt; is a non-fixed frequency  Pandas doesn\u0026rsquo;t like the fact that not all months contain the same amount of data. From what I\u0026rsquo;ve found online, the easiest way to remedy this is to resample the data at a fixed frequency. All of the entries that don\u0026rsquo;t have any actual data will be marked null or Nan. To make this more complicated, the sample data isn\u0026rsquo;t really taken at a fixed frequency. A majority of it is at 15 minute intervals, but I see some values at odd times. I don\u0026rsquo;t know how inefficient this would be, but perhaps I could resample at a high detail, like every minute.\nI tried this with school_main.reindex(pd.date_range(school_main.index.min(),school_main.index.max(),freq='1min')) but still got the same non-fixed frequency error.\nOddly enough when I use days instead of months, I don\u0026rsquo;t get an error. I assumed the error was because some months are longer than others, but every week is the same duration of time. Oh well. I will just stick with days for now. I can simulate a month by just saying 30 days.\n Note: time index based rolling windows don\u0026rsquo;t let you use the center feature.\n Once I change to a time-based window I will have to redo my plotting code, since it is all based on count rather than time index. Here\u0026rsquo;s the new plot:\n  You can see the gaps where the plot line jumps suddenly. Those are the gaps in the data. Unfortunately there\u0026rsquo;s no easy way to hide those interpolated lines. Pandas will only show a gap if there\u0026rsquo;s a value marked Nan there. If my data were at a constant requency, I could resample and all the areas with gaps would default to Nan, but since the data is recorded at varying rates I can\u0026rsquo;t do this.\nThis definitely doesn\u0026rsquo;t look as pretty as the last chart, but the data it represents is a little more useful. It doesn\u0026rsquo;t seem to wrap around the power plot quite as nicely because of the lack of a center option. I\u0026rsquo;m going to have to revise my ranking function to take this into account too. More tomorrow.\n","description":"Rolling MAD, switching to time based windows","href":"https://mattrossman.github.io/energize-andover-blog/post/day-10/","title":"Day 10"},{"basename":"day-9","content":" In Day 4\u0026rsquo;s post I talked about median absolute deviations and their relationship to estimate population standard deviation. It\u0026rsquo;s a start, but its not a great way to detect anomalies. It\u0026rsquo;s a bit too static.\nTo illustrate this, here\u0026rsquo;s a plot of the Main power entries from yesterday\u0026rsquo;s filter. I ran the calculations of median, MAD and $\\hat{\\sigma}$.\n  The black line is at the sample median, and the red line is $3\\hat{\\sigma}$ above that. Previously, I stated that those values that surpass $median+3\\hat{\\sigma}$ should be flagged as anomalies.\nIf you look at the graph however, it seems like this is ignoring a lot of details. A horizontal line is too broad for a pool of data this large. The sample plot doesn\u0026rsquo;t naturally stay flat; it has patterns of bumps and dips throughout the months.\nRolling windows In the stock market, a common indicator of the changing trend of data is the moving average. This measures the average value of something over a set window size. Whereas a regular average looks at the entire sample, a moving average might look at only the past 20 values in the sample. If you take this windowed average at each point, you get a plot that follows the overall trend of the data but with less volatility. The smoothness of the plot increases as the window size increases (approaching a flat line when the window size equals the sample size).\nBecause we are assuming that there are anomalies in our data, I\u0026rsquo;m going to stick with using the median instead of the mean. So instead of moving averages, I\u0026rsquo;m looking at moving medians. Likewise, I can calculate a rolling MAD value and use that to detect local anomalies.\nReturn of the gaps Now\u0026rsquo;s the time where I\u0026rsquo;m starting to feel the downside of having gaps in the data. There\u0026rsquo;s two ways I can approach the rolling window.\nFirst, I could set my window by timespan. Pandas calls this the offset. The problem with this is I\u0026rsquo;m looking at filtered data. When I tell pandas to look at windows that are \u0026lsquo;1-day\u0026rsquo; sized, it will start from the start date and move forward by one day until it reaches the end date. It doesn\u0026rsquo;t care what filters I may have on the data. Thus it will include windows during weekends, vacation, etc. This makes the rolling plot appear disjointed and jittery.\nThe other option is to set the window by data count. For example, telling pandas to look at the last 30 data points. For this method I could take two approaches:\nI could remove all missing entries and go from there, however the data density is not consistent. Depending on where you are in the data, 30 points could represent 7 hours or it could mean 15 hours. That would make the rolling window inconsistent.\nAlternatively, you could use the past 30 points included the blank entries. This would dilute the median calculation in some areas, but would keep the window interval consistent. This assumes data is being recorded consistently. As I understand, the data is being written at 15 minute intervals (regardless of whether that records a real value or Nan). This sounds like the best option to me.\nThe other problem with gaps is that they can\u0026rsquo;t be plotted properly, and they make pandas throw a fit with window calculations. One solution is to use the interpolate() function to let pandas fill in the gaps with a midpoint value. That\u0026rsquo;s perfect for plotting since visually I\u0026rsquo;m happy with a midpoint placeholder.\nI can\u0026rsquo;t use that strategy for the calculations though. Interpolated values don\u0026rsquo;t appear to affect the median of a sample, but they do affect the MAD. Instead, I can use the rolling min_periods parameter to set the minimum number of non-Nan values that are acceptable in my window. I don\u0026rsquo;t think there\u0026rsquo;s any harm in just setting this value to 1 since I\u0026rsquo;m not aware of any massive gaps in the data. The median and MAD are not affected by adding in random Nan values so I think this is valid.\n  I\u0026rsquo;ll take a break from babbling to show you a picture. I interpolated the missing values to show you the plot of the sample data points (I did that on the first image too). Then I took a rolling median with min_periods=1 and window=533.\nI got 533 because there are 4 data points per hour. There are 6 hours, 40 minutes in a standard school day, or $\\frac{20}{3}$ hours. There are 5 school days in a week and 4 weeks in a month. That comes out to $4 \\cdot \\frac{20}{3} \\cdot 5 \\cdot 4 \\approx 533$ data points in a month. So the red line should follow a month-wide trend. The x-axis is set to data count rather than date because otherwise pandas makes large gaps during vacation times. I\u0026rsquo;m working on figuring out how to keep the plot as it is but change the axis label to show the dates, even if the spacing isn\u0026rsquo;t consistent.\nNext week I will work on the rolling MAD and using that to track local anomalies.\n","description":"Rolling windows intro","href":"https://mattrossman.github.io/energize-andover-blog/post/day-9/","title":"Day 9"},{"basename":"day-8","content":" I printed the 2016-17 Andover District Calendar and got to work entering the data. Ultimately I organized it into two calendars, one for days with no school entirely (which I inputted as \u0026lsquo;all-day\u0026rsquo; events) and one for half days (to ensure sufficient overlap I entered the events starting from 10:50AM when school gets out and ran until 11:59PM at night).\nI exported these .ics files, then imported them into python with my .ical range processor:\nno_school = egz.ical_ranges('resources/no_school_2016-17.ics') half_day = egz.ical_ranges('resources/half_days_2016-17.ics')  The filtering process seemed really smooth:\ndf_school = egz.time_filter(df_energy, include = ('9/2/16','6/16/17'), blacklist = no_school + half_day, daysofweek=[0,1,2,3,4], times=('07:40','14:20'))  Except that when I looked at the data I was getting, it wasn\u0026rsquo;t completely right. First, all of the vacation days were still included because I accidentally exclude exclude instead of blacklist, but I got that sorted out eventually.\nThen, I noticed that my half days were not filtering out. Instead of filtering out the half days, I tried solely included the half day ranges to see what was going wrong. The events that should have been starting at 10:50AM were starting at 2:50PM when school would have already been out, hence why they seemed to be missing.\nTime zones Ultimately, I determined the problem to be a timezone conflict. The ranges that my ical function was outputting were in UTC time, which is 4 hours ahead. Based on how smooth things had been going so far, I assumed it would be easy to convert this object to a different timezone, right?\nNot quite. This was an incredibly convoluted and time intensive debugging process. I followed a few tutorials online but none were doing what I wanted. I ended up getting it to work, but the result isn\u0026rsquo;t pretty. I had to overhaul the ical_ranges function and make a helper function that handles the timezone conversion. I also had to add a check to see whether the item was a datetime.datetime object (in which case I would perform the conversion) or just a datetime.date object (in which case there\u0026rsquo;s no time to perform any conversions on. Otherwise the program would throw an error.\n\u0026quot;\u0026quot;\u0026quot; convert_range_tz : DataRange(datetime.datetime), timezone --\u0026gt; DataRange converts the ical default UTC timezone to the desired timezone \u0026quot;\u0026quot;\u0026quot; def convert_range_tz(range_utc, local_tz): convert = lambda time: pytz.utc.localize( time.replace(tzinfo=None)).astimezone( local_tz).replace(tzinfo=None) return tuple(map(convert,range_utc)) \u0026quot;\u0026quot;\u0026quot; ical_ranges: File Path --\u0026gt; ListOf DataRanges reads the ics file at the given path, and turns the event start and end times into data ranges that can be read by the time_filter function \u0026quot;\u0026quot;\u0026quot; def ical_ranges(file): cal = Calendar.from_ical(open(file,'rb').read()) ranges = [] cal_tz = pytz.timezone(cal['X-WR-TIMEZONE']) for event in cal.subcomponents: event_range=(event['dtstart'].dt,event['dtend'].dt) if isinstance(event_range[0],datetime.datetime): event_range = convert_range_tz(event_range, cal_tz) ranges.append(event_range) return ranges  The conversion process is pretty ugly. It seems like there shouldn\u0026rsquo;t be repetition of that tzinfo=None statement, but that was the only was I could get it to work. The first time it occurs is because the localize function requires that tzinfo is not already set, and the second time is because the resultsing datetime objects gets another timezone attribute tacked on that messes up the pandas indexing. You\u0026rsquo;d think that I could just set tzinfo to the local_tz and it would just work properly, but I can assure you that I tried all the simpler possibilities first and none of them worked.\nAnyways, I went back to that filter from the beginning of the post, and now it filters the half days properly. There are 4 remaining low-outlier days, and I cross checked these with online sources to verify that they were snow days. I added these days as a seperate list to the blacklist.\nThe filter currently runs as:\ndf_school = egz.time_filter(df_energy, include = ('9/2/16','6/16/17'), blacklist = no_school + half_day + ['2/9/17','2/13/17','3/14/17','3/15/17'], daysofweek=[0,1,2,3,4], times=('07:40','14:20'))    Now I have a nice unimodal distribution of the main power usage just during school hours. You can visually see that blob sticking off to the right which probably is a sign of power waste.\nTomorrow I will start working on the functional implemetation of anomaly detection.\n","description":"Debugging the calendar parser","href":"https://mattrossman.github.io/energize-andover-blog/post/day-8/","title":"Day 8"},{"basename":"day-7","content":" For institutions with very complex schedules, the basic filtering I created yesterday probably won\u0026rsquo;t suffice. Instead, it may be nicer to let the user set their desired schedules in a graphical environment and use that as a time filter in the application.\nRather than making my own graphical solution, I\u0026rsquo;ll let the user do so in their environment of choice and simply import that data in the popular .ical format, which consists of .ics calendar files. Applications like Google Calendar let you export this type of data.\nI will have to write some code to parse these .ics files and generate lists of time ranges. I\u0026rsquo;m thinking I\u0026rsquo;ll generate these as lists of timestamp tuples, which can then simply be passed to the daterange parameter of my time_filter function. I won\u0026rsquo;t care about any features of the calendar events besides their start and end timestamps. It will be up to the user to properly seperate their calendars into desired categories.\nICS Format I started by making a fresh calendar with a single event and exporting it. This is the contents of the .ics file generated:\nBEGIN:VCALENDAR PRODID:-//Google Inc//Google Calendar 70.9054//EN VERSION:2.0 CALSCALE:GREGORIAN METHOD:PUBLISH X-WR-CALNAME:Energize Andover Test X-WR-TIMEZONE:America/New_York X-WR-CALDESC: BEGIN:VEVENT DTSTART:20160307T130000Z DTEND:20160311T220000Z DTSTAMP:20170614T160347Z UID:lbinms85k7rmr6et5a1m19h854@google.com CREATED:20170614T160336Z DESCRIPTION: LAST-MODIFIED:20170614T160336Z LOCATION: SEQUENCE:0 STATUS:CONFIRMED SUMMARY:Test Event TRANSP:OPAQUE END:VEVENT END:VCALENDAR  Events are stored in VEVENT blocks, all within a main VCALENDAR block. Rather than trying to parse this data myself, I\u0026rsquo;m going to see if there\u0026rsquo;s an existing way to do it.\nSomeone made an iCalendar library that can probably help me. Documentation is awful, but all I need are datetime objects from my events so I should be able to scrap together a solution from forum posts.\nSince the library is not included with the Anaconda installer, I first updated the pip package installer with conda install pip and then used pip to install the package with pip install icalendar.\nObject format Components in icalendar function kind of like Dictionaries. They can have key-access values like 'DTSTART' (case not sensitive) which you could access like a normal Dictionary. They also can function as bundles of components, which you can access with component.subcomponents.\nFirst I read in my test file with\nfrom icalendar import Calendar cal = Calendar.from_ical(open('test_cal.ics','rb').read())  Now I can get a list of my calendar events with cal.subcomponents, iterate through them, and index their dtstart and dtend values. These values must be followed by .dt to turn them into a standard datetime or date object, otherwise they\u0026rsquo;re in a propietary format for the module.\nUp and running Much to my surprise, the implementation was remarkably straightforward.\ndef ical_daterange(file): cal = Calendar.from_ical(open(file,'rb').read()) timeranges = [] for event in cal.subcomponents: timeranges.append((event['dtstart'].dt,event['dtend'].dt)) return timeranges  Just like that, I have a list of daterange tuples. The real test was whether this would play nicely with my filter function. Python confuses me with its vast interweaving of very similar object types, and I was doubting that the generic datetime objects created by the icalendar library would translate over to the Pandas datetime indexes.\nIt turns out, they do translate over. I ran a test with\ntest_range = ical_daterange('test_cal.ics') test_filtered = time_filter(df_energy,daterange=test_range)  and got a dataframe filtered by exactly the times I specified in my calendar.\nEdge behavior In my test calendar I included a normal event with a start and end time, as well as an \u0026lsquo;all-day\u0026rsquo; event that only holds a date, no time. The date/time event filters inclusively by the timespan specified (an event from 3PM-5PM will include 3PM and 5PM entries).\nThe \u0026lsquo;all-day\u0026rsquo; event spans from midnight to midnight. So an event on 3/20/16 will span from 00:00:00 on 3/20/17 to 00:00:00 on 3/21/17. So it\u0026rsquo;s just a hair over 24 hours, not a problem.\nModifying the time filter method Now that I know what kind of data I might be feeding it, I think I should change how the time_filter function operates. Right now I have daterange and inclusions parameters which perform similar tasks. Also, you can only exclude specific days, you can\u0026rsquo;t specify a range. The whole date selection process could be make more robust if the process for including and excluding dates functioned the same way, and if the date inclusion feature was just an extension of the daterange parameter. Furthermore, the label daterange is misleading because it can also include timestamps\nI\u0026rsquo;m going to get rid of the inclusions, exclusions, and daterange parameters. They will be replaced by include and blacklist. The timerange will be renamed to times for simplicity.\nThe include parameter will accept either a single time range tuple, a single date, or a list of any combination of the two. I wanted to call it range but that is already a Python keyword.\nThe blacklist will function the same way, but it will override the include parameter. If it becomes necessary in the future, I may also add a whitelist that overrides both.\nI\u0026rsquo;ll also take this opportunity to add the months filter I mentioned yesterday.\nThe updated code I added a couple of helper functions to reduce the amount of duplicate code. Their signatures are provided.\n\u0026quot;\u0026quot;\u0026quot; range_token_df: DataFrame, RangeToken --\u0026gt; DataFrame Returns a dataframe filtered by the range token provided. A RangeToken is either a datetime index (parial or formal) or a tuple of start/end datetime indexes \u0026quot;\u0026quot;\u0026quot; def range_token_df(data, token): if (type(token)==str): return data[token] else: # token is a start/end tuple return data[slice(*token)] \u0026quot;\u0026quot;\u0026quot; data_in_range : DataFrame/Series, Data range --\u0026gt; DataFrame/Series filters the input data by the date range provided \u0026quot;\u0026quot;\u0026quot; def data_in_range(data, d_range): if (type(d_range[0])==tuple): return pd.concat(list(map( lambda token: range_token_df(data,token), d_range))).sort_index() else: return range_token_df(data,d_range)  And here\u0026rsquo;s the updated time_filter code and signature:\n\u0026quot;\u0026quot;\u0026quot; time_filter: DataFrame/Series, ... --\u0026gt; DataFrame/Series filters data by properties like date and time PARAMETERS: data : DataFrame or Series with DateTimeIndex *times: Tuple with start and end time strings as 'HH:MM' or list of such tuples *include: Accepts: 1) A datetime index (partial or formal) 2) A tuple of start and end datetime indexes (See 1) Enter None to set to range min or max 3) A list that contains any combination of types 1 and 2 *blacklist: range of dates to be excluded. See include parameter for acceptable format Overrides include parameter *daysofweek: List of integers for days to be included 0 = Mon, 6 = Sun *months: List of integers for months to be included 1 = Jan, 12 = Dec starred parameters are optional ranges are all inclusive \u0026quot;\u0026quot;\u0026quot; def time_filter(data, **kwds): out = data if ('include' in kwds): out = data_in_range(out,kwds['include']) if ('times' in kwds): timerange = kwds['times'] if type(timerange[0]) is tuple: out = pd.concat(list(map( lambda subrange: out.between_time(*subrange), timerange))).sort_index() else: out = out.between_time(*timerange) if ('daysofweek' in kwds): out = out[[day in kwds['daysofweek'] for day in out.index.weekday]] if ('months' in kwds): out = out[[month in kwds['months'] for month in out.index.month]] if ('blacklist' in kwds): out = out.drop(data_in_range(data, kwds['blacklist']).index, errors='ignore') return out  I think it\u0026rsquo;s looking much cleaner. Based on my testing so far, everything should work out nicely. Tomorrow I will work on entering the Andover District Calendar data into Google Calendar, then running that calendar output through my range converter and time filterer.\n","description":"Generating complex time ranges with ical files, and improvements to time filtering","href":"https://mattrossman.github.io/energize-andover-blog/post/day-7/","title":"Day 7"},{"basename":"day-6","content":" Today I\u0026rsquo;ll be implementing the time_filter function that I outlined yesterday. Since then, I made some slight adjustments to the signature that you can see in the final version below.\nPacking and Unpacking First thing I had to look up was how to handle optional parameters. It seems that you can use an asterisk * before an argument to get a Tuple of optional positional parameters, while a double asterisk ** is for a Dictionary of optional keyword parameters.\nThis is a subset of the larger topic of packing and unpacking in Python. When used in a parameter field, the * and ** operations pack arguments into a Tuple or Dictionary, respectively. Outside of that context, they perform the inverse operation, unpacking a Tuple or Dictionary into its contained arguments.\nDates and Times This proves useful with my time_filter function which has a number or optional parameters, some of which are Tuples themselves which simply need to be unpacked.\nFor example, the line:\ndata.between_time(*timerange)  will unpack the start and end times contained in the timerange parameter and pass them as arguments to the between_time function.\nI also learned about the slice() function which turns its arguments into a slice to use for indexing. In combination with the * operator, I can say\ndata[slice(*daterange)]  to unpack the start and end dates, turn them into a slice, and use that slice to state the index bounds.\nTo handle the possible list of entries for each of these parameters, I can check if type(daterange[0]) is tuple and in that case do some more lambda calculus to perform the operation on all the subranges provided, then concatenate the results. I learned that it is important to then perform the sort_index function on the concatenated result since it does not automatically reorder the entries by their timestamp.\nFor the daysofweek parameter, I can use Python\u0026rsquo;s list comprehension feature to generate a boolean array of whether each day\u0026rsquo;s datetime.dayofweek attribute is contained in the list provided by the user:\ndata[[day in opt_kwds['daysofweek'] for day in data.index.weekday]]  Inclusion / Exclusion The exclusions parameter is a little tricky, mainly from my minimal familiarity with Timestamp and DateTimeIndex objects. Normally you would use the pandas.DataFrame.drop() function to remove objects based on their index, but it requires the exact index labels. The partial string indexing that lets you easily index all entries from a certain day don\u0026rsquo;t apply here. One solution is to use partial string indexing to get a series of entries from the given days, retrieve the indexes of those days, and pass that list of indexes to the drop() function:\nThe other annoying part is that partial string indexing, unlike exact string indexing, doesn\u0026rsquo;t let you pass in a list of items to select. So I\u0026rsquo;ll have to iterate through the exclusion dates and build up a cumulative list of indexes. Not a big deal but I wish there was a built in way to do this.\nInstead of doing this with a loop, I can use the map() function and lambda keyword to apply my action over each date. The list of indexes that .index returns is an ndarray object, part of the numpy library. As such, I have to use the numpy.concatenate function as described in the documentation.\nto_drop = np.concatenate(list (map(lambda x: out[x].index, opt_kwds['exclusions']))) out = out.drop(to_drop)  Adding elements is a simpler process so I will just use a loop to add the desired entries in the inclusions list:\nfor date in opt_kwds['inclusions']: out = out + data[date]  Final Product In my examples I operated on the data directly but really I\u0026rsquo;m keeping a running out DataFrame variable that tracks all of my changes.\nThis is the function in its current state, subject to change as I learn more efficient ways to use Pandas and NumPy.\n\u0026quot;\u0026quot;\u0026quot; time_filter: filters data by properties like date and time ARGS: data : DataFrame or Series with DateTimeIndex *timerange: Tuple with start and end time strings as 'HH:MM' or list of such tuples *daterange: Tuple with start and end dates as 'YYYY-MM-DD' or list of such tuples. Enter None to set to min or max date *exclusions: List of dates to be excluded as 'YYYY-MM-DD' *inclusions: List of dates to be explicity included as 'YYYY-MM-DD' This will override the daterange property *daysofweek: List of integers for days to be included 0 = Mon, 6 = Sun starred parameters are optional ranges are all inclusive \u0026quot;\u0026quot;\u0026quot; def time_filter(data, **opt_kwds): out = data if ('exclusions' in opt_kwds): to_drop = np.concatenate(list (map(lambda x: out[x].index, opt_kwds['exclusions']))) out = out.drop(to_drop) if ('timerange' in opt_kwds): timerange = opt_kwds['timerange'] if type(timerange[0]) is tuple: out = pd.concat(list(map( lambda subrange: out.between_time(*subrange), timerange))).sort_index() else: out = out.between_time(*timerange) if ('daterange' in opt_kwds): daterange = opt_kwds['daterange'] if type(daterange[0]) is tuple: out = pd.concat(list(map( lambda subrange: out[slice(*subrange)], daterange))).sort_index() else: out = out[slice(*daterange)] if ('daysofweek' in opt_kwds): out = out[[day in opt_kwds['daysofweek'] for day in out.index.weekday]] if ('inclusions' in opt_kwds): for date in opt_kwds['inclusions']: out = out + data[date] return out  As an example, if I want entries from March and April 2016 between 7:40AM and 2:20PM on Mon Wed Fri, I can do so with\ntime_filter(df_energy,daterange=('2016-03','2016-04'),timerange=('7:40','14:20'),daysofweek=(0,2,4))  As I write this I remember I will want to add another parameter for month selection, since the daterange argument only lets you select months from a specified year.\n","description":"Time filtering","href":"https://mattrossman.github.io/energize-andover-blog/post/day-6/","title":"Day 6"},{"basename":"day-5","content":" Daytime usage I haven\u0026rsquo;t really addressed the daytime power usage yet. To be consistent with the EnergizeApps parser, I\u0026rsquo;ll define \u0026lsquo;day\u0026rsquo; as 5AM - 8PM (inclusive, exclusive). Likewise going forward I should define \u0026lsquo;night\u0026rsquo; as 11PM - 4AM.\nI will not only want to look at the overall daytime usage, but isolate the weekday and weekend usage since I\u0026rsquo;d expect the building to be unoccupied on weekends. Once again I get to leverage the handy DatetimeIndex structure which holds a weekday component. This component is an integer which reads as follows:\n   Mon Tue Wed Thu Fri Sat Sun     0 1 2 3 4 5 6    So the weekdays correspond to days 0-4 while weekends are days 5-6.\nday_bools = (df_energy.index.hour \u0026gt;= 5) \u0026amp; (df_energy.index.hour \u0026lt; 20) df_day = df_energy[day_bools] df_weekday = df_day[df_day.index.weekday \u0026lt;= 4] df_weekend = df_day[df_day.index.weekday \u0026gt; 4]  You can not simply join the conditionals with the and keyword that Python would normally understand. If you did, it would try to evaluate the entire array to either just a true or false value. Instead you use the \u0026amp; operator to perform an item-wise comparison of all individual element pairs within the boolean arrays.\n EDIT: In hindsight, it would be simpler to use the pandas.DataFrame.between_time function here\n Now to look at the plots. For overall daytime power distribution we get:\n  Another bimodal distribution. At first I assumed this was just because of the distinction between weekday and weekend usage. But the weekend is only 2 days, you would assume that the low-power peak would be much smaller than the high-power peak.\nWhen we look at just the weekend usage we get:\n  It\u0026rsquo;s a weird shape, but at least it\u0026rsquo;s unimodal. Looking at the weekday usage, things get weirder.\n  The shape is still bimodal. That means there\u0026rsquo;s some factor here that\u0026rsquo;s still causing a split in the data. Then I remembered, this includes the whole year\u0026rsquo;s worth of data. That includes the 180 days of school time, but also holidays, breaks, and summer vacation. So the heap of low power usage must be primarily from vacation.\nTo test this, I can look at two charts: one with a month from the summer, and one with a month during the school year. July is a good summer midpoint, and after looking at the Andover academic calendar it looks like the month of March doesn\u0026rsquo;t have many days off from school. I can extract these with the pandas.DatetimeIndex.month property. In this case the index starts at 1 for Jan and extends to 12 for Jan.\nThe weekday-daytime power distribution for July:\n  And for March:\n  At first I was baffled, but then once again I realized an oversight. I am defining day as the hours from 5AM-8PM. This isn\u0026rsquo;t a good index to use for a school because school doesn\u0026rsquo;t run the entire day. To put this issue to rest, I\u0026rsquo;m going to isolate March\u0026rsquo;s data between 7:40 and 2:20. I found a pandas.Series.between_time() method in the documentation, I\u0026rsquo;ll use that here and probably change my use of the .hour attribute from earlier. If this doesn\u0026rsquo;t give me a unimodal shape I don\u0026rsquo;t know what will.\n  I\u0026rsquo;m satisfied with this result. In the console I printed the dataset that was registering under 250 kW which consisted of 4 days, 2 of which had no school in 2016 and the other 2 I\u0026rsquo;m guessing didn\u0026rsquo;t have school in 2015. The rest of the distribution looks nice and symmetric.\nFilters All of this work has just made it very clear how necessary it is to implement a robust filtering system to let administrators choose which days to include and exclude to isolate days that follow similar power usage models.\nThe program won\u0026rsquo;t know what kind of data is being fed to it, and without that info it can\u0026rsquo;t do any useful analysis. It\u0026rsquo;s up to the person running the program to be able to seperate groups of similar data to individually perform analysis upon.\nTo make this easier, I may implement some feature that detects multiple peaks and looks for patterns within them to suggest how to make further isolations.\nUp until this point, most of the programming work has been temporary scripting, but I need to start putting code into usable functions. Filters would be very helpful, so that will be my next task.\nTemplating the code In my semester at Northeastern I learned about the \u0026ldquo;design recipe\u0026rdquo; which takes a very systematic approach to programming. It\u0026rsquo;s a good habit to have, and I\u0026rsquo;ll loosely follow it here.\n\u0026quot;\u0026quot;\u0026quot; time_filter: filters data by properties like date and time ARGS: data : DataFrame or Series with DateTimeIndex *timerange: Tuple with start and end time strings as HH:MM or list of such tuples *daterange: Tuple with start and end dates as YYYY-MM-DD or list of such tuples. Blank Tuple element will default to MIN or MAX *exclusions: List of dates to be excluded as YYYY-MM-DD *inclusions: List of dates to be explicity included as YYYY-MM-DD This will override the daterange property *weekdays: List of integers for days to be included 0 = Mon, 6 = Sun *starred parameters are optional \u0026quot;\u0026quot;\u0026quot;  So I don\u0026rsquo;t forget it later, I recently found out that I can easily implement the daterange and excludions properties with the .loc function. E.g. I can simply type data['2016-03-01':'2016-03-20'] to filter between those dates.\nThis method signature seems to cover all the filtering features that I\u0026rsquo;ve needed so far. Tomorrow I\u0026rsquo;ll start making the code for it.\n","description":"Daytime usage first look","href":"https://mattrossman.github.io/energize-andover-blog/post/day-5/","title":"Day 5"},{"basename":"day-4","content":" I did some searching about median absolute deviation in pandas and found this post. The devoper mentioned that they were considering adding a center parameter to the built in Pandas.Series.mad() function, but advocates against using the function altogether and instead just implementing median absolute deviation manually like this:\nabs(x - x.median()).median()  In my case, I\u0026rsquo;ll be using x_right - x.median() since I\u0026rsquo;m only looking at the upper values.\nIn the context of my own code:\nnight_mad = abs(s_night_right - night_med).median()  Which comes out as:\nIn [1]: night_mad Out[1]: 20.2772  Side note, when calculating the MAD with all the data instead of just the right side, I got 18.7547 which is just a bit smaller.\nAfter some time reading the matplotlib documentation, I was able to generate the following chart:\n  I added shaded regions to highlight the areas formed by various multiples of the MAD. When working with a normal distribution, you would consider points that are more than 2 standard deviations (SDs) away from the mean to be unusual and those more than 3 SDs to be very unusual, since less than 1% of points should occupy that area. The MAD is not a direct equivalent to the standard deviation, so we can\u0026rsquo;t just use the same indicators for it. Yesterday I mentioned a constant factor of 1.4826 that can be used to approximate the standard deviation of a normal distribution. I can\u0026rsquo;t be sure exactly what kind of distribution to expect here, but for now I will just pretend that we should expect a normal distribution, at least for the right side of the data. And if we assume that, we can also assume that the mean is the same as the median.\n  Here\u0026rsquo;s the same plot but with the estimated standard deviations as region bounds instead of plain MADs. The value of $\\hat{\\sigma}$ came out to 30.063.\nVisually, it looks like a pretty good way to see which regions contain values that are too large. But statistically, I\u0026rsquo;m not too sure of its validity. It\u0026rsquo;s really a judgement call of whether the population is supposed to be normal or not. I\u0026rsquo;ll have to test it out on other samples to see if it still holds up.\nGaps in the data It recently came to my attention that the parsed data is not a perfectly continuous stream. It starts off fine, but as the data goes on there are some missing points. Eventually it becomes half as dense as it started out, with measurements for only every other timestamp. I need to keep that in mind when performing these calculations. I think that this is not going to be an issue for the time being, because even though parts of the data are less populated they still should show a similar distribution.\nPandas has a guide for how to work with missing data points in your code. Going forward, I can use notnull() and isnull() to filter such points out of my dataframe/series. For example, the basic line plot goes blank when trying to plot between empty values.\nStatus Update At this point given a CSV input file I can select a column and create a series of the entries that are more than 3 estimated population standard deviations away from the estimated population mean. The validity and usefulness of this measure is still yet to be tested. It\u0026rsquo;s a good start though. In the future, I hope to refine my understanding of the expected model for the population. At this point I\u0026rsquo;m trying to mold the right side of the sample into a normal curve and comparing against that. But perhaps it would be better to use the left side as a model and mirror it over to the right side. Or maybe I should be taking the MAD of the entire sample instead of just the right side. These are things I will have to keep in the back of my mind.\nOutliers are just one piece of the puzzle though. We\u0026rsquo;re not just looking for single days where there was an overuse of power, we\u0026rsquo;re looking at overall ways to reduce power. Perhaps the data is not centered where it should be. It is challenging to determine where the center of a sample should be. We want the analysis to be able to handle many different types of buildings. Some might actually require much power. That is, given two samples with similar centers, we should be able to detect which one is normal and which one is not. I think to do so, I\u0026rsquo;ll want to somehow use the lower end of the sample as a reference point.\n","description":"Median absolute deviations","href":"https://mattrossman.github.io/energize-andover-blog/post/day-4/","title":"Day 4"},{"basename":"day-3","content":" Quick blog update Just wanted to share, I was playing around with Hugo today and learned about shortcodes. I was looking for a better way to handle images. Previously I would have to type ![[alt](/path/to/img)](/path/to/img) to have self-linking images. Instead, I made a shortcode that just needs the path once to create the linked image. Furthermore, I was manually typing the image paths, which were organized as /month/day/file.png. Now the shortcode reads the post\u0026rsquo;s date and inserts the month and day straight into the path, so all I need are file names. So I just need to say:\n{{\u0026lt; linked-img file.png \u0026gt;}}  The code is available here.\nBack to the analysis Yesterday I got a feel for the sample data distribution. For now I will continue to focus on the night data since it\u0026rsquo;s reasonable to assume that there should be little power usage or variation going on during these hours.\nWith a normal distribution, the two constants we would want to consider are the mean ( $\\mu$ ) and the standard deviation ( $\\sigma$ ). However, as we saw yesterday, our data set does not follow a normal distrubution. I am not certain that it even should follow a normal distribution. For example, suppose a building uses very minimal (close to zero) power at night. There will naturally be some variation extending to the right, but it\u0026rsquo;s not possible to use negative power so there will not be much room for variation on the left. This is not likely to happen, but it still demonstrates how its easy for data points to pile up on the lower end of the scale, causing a tail to emerge off the right. The goal therefore is to reduce the presence of this tail.\nSince we are not dealing with a normal distribution, it would be more appropriate to consider the data\u0026rsquo;s median ( $\\eta$ ) and median absolute deviation, or MAD. The median is less subject to the influence of outliers or a tail because it is concerned with the midpoint according to count rather than by value. The same benefit applies to the MAD, which otherwise is comparable to the standard deviation.\nThe MAD is calculated as follows: $$ MAD = median( | x_{i} - x | )$$\nThis can then be used as an estimation of the population\u0026rsquo;s true standard deviation using the following equation:\n$$ \\hat{\\sigma}=1.4826 \\cdot MAD $$\nThis specific equation assumes that the population is actually normal. A full explaination of the 1.4826 constant can be found here.\nThis isn\u0026rsquo;t quite right though. We don\u0026rsquo;t really know if the population (where the \u0026lsquo;population\u0026rsquo; refers to the power usage of all similar buildings) is normally distributed or not. Our sample shows a skew, and as I explained above, it\u0026rsquo;s not unreasonable to expect some skew.\nAlso, in our case we don\u0026rsquo;t really care about the left half of the data, at least for this calculation. All we\u0026rsquo;re looking for is abnormally high data points, so a \u0026lsquo;low\u0026rsquo; outlier is just a pleasant surprise. I\u0026rsquo;ll worry about the data spread at a later time. Splitting the data into two halves is the same strategy described in this guide.\nWorking with the code I started yesterday, I can perform the split with this:\n# extract the main series from night dataframe s_night = df_night['Main (kW)'] # calculate its median night_med = s_night.median() # locate the values which are greater than the median s_night_right = s_night.loc[s_night\u0026gt;night_med]    Here\u0026rsquo;s the plot of the selected data. It\u0026rsquo;s roughly a slice from the right side of the peak. This is the part of the data that I\u0026rsquo;m going to want to analyze for values that have an abnormal variation.\nUnfortunately, pandas is sneaky and provides a Pandas.Series.mad() function, but it\u0026rsquo;s actually for the mean absolute deviation, not the median absolute deviation. Tomorrow I\u0026rsquo;ll see if there\u0026rsquo;s already one in existence, otherwise I\u0026rsquo;ll just make the function myself.\n","description":"Night distribution continued","href":"https://mattrossman.github.io/energize-andover-blog/post/day-3/","title":"Day 3"},{"basename":"day-2","content":" Now that the blog is running relatively smoothly I\u0026rsquo;m able to focus on the actual problem at hand. I started by installing the latest release of Anaconda, which I then used to install the pandas library.\nPandas in a Nutshell The pandas library is centered around DataFrame structures and their child Series structures. A dataframe functions like an excel spreadsheed, with various columns of data matched by each row\u0026rsquo;s index. Each column can be pulled out as a series object, but a single-columned dataframe is not necessarily the same as a series object. An example of a dataframe:\n   Index Main (kW) Gym (kW)     2017-03-21 02:45:00 70.81228 10.282110   2017-03-21 03:00:00 76.02322 9.198661   2017-03-21 03:15:00 75.19489 10.011790   2017-03-21 03:30:00 69.43213 10.757050    Initial analysis I used the current Metasys Parser application to download the parsed data from the 24 Hr detailed electrical period from Mar 2017 - Aug 2016 as a CSV file. This file has over 85,000 entries. Pandas is pretty quick to load it all in though:\ndf_energy = pd.read_csv(data_path, skipfooter=3, engine='python', index_col=0) df_energy.index = pd.to_datetime(df_energy.index)  The skipfooter parameter is included because the last 3 lines of the parsed CSV file contain some descriptive info that is not relevant data.\nBy default the index starts at 0 and goes up to (n-1) entries, but since we\u0026rsquo;re dealing with a time series I set the index for the dataframe to be what used to be its first column (the time stamps). I then converted these to a date/time object format that pandas can understand.\nTheoretically, the energy data should follow some kind of model. That model could be used to make comparisons with to detect abnormalities. I wanted to start by getting an idea of the overall distribution of all the data. Pandas has some basic plotting features built in that work in tandem with the matplotlib library.\n  As you can see, the data follows a sort of bimodal distribution. The main peak seems to be an indication of the general nighttime usage (since it occurs at a lower kW value), while the much smaller peak to the right of it likely marks the daytime usage. It\u0026rsquo;s good to see that the most frequent power usage is a lower value. Ideally we would see two somewhat defined peaks, with minimal power leakage in between or around the edges.\nI broke this chart down a bit further to single out the power usage between the hours of 00:00 and 04:00. Since the dataframe index is stored as a datetime object, it\u0026rsquo;s pretty easy to filter out time indexes as follows:\ndf_night = df_energy.loc[df_energy.index.hour \u0026lt;= 4]    As expected, we see a similar shape here as we did from the left side of the 24 hr chart. What\u0026rsquo;s concerning here is the right skew. Not a lot of activity should be going on during these late hours, so the distribution should be pretty symmetric. The trailing values to the right of the peak suggest areas of wasted energy. The next step is to figure out where that wastage is coming from.\nYesterday I spent a little time looking at the plots from the EnergizeApps site. I wanted to see if I could visually spot areas of odd power usage. I picked an average sample week (from Oct 17th to 23rd, 2016) which I checked didn\u0026rsquo;t have any scheduling abnormalities.\nHere\u0026rsquo;s the plot showing the usage from the lighting and gym mains.   The lighting chart is a pretty good indicator of the day/night cycle. The lighting units die down after about 3pm, but the gym doesn\u0026rsquo;t reach its minimum power usage until well past midnight. Certainly there are late night sports practices, but it seems excessive for the usage to extend that late.\nI\u0026rsquo;ll dig deeper tomorrow.\n","description":"Initial distribution analysis","href":"https://mattrossman.github.io/energize-andover-blog/post/day-2/","title":"Day 2"},{"basename":"day-1","content":"Today marks the start of my work on the Energize Andover project. I met with Anil at the library and went over the main goals of my team\u0026rsquo;s project.\nBasically, given an arbitrary data set of timestamped energy measures, we want to be able to perform various analytics and use those to identify key areas where there is potential for energy savings. How we will accomplish that is to be decided, though I was given some places to start.\nMost of today was devoted to setting up this blog. It\u0026rsquo;s hosted on GitHub so I have total control over all the assets. I tried using a Jekyll theme at first but has issues with it rendering properly. I had similar issues after switching to Hugo, but found that it was a minor matter of using an http url instead of https.\nI installed Ubuntu on my work laptop and got a feel for using the command interface. I got lots of practice with Git after repeatedly pushing updates as struggled to get the site rendering the same way on GitHub as it was locally. I eventually wrote a small script that runs the necessary commands to build and publish the web page so it should be easier to update going forward.\nEarlier today I started poking around with the output energy data from EnergizeApps, but I\u0026rsquo;ll wait to post about that since the images are stored on my other laptop for now.\n","description":"Introduction","href":"https://mattrossman.github.io/energize-andover-blog/post/day-1/","title":"Day 1"}]